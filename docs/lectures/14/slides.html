<!DOCTYPE html>
<html lang="en"><head>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.32">

  <meta name="author" content="Marcel Turcotte">
  <title>CSI 4106 - Fall 2025 – Neural Networks Architectures</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-f563837468303362081e247dddd440d0.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
  
  <script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<meta property="og:title" content="Neural Networks Architectures – CSI 4106 - Fall 2025">
<meta property="og:description" content="CSI 4106 - Fall 2025">
<meta property="og:site_name" content="CSI 4106 - Fall 2025">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Neural Networks Architectures</h1>
  <p class="subtitle">CSI 4106 - Fall 2025</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Marcel Turcotte 
</div>
</div>
</div>

  <p class="date">Version: Jul 10, 2025 16:56</p>
</section>
<section>
<section id="preamble" class="title-slide slide level1 center">
<h1>Preamble</h1>

</section>
<section id="quote-of-the-day" class="slide level2">
<h2>Quote of the Day</h2>
<p></p><div id="tweet-55889"></div><script>tweet={"url":"https:\/\/twitter.com\/ylecun\/status\/1795426059921268969","author_name":"Yann LeCun","author_url":"https:\/\/twitter.com\/ylecun","html":"\u003Cblockquote class=\"twitter-tweet\" align=\"center\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003ECurious to know how you could possibly do real-time camera image understanding in FSD without ConvNets, TBH.\u003C\/p\u003E&mdash; Yann LeCun (@ylecun) \u003Ca href=\"https:\/\/twitter.com\/ylecun\/status\/1795426059921268969?ref_src=twsrc%5Etfw\"\u003EMay 28, 2024\u003C\/a\u003E\u003C\/blockquote\u003E\n\u003Cscript async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"\u003E\u003C\/script\u003E\n\n","width":550,"height":null,"type":"rich","cache_age":"3153600000","provider_name":"Twitter","provider_url":"https:\/\/twitter.com","version":"1.0"};document.getElementById("tweet-55889").innerHTML = tweet["html"];</script><p></p>

<aside><div>
<p><a href="https://x.com/ylecun">Yann LeCun</a>, recognized as one of the three pioneers of deep learning and the inventor of Convolutional Neural Networks (CNNs), frequently engages in discussions with <a href="https://x.com/elonmusk">Elon Musk</a> on the social media platform X (previously known as Twitter).</p>
</div></aside></section>
<section id="learning-objectives" class="slide level2">
<h2>Learning objectives</h2>
<ul>
<li><strong>Explain</strong> the Hierarchy of Concepts in Deep Learning</li>
<li><strong>Compare</strong> Deep and Shallow Neural Networks</li>
<li><strong>Describe</strong> the Structure and Function of Convolutional Neural Networks (CNNs)</li>
<li><strong>Understand</strong> Convolution Operations Using Kernels</li>
<li><strong>Explain</strong> Receptive Fields, Padding, and Stride in CNNs</li>
<li><strong>Discusss</strong> the Role and Benefits of Pooling Layer</li>
</ul>
<aside class="notes">
<p>Today, we have a particularly dense agenda. The study of convolutional networks involves multiple levels of complexity. Please feel free to ask questions if you need clarification.</p>
<p>Detailed learning objectives.</p>
<ol type="1">
<li><strong>Explain the Hierarchy of Concepts in Deep Learning</strong>
<ul>
<li>Understand how deep learning models build hierarchical representations of data.</li>
<li>Recognize how this hierarchy reduces the need for manual feature engineering.</li>
</ul></li>
<li><strong>Compare Deep and Shallow Neural Networks</strong>
<ul>
<li>Discuss why deep networks are more parameter-efficient than shallow networks.</li>
<li>Explain the benefits of depth in neural network architectures.</li>
</ul></li>
<li><strong>Describe the Structure and Function of Convolutional Neural Networks (CNNs)</strong>
<ul>
<li>Understand how CNNs detect local patterns in data.</li>
<li>Explain how convolutional layers reduce the number of parameters through weight sharing.</li>
</ul></li>
<li><strong>Understand Convolution Operations Using Kernels</strong>
<ul>
<li>Describe how kernels (filters) are applied over input data to perform convolutions.</li>
<li>Explain how feature maps are generated from convolution operations.</li>
</ul></li>
<li><strong>Explain Receptive Fields, Padding, and Stride in CNNs</strong>
<ul>
<li>Define the concept of a receptive field in convolutional layers.</li>
<li>Understand how padding and stride affect the output dimensions and computation.</li>
</ul></li>
<li><strong>Discuss the Role and Benefits of Pooling Layers</strong>
<ul>
<li>Explain how pooling layers reduce spatial dimensions and control overfitting.</li>
<li>Describe how pooling introduces translation invariance in CNNs.</li>
</ul></li>
</ol>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="introduction" class="title-slide slide level1 center">
<h1>Introduction</h1>

</section>
<section id="hierarchy-of-concepts" class="slide level2">
<h2>Hierarchy of concepts</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="../../assets/images/cnn_lecun_et_al_nature.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img data-src="../../assets/images/cnn_lecun_et_al_nature.png" class="quarto-figure quarto-figure-center" style="width:100.0%"></a></p>
</figure>
</div>

<aside class="notes">
<p>In the book “Deep Learning” <span class="citation" data-cites="goodfellow:2016">(<a href="#/references" role="doc-biblioref" onclick="">Goodfellow, Bengio, and Courville 2016</a>)</span>, authors Goodfellow, Bengio, and Courville define deep learning as a subset of machine learning that enables computers to “understand the world in terms of a hierarchy of concepts.”</p>
<p>This hierarchical approach is one of deep learning’s most significant contributions. It reduces the need for manual feature engineering and redirects the focus toward the engineering of neural network architectures.</p>
<p>Convolutional Neural Networks (CNNs) have had a profound impact on the field of machine learning, particularly in areas involving image and video processing.</p>
<ol type="1">
<li><p><strong>Revolutionizing Image Recognition</strong>: CNNs have significantly advanced the state of the art in image recognition and classification, achieving high accuracy across various datasets. This has led to breakthroughs in fields such as medical imaging, autonomous vehicles, and facial recognition.</p></li>
<li><p><strong>Feature Extraction</strong>: CNNs automatically learn to extract features from raw data, eliminating the need for manual feature engineering. This capability has been crucial in handling complex data patterns and has expanded the applicability of machine learning to diverse domains.</p></li>
<li><p><strong>Transfer Learning</strong>: CNNs facilitate transfer learning, where pre-trained networks on large datasets can be fine-tuned for specific tasks with limited data. This has made CNNs accessible and effective for a wide range of applications beyond their original training scope.</p></li>
<li><p><strong>Advancements in Deep Learning</strong>: The success of CNNs has spurred further research in deep learning architectures, inspiring the development of more sophisticated models like recurrent neural networks (RNNs), long short-term memory networks (LSTMs), and transformer models.</p></li>
<li><p><strong>Broader Application Areas</strong>: Beyond image processing, CNNs have been adapted for natural language processing, audio processing, and even in bioinformatics for tasks such as protein structure prediction and genomics.</p></li>
<li><p><strong>Implications for Real-World Applications</strong>: CNNs have enabled practical applications in fields such as healthcare, where they assist in diagnostic imaging, and in security, where they enhance surveillance systems. They have also contributed to advancements in virtual reality, gaming, and augmented reality.</p></li>
</ol>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p><strong>Attribution</strong>: <span class="citation" data-cites="lecun:2015dt">LeCun, Bengio, and Hinton (<a href="#/references" role="doc-biblioref" onclick="">2015</a>)</span></p>
</div></aside></section>
<section id="hierarchy-of-concepts-1" class="slide level2 smaller">
<h2>Hierarchy of concepts</h2>
<ul>
<li>Each layer detects <strong>patterns</strong> from the output of the <strong>layer preceding it</strong>.
<ul>
<li>In other words, proceeding from the input to the output of the network, the network uncovers “<strong>patterns of patterns</strong>”.
<ul>
<li>Analyzing an image, the networks first detect simple patterns, such as <strong>vertical</strong>, <strong>horizontal</strong>, <strong>diagonal</strong> lines, <strong>arcs</strong>, etc.</li>
<li>These are then combined to form <strong>corners</strong>, <strong>crosses</strong>, etc.</li>
</ul></li>
</ul></li>
<li>(This explains how <strong>transfer learning</strong> works and why selecting the bottom layers only.)</li>
</ul>
</section>
<section id="but-also" class="slide level2">
<h2>But also …</h2>
<blockquote>
<p>“An MLP with just <strong>one hidden layer</strong> can theoretically model even the most <strong>complex functions</strong>, provided it <strong>has enough neurons</strong>. But for complex problems, <strong>deep networks</strong> have a much <strong>higher parameter efficiency</strong> than shallow ones: they can model complex functions <strong>using exponentially fewer neurons</strong> than shallow nets, allowing them to reach much <strong>better performance</strong> with the same amount of training data.”</p>
</blockquote>

<aside class="notes">
<p>During the lecture, attempt to discern why convolutional neural networks possess fewer parameters compared to fully connected feedforward networks.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p><span class="citation" data-cites="geron2019">Géron (<a href="#/references" role="doc-biblioref" onclick="">2019</a>)</span> § 10</p>
</div></aside></section>
<section id="how-many-layers" class="slide level2">
<h2>How many layers?</h2>
<ul>
<li>Start with one layer, then <strong>increase the number of layers</strong> until the model starts <strong>overfitting</strong> the training data.</li>
<li><strong>Finetune</strong> the model adding regularization (dropout layers, regularization terms, etc.).</li>
</ul>

<aside><div>
<p>The number of neurons and other hyperparameters are determined using a grid search.</p>
</div></aside></section>
<section id="observation" class="slide level2 smaller">
<h2>Observation</h2>
<p>Consider a <strong>feed-forward network</strong> (FFN) and its model:</p>
<p><span class="math display">\[
  h_{W,b}(X) = \phi_k(\ldots \phi_2(\phi_1(X)) \ldots)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
  \phi_l(Z) = \sigma(W_l Z + b_l)
\]</span></p>
<p>for <span class="math inline">\(l=1 \ldots k\)</span>. - The <strong>number of parameters</strong> in grows rapidly:</p>
<p><span class="math display">\[
  (\text{size of layer}_{l-1} + 1) \times \text{size of layer}_{l}
\]</span></p>
<p><strong>Two layers</strong> <strong>1,000-unit</strong> implies <strong>1,000,000</strong> parameters!</p>
</section></section>
<section>
<section id="convolutional-neural-network" class="title-slide slide level1 center">
<h1>Convolutional Neural Network</h1>

</section>
<section id="convolutional-neural-network-cnn" class="slide level2">
<h2>Convolutional Neural Network (CNN)</h2>
<iframe data-external="1" src="https://www.youtube.com/embed/QzY57FaENXg" width="889" height="500" title="What are Convolutional Neural Networks (CNNs)?" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<aside><div>
<p>An excellent high-level overview of CNNs.</p>
</div></aside></section>
<section id="convolutional-neural-network-cnn-1" class="slide level2 smaller">
<h2>Convolutional Neural Network (CNN)</h2>
<ul>
<li><p>Crucial pattern information is often <strong>local</strong>.</p>
<ul>
<li>e.g., edges, corners, crosses.</li>
</ul></li>
<li><p><strong>Convolutional layers</strong> reduce parameters significantly.</p>
<ul>
<li><p>Unlike dense layers, neurons in a convolutional layer are <strong>not fully connected</strong> to the preceding layer.</p></li>
<li><p>Neurons connect only within their <strong>receptive fields</strong> (rectangular regions).</p></li>
</ul></li>
</ul>

<aside><div>
<p><strong>Convolutional networks</strong> originate from the domain of <strong>machine vision</strong>, which explains their intrinsic compatibility with <strong>grid-structured inputs</strong>.</p>
<p>The original publication by Yann Lecun has been cited nearly 35,000 times <span class="citation" data-cites="Lecun:1998aa">(<a href="#/references" role="doc-biblioref" onclick="">Lecun et al. 1998</a>)</span>.</p>
</div></aside></section></section>
<section>
<section id="kernel" class="title-slide slide level1 center">
<h1>Kernel</h1>

</section>
<section id="kernel-1" class="slide level2">
<h2>Kernel</h2>
<div id="d42815bf" class="cell" data-execution_count="2">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-3-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img data-src="slides_files/figure-revealjs/cell-3-output-1.png" class="quarto-figure quarto-figure-center" width="868" height="374"></a></p>
</figure>
</div>
</div>
</div>

<aside><div>
<p>A <strong>kernel</strong> is a small matrix, usually <span class="math inline">\(3 \times 3\)</span>, <span class="math inline">\(5 \times 5\)</span>, or similar in size, that slides over the input data (such as an image) to perform convolution.</p>
</div></aside></section>
<section id="kernel-2" class="slide level2">
<h2>Kernel</h2>
<div id="8c000a4a" class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-4-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img data-src="slides_files/figure-revealjs/cell-4-output-1.png" class="quarto-figure quarto-figure-center" width="868" height="374"></a></p>
</figure>
</div>
</div>
</div>

<aside><div>
<p>Beginning with the kernel positioned to overlap the upper-left corner of the input matrix.</p>
</div></aside></section>
<section id="kernel-3" class="slide level2">
<h2>Kernel</h2>
<div id="a0a5f3cc" class="cell" data-execution_count="4">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-5-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img data-src="slides_files/figure-revealjs/cell-5-output-1.png" class="quarto-figure quarto-figure-center" width="868" height="374"></a></p>
</figure>
</div>
</div>
</div>

<aside><div>
<p>It can be moved to the right three times.</p>
</div></aside></section>
<section id="kernel-4" class="slide level2">
<h2>Kernel</h2>
<div id="53daea08" class="cell" data-execution_count="5">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-6-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img data-src="slides_files/figure-revealjs/cell-6-output-1.png" class="quarto-figure quarto-figure-center" width="868" height="374"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="kernel-5" class="slide level2">
<h2>Kernel</h2>
<div id="5a05c295" class="cell" data-execution_count="6">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-7-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img data-src="slides_files/figure-revealjs/cell-7-output-1.png" class="quarto-figure quarto-figure-center" width="868" height="374"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="kernel-6" class="slide level2">
<h2>Kernel</h2>
<div id="a55c7aa0" class="cell" data-execution_count="7">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-8-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img data-src="slides_files/figure-revealjs/cell-8-output-1.png" class="quarto-figure quarto-figure-center" width="868" height="374"></a></p>
</figure>
</div>
</div>
</div>

<aside class="notes">
<p>How many placements of the kernel over the input matrix are there? <span class="math inline">\(4 \times 4 = 16\)</span>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p>The kernel can then be moved to the second row of the input matrix, and moved to the right three times.</p>
</div></aside></section>
<section id="kernel-placements" class="slide level2 scrollable">
<h2>Kernel Placements</h2>
<div id="ad23f4f5" class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-9-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img data-src="slides_files/figure-revealjs/cell-9-output-1.png" class="quarto-figure quarto-figure-center" width="1121" height="1142"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="kernel-7" class="slide level2">
<h2>Kernel</h2>
<div id="35fd1562" class="cell" data-execution_count="9">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-10-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img data-src="slides_files/figure-revealjs/cell-10-output-1.png" class="quarto-figure quarto-figure-center" width="868" height="374"></a></p>
</figure>
</div>
</div>
</div>

<aside><div>
<p>With the kernel placed over a specific region of the input matrix, the <strong>convolution</strong> is <strong>element-wise multiplication</strong> (each element of the kernel is multiplied by the corresponding element of the input matrix region it overlaps) followed by a <strong>summation</strong> of the results to produce a <strong>single scalar value</strong>.</p>
</div></aside></section>
<section id="kernel-8" class="slide level2">
<h2>Kernel</h2>
<div id="c45d5761" class="cell" data-execution_count="10">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-11-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img data-src="slides_files/figure-revealjs/cell-11-output-1.png" class="quarto-figure quarto-figure-center" width="868" height="374"></a></p>
</figure>
</div>
</div>
</div>

<aside><div>
<p><span class="math inline">\(1 \times 1 + 2 \times 0 + 3 \times (-1) + 6 \times 1 + 5 \times 0 + 4 \times (-1) + 1 \times 1 + 2 \times 0 + 3 \times (-1) = -2\)</span></p>
</div></aside></section>
<section id="kernel-9" class="slide level2">
<h2>Kernel</h2>
<div id="01f1865a" class="cell" data-execution_count="11">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><a href="slides_files/figure-revealjs/cell-12-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11"><img data-src="slides_files/figure-revealjs/cell-12-output-1.png" width="1129" height="391"></a></p>
</figure>
</div>
</div>
</div>

<aside class="notes">
<p>It is referred to as a feature map because these outputs serve as features for the subsequent layer. In CNNs, the term “feature map” refers to the output of a convolutional layer after applying filters to the input data. These feature maps capture various patterns or features from the input, such as edges or textures in image data.</p>
<p>The output feature maps of one layer become the input for the next layer, effectively serving as features that the subsequent layer can use to learn more complex patterns. This hierarchical feature extraction process is a key characteristic of CNNs, allowing them to build progressively more abstract and high-level representations of the input data as the network depth increases.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p>The 16 resulting values can be organized into an <strong>output matrix</strong>. The element at position (0,0) in this output matrix represents the result of applying the convolution operation with the kernel at the initial position on the input matrix. In convolutional neural networks, the output matrix is referred to as a <strong>feature map</strong>.</p>
</div></aside></section>
<section id="blurring" class="slide level2">
<h2>Blurring</h2>
<div id="ed1b6060" class="cell" data-execution_count="12">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><a href="slides_files/figure-revealjs/cell-13-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img data-src="slides_files/figure-revealjs/cell-13-output-1.png" width="950" height="162"></a></p>
</figure>
</div>
</div>
</div>
<div id="c6589aac" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="co"># Define the 3x3 averaging kernel</span></span>
<span id="cb1-2"><a></a></span>
<span id="cb1-3"><a></a>kernel <span class="op">=</span> np.array([</span>
<span id="cb1-4"><a></a>    [<span class="dv">1</span><span class="op">/</span><span class="dv">9</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">9</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">9</span>],</span>
<span id="cb1-5"><a></a>    [<span class="dv">1</span><span class="op">/</span><span class="dv">9</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">9</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">9</span>],</span>
<span id="cb1-6"><a></a>    [<span class="dv">1</span><span class="op">/</span><span class="dv">9</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">9</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">9</span>]</span>
<span id="cb1-7"><a></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>

<aside class="notes">
<p>The application of kernels to images has been a longstanding practice in the field of image processing.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p>A pixel is transformed into the <strong>average of itself and its eight surrounding neighbors</strong>, resulting in a blurred effect on the image.</p>
</div></aside></section>
<section id="vertical-edge-detection" class="slide level2">
<h2>Vertical Edge detection</h2>
<div id="650973e0" class="cell" data-execution_count="14">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><a href="slides_files/figure-revealjs/cell-15-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img data-src="slides_files/figure-revealjs/cell-15-output-1.png" width="950" height="162"></a></p>
</figure>
</div>
</div>
</div>
<div id="6410475b" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a><span class="co"># Define the 3x3 averaging kernel</span></span>
<span id="cb2-2"><a></a></span>
<span id="cb2-3"><a></a>kernel <span class="op">=</span> np.array([</span>
<span id="cb2-4"><a></a>    [<span class="op">-</span><span class="fl">0.25</span>, <span class="dv">0</span>, <span class="fl">0.25</span>],</span>
<span id="cb2-5"><a></a>    [<span class="op">-</span><span class="fl">0.25</span>, <span class="dv">0</span>, <span class="fl">0.25</span>],</span>
<span id="cb2-6"><a></a>    [<span class="op">-</span><span class="fl">0.25</span>, <span class="dv">0</span>, <span class="fl">0.25</span>]</span>
<span id="cb2-7"><a></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>

<aside><div>
<p>This kernel detects vertical edges by <strong>emphasizing differences in intensity between adjacent columns</strong>. It subtracts pixel values on the left from those on the right, enhancing vertical transitions and suppressing uniform regions.</p>
</div></aside></section>
<section id="horizontal-edge-detection" class="slide level2">
<h2>Horizontal Edge detection</h2>
<div id="7c49747c" class="cell" data-execution_count="16">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><a href="slides_files/figure-revealjs/cell-17-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14"><img data-src="slides_files/figure-revealjs/cell-17-output-1.png" width="950" height="162"></a></p>
</figure>
</div>
</div>
</div>
<div id="e7e44181" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a><span class="co"># Define the 3x3 averaging kernel</span></span>
<span id="cb3-2"><a></a></span>
<span id="cb3-3"><a></a>kernel <span class="op">=</span> np.array([</span>
<span id="cb3-4"><a></a>    [<span class="op">-</span><span class="fl">0.25</span>, <span class="op">-</span><span class="fl">0.25</span>, <span class="op">-</span><span class="fl">0.25</span>],</span>
<span id="cb3-5"><a></a>    [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb3-6"><a></a>    [<span class="fl">0.25</span>, <span class="fl">0.25</span>, <span class="fl">0.25</span>]</span>
<span id="cb3-7"><a></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>

<aside><div>
<p>This kernel detects horizontal edges by <strong>highlighting differences in intensity between adjacent rows</strong>. It subtracts pixel values in the upper row from those in the lower row, accentuating horizontal transitions while minimizing uniform areas.</p>
</div></aside></section>
<section id="convolutions-in-image-processing" class="slide level2">
<h2>Convolutions in Image Processing</h2>
<iframe data-external="1" src="https://www.youtube.com/embed/8rrHTtUzyZA" width="889" height="500" title="Convolutions in Image Processing" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<aside><div>
<p>Uses the <strong><a href="https://julialang.org">Julia</a></strong> programming language.</p>
</div></aside></section>
<section id="but-what-is-a-convolution" class="slide level2">
<h2>But what is a convolution?</h2>
<iframe data-external="1" src="https://www.youtube.com/embed/KuXjwB4LzSA" width="889" height="500" title="But what is a convolution?" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<aside><div>
<p><strong>Convolution</strong> extending beyond its application in image processing.</p>
</div></aside></section>
<section id="kernels" class="slide level2">
<h2>Kernels</h2>
<p>In contrast to image processing, where kernels are manually defined by the user, in convolutional networks, the kernels are automatically learned by the network.</p>

<aside><div>
<p>To be continued <span class="math inline">\(\ldots\)</span></p>
</div></aside></section></section>
<section>
<section id="receptive-field" class="title-slide slide level1 center">
<h1>Receptive field</h1>

</section>
<section id="receptive-field-1" class="slide level2">
<h2>Receptive field</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="../../assets/images/geron-2019_14_2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15"><img data-src="../../assets/images/geron-2019_14_2.png" class="quarto-figure quarto-figure-center" height="500"></a></p>
</figure>
</div>

<aside><div>
<p><strong>Attribution</strong>: <span class="citation" data-cites="geron2019">Géron (<a href="#/references" role="doc-biblioref" onclick="">2019</a>)</span> Figure 14.2</p>
</div></aside></section>
<section id="receptive-field-2" class="slide level2">
<h2>Receptive field</h2>
<ul>
<li>Each unit is connected to neurons in its <strong>receptive fields</strong>.
<ul>
<li>Unit <span class="math inline">\(i,j\)</span> in layer <span class="math inline">\(l\)</span> is connected to the units <span class="math inline">\(i\)</span> to <span class="math inline">\(i+f_h-1\)</span>, <span class="math inline">\(j\)</span> to <span class="math inline">\(j+f_w-1\)</span> of the layer <span class="math inline">\(l-1\)</span>, where <span class="math inline">\(f_h\)</span> and <span class="math inline">\(f_w\)</span> are respectively the <strong>height</strong> and <strong>width</strong> of the <strong>receptive field</strong>.</li>
</ul></li>
</ul>
</section>
<section id="padding" class="slide level2">
<h2>Padding</h2>
<p><strong>Zero padding</strong>. In order to have layers of the same size, the grid can be padded with zeros.</p>
</section>
<section id="padding-1" class="slide level2">
<h2>Padding</h2>
<div class="columns">
<div class="column" style="width:33%;">
<p>No padding</p>
<p><a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/no_padding_no_strides.gif"><img data-src="https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_no_strides.gif"></a></p>
</div><div class="column" style="width:33%;">
<p>Half padding</p>
<p><a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/same_padding_no_strides.gif"><img data-src="https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/same_padding_no_strides.gif"></a></p>
</div><div class="column" style="width:33%;">
<p>Full padding</p>
<p><a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/full_padding_no_strides.gif"><img data-src="https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/full_padding_no_strides.gif"></a></p>
</div></div>

<aside><div>
<p><strong>Attribution</strong>: <a href="https://github.com/vdumoulin/conv_arithmetic/">github.com/vdumoulin/conv_arithmetic</a></p>
</div></aside></section>
<section id="stride" class="slide level2">
<h2>Stride</h2>
<p><strong>Stride</strong>. It is possible to connect a larger layer <span class="math inline">\((l-1)\)</span> to a smaller one <span class="math inline">\((l)\)</span> by skipping units. The number of units skipped is called <strong>stride</strong>, <span class="math inline">\(s_h\)</span> and <span class="math inline">\(s_w\)</span>.</p>
<div class="fragment">
<ul>
<li>Unit <span class="math inline">\(i,j\)</span> in layer <span class="math inline">\(l\)</span> is connected to the units <span class="math inline">\(i \times s_h\)</span> to <span class="math inline">\(i \times s_h + f_h - 1\)</span>, <span class="math inline">\(j \times s_w\)</span> to <span class="math inline">\(j \times s_w + f_w - 1\)</span> of the layer <span class="math inline">\(l-1\)</span>, where <span class="math inline">\(f_h\)</span> and <span class="math inline">\(f_w\)</span> are respectively the <strong>height</strong> and <strong>width</strong> of the <strong>receptive field</strong>, <span class="math inline">\(s_h\)</span> and <span class="math inline">\(s_w\)</span> are respectively the <strong>height</strong> and <strong>width</strong> <strong>strides</strong>.</li>
</ul>
</div>
</section>
<section id="stride-1" class="slide level2">
<h2>Stride</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>No padding, strides</p>
<p><a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/no_padding_strides.gif"><img data-src="https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_strides.gif"></a></p>
</div><div class="column" style="width:50%;">
<p>Padding, strides</p>
<p><a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/padding_strides.gif"><img data-src="https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/padding_strides.gif"></a></p>
</div></div>

<aside><div>
<p><strong>Attribution</strong>: <a href="https://github.com/vdumoulin/conv_arithmetic/">github.com/vdumoulin/conv_arithmetic</a></p>
</div></aside></section></section>
<section>
<section id="filters" class="title-slide slide level1 center">
<h1>Filters</h1>

</section>
<section id="filters-1" class="slide level2">
<h2>Filters</h2>
<ul>
<li><p>A <strong>window</strong> of size <span class="math inline">\(f_h \times f_w\)</span> is moved over the output of layers <span class="math inline">\(l-1\)</span>, referred to as the <strong>input feature map</strong>, position by position.</p></li>
<li><p><strong>For each location</strong>, the product is calculated between the extracted patch and a matrix of the same size, known as a <strong>convolution kernel</strong> or <strong>filter</strong>. The <strong>sum</strong> of the values in the resulting matrix constitutes the <strong>output</strong> for that location.</p></li>
</ul>
</section></section>
<section>
<section id="model" class="title-slide slide level1 center">
<h1>Model</h1>

</section>
<section id="model-1" class="slide level2">
<h2>Model</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="../../assets/images/geron-2019_14_6.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16"><img data-src="../../assets/images/geron-2019_14_6.png" class="quarto-figure quarto-figure-center" height="200"></a></p>
</figure>
</div>
<p><span class="math display">\[
z_{i,j,k} = b_k + \sum_{u=0}^{f_h-1} \sum_{v=0}^{f_w-1} \sum_{k'=0}^{f_{n'}-1} x_{i',j',k'} \cdot w_{u,v,k',k}
\]</span></p>
<p>where <span class="math inline">\(i' = i \times s_h + u\)</span> and <span class="math inline">\(j' = j \times s_w + v\)</span>.</p>

<aside><div>
<p><strong>Attribution:</strong> <span class="citation" data-cites="geron2019">Géron (<a href="#/references" role="doc-biblioref" onclick="">2019</a>)</span> Figure 14.6</p>
</div></aside></section>
<section id="convolutional-layer" class="slide level2">
<h2>Convolutional Layer</h2>
<ul>
<li class="fragment"><p>“Thus, a layer full of neurons using the <strong>same filter</strong> outputs a <strong>feature map</strong>.”</p></li>
<li class="fragment"><p>“Of course, you do not have to define the filters manually: instead, <strong>during training the convolutional layer will automatically learn the most useful filters for its task</strong>.”</p></li>
</ul>

<aside><div>
<p><span class="citation" data-cites="geron2019">Géron (<a href="#/references" role="doc-biblioref" onclick="">2019</a>)</span> § 14</p>
</div></aside></section>
<section id="convolutional-layer-1" class="slide level2">
<h2>Convolutional Layer</h2>
<ul>
<li class="fragment"><p>“(…) and <strong>the layers above will learn to combine them into more complex patterns</strong>.”</p></li>
<li class="fragment"><p>“The fact that <strong>all neurons in a feature map</strong> <strong>share the same parameters</strong> dramatically reduces the number of parameters in the model.”</p></li>
</ul>

<aside><div>
<p><span class="citation" data-cites="geron2019">Géron (<a href="#/references" role="doc-biblioref" onclick="">2019</a>)</span> § 14</p>
</div></aside></section>
<section id="summmary" class="slide level2">
<h2>Summmary</h2>
<ol type="1">
<li><strong>Feature Map:</strong> In convolutional neural networks (CNNs), the output of a convolution operation is known as a feature map. It captures the features of the input data as processed by a specific kernel.</li>
</ol>
</section>
<section id="summmary-1" class="slide level2">
<h2>Summmary</h2>
<ol start="2" type="1">
<li><strong>Kernel Parameters:</strong> The parameters of the kernel are learned through the backpropagation process, allowing the network to optimize its feature extraction capabilities based on the training data.</li>
</ol>
</section>
<section id="summmary-2" class="slide level2">
<h2>Summmary</h2>
<ol start="3" type="1">
<li><strong>Bias Term:</strong> A single bias term is added uniformly to all entries of the feature map. This bias helps adjust the activation level, providing additional flexibility for the network to better fit the data.</li>
</ol>
</section>
<section id="summmary-3" class="slide level2">
<h2>Summmary</h2>
<ol start="4" type="1">
<li><strong>Activation Function:</strong> Following the addition of the bias, the feature map values are typically passed through an activation function, such as ReLU (Rectified Linear Unit). The ReLU function introduces non-linearity by setting negative values to zero while retaining positive values, enabling the network to learn more complex patterns.</li>
</ol>
</section></section>
<section>
<section id="pooling" class="title-slide slide level1 center">
<h1>Pooling</h1>

</section>
<section id="pooling-1" class="slide level2">
<h2>Pooling</h2>
<ul>
<li class="fragment"><p>A <strong>pooling layer</strong> exhibits similarities to a <strong>convolutional layer</strong>.</p>
<ul>
<li class="fragment">Each neuron in a pooling layer is connected to a set of neurons within a <strong>receptive field</strong>.</li>
</ul></li>
<li class="fragment"><p>However, unlike convolutional layers, pooling layers do not possess <strong>weights</strong>.</p>
<ul>
<li class="fragment">Instead, they produce an output by applying an aggregating function, commonly <strong>max</strong> or <strong>mean</strong>.</li>
</ul></li>
</ul>

<aside class="notes">
<p>In a pooling layer, specifically max pooling, the max function is inherently nondifferentiable because it involves selecting the maximum value from a set of inputs. However, in the context of backpropagation in neural networks, we can work around this by using a concept known as the “gradient of the max function.”</p>
<p>Here’s how it is done:</p>
<ol type="1">
<li><p><strong>Forward Pass</strong>: During the forward pass, the max pooling layer selects the maximum value from each pooling region (e.g., a 2x2 window) and passes these values to the next layer.</p></li>
<li><p><strong>Backward Pass</strong>: During backpropagation, the gradient is propagated only to the input that was the maximum value in the forward pass. This means that the derivative is 1 for the position that held the maximum value and 0 for all other positions within the pooling window.</p></li>
</ol>
<p>This approach effectively allows the max operation to participate in gradient-based optimization processes like backpropagation, even though the max function itself is nondifferentiable. By assigning the gradient to the position of the maximum value, the network can learn which features are most important for the task at hand.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p>Similar to convolutional layers, pooling layers allow specification of the receptive field size, padding, and stride. For the <code>MaxPool2D</code> function, the default receptive field size is <span class="math inline">\(2 \times 2\)</span>.</p>
</div></aside></section>
<section id="pooling-2" class="slide level2">
<h2>Pooling</h2>
<ul>
<li class="fragment"><p>This subsampling process leads to a <strong>reduction in network size</strong>; each window of dimensions <span class="math inline">\(f_h \times f_w\)</span> is condensed to a single value, typically the <strong>maximum</strong> or <strong>mean</strong> of that window.</p></li>
<li class="fragment"><p>According to <span class="citation" data-cites="geron2019">Géron (<a href="#/references" role="doc-biblioref" onclick="">2019</a>)</span>, a max pooling layer provides a degree of <strong>invariance to small translations</strong> (§&nbsp;14).</p></li>
</ul>
</section>
<section id="pooling-3" class="slide level2">
<h2>Pooling</h2>
<ol type="1">
<li><strong>Dimensionality Reduction:</strong> Pooling layers reduce the spatial dimensions (width and height) of the input feature maps. This reduction decreases the number of parameters and computational load in the network, which can help prevent overfitting.</li>
</ol>
</section>
<section id="pooling-4" class="slide level2">
<h2>Pooling</h2>
<ol start="2" type="1">
<li><strong>Feature Extraction:</strong> By summarizing the presence of features in a region, pooling layers help retain the most critical information while discarding less important details. This process enables the network to focus on the most salient features.</li>
</ol>
</section>
<section id="pooling-5" class="slide level2">
<h2>Pooling</h2>
<ol start="3" type="1">
<li><strong>Translation Invariance:</strong> Pooling introduces a degree of invariance to translations and distortions in the input. For instance, max pooling captures the most prominent feature in a local region, making the network less sensitive to small shifts or variations in the input.</li>
</ol>
</section>
<section id="pooling-6" class="slide level2">
<h2>Pooling</h2>
<ol start="4" type="1">
<li><strong>Noise Reduction:</strong> Pooling can help smooth out noise in the input by aggregating information over a region, thus emphasizing consistent features over random variations.</li>
</ol>
</section>
<section id="pooling-7" class="slide level2">
<h2>Pooling</h2>
<ol start="5" type="1">
<li><strong>Hierarchical Feature Learning:</strong> By reducing the spatial dimensions progressively through the layers, pooling layers allow the network to build a hierarchical representation of the input data, capturing increasingly abstract and complex features at deeper layers.</li>
</ol>
</section>
<section id="keras" class="slide level2 scrollable">
<h2>Keras</h2>
<div id="7ff0888f" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb4-2"><a></a><span class="im">from</span> functools <span class="im">import</span> partial  </span>
<span id="cb4-3"><a></a></span>
<span id="cb4-4"><a></a>DefaultConv2D <span class="op">=</span> partial(tf.keras.layers.Conv2D, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="st">"same"</span>, activation<span class="op">=</span><span class="st">"relu"</span>, kernel_initializer<span class="op">=</span><span class="st">"he_normal"</span>) </span>
<span id="cb4-5"><a></a></span>
<span id="cb4-6"><a></a>model <span class="op">=</span> tf.keras.Sequential([     </span>
<span id="cb4-7"><a></a>  DefaultConv2D(filters<span class="op">=</span><span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">7</span>, input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>]), </span>
<span id="cb4-8"><a></a>  tf.keras.layers.MaxPool2D(),     </span>
<span id="cb4-9"><a></a>  DefaultConv2D(filters<span class="op">=</span><span class="dv">128</span>),</span>
<span id="cb4-10"><a></a>  DefaultConv2D(filters<span class="op">=</span><span class="dv">128</span>),</span>
<span id="cb4-11"><a></a>  tf.keras.layers.MaxPool2D(),</span>
<span id="cb4-12"><a></a>  DefaultConv2D(filters<span class="op">=</span><span class="dv">256</span>),</span>
<span id="cb4-13"><a></a>  DefaultConv2D(filters<span class="op">=</span><span class="dv">256</span>),</span>
<span id="cb4-14"><a></a>  tf.keras.layers.MaxPool2D(),</span>
<span id="cb4-15"><a></a>  tf.keras.layers.Flatten(),</span>
<span id="cb4-16"><a></a>  tf.keras.layers.Dense(units<span class="op">=</span><span class="dv">128</span>, activation<span class="op">=</span><span class="st">"relu"</span>, kernel_initializer<span class="op">=</span><span class="st">"he_normal"</span>),     </span>
<span id="cb4-17"><a></a>  tf.keras.layers.Dropout(<span class="fl">0.5</span>),</span>
<span id="cb4-18"><a></a>  tf.keras.layers.Dense(units<span class="op">=</span><span class="dv">64</span>, activation<span class="op">=</span><span class="st">"relu"</span>, kernel_initializer<span class="op">=</span><span class="st">"he_normal"</span>),</span>
<span id="cb4-19"><a></a>  tf.keras.layers.Dropout(<span class="fl">0.5</span>),</span>
<span id="cb4-20"><a></a>  tf.keras.layers.Dense(units<span class="op">=</span><span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>) ])  </span>
<span id="cb4-21"><a></a></span>
<span id="cb4-22"><a></a>model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


<aside class="notes">
<p>The previously discussed model, which comprised fully connected (Dense) layers, attained a test accuracy of 88%.</p>
<p>We will look at pooling next.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p><span class="citation" data-cites="Geron:2022aa">Géron (<a href="#/references" role="doc-biblioref" onclick="">2022</a>)</span> Chapter 11, test accuracy of 92% on the Fashion-MNIST dataset</p>
</div></aside></section>
<section id="keras-output" class="slide level2 scrollable output-location-slide"><h2>Keras</h2><div class="cell output-location-slide" data-execution_count="18">
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ conv2d (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)                 │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">28</span>, <span style="color: #00af00; text-decoration-color: #00af00">28</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)     │         <span style="color: #00af00; text-decoration-color: #00af00">3,200</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d (<span style="color: #0087ff; text-decoration-color: #0087ff">MaxPooling2D</span>)    │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">14</span>, <span style="color: #00af00; text-decoration-color: #00af00">14</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)     │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">14</span>, <span style="color: #00af00; text-decoration-color: #00af00">14</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)    │        <span style="color: #00af00; text-decoration-color: #00af00">73,856</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_2 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">14</span>, <span style="color: #00af00; text-decoration-color: #00af00">14</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)    │       <span style="color: #00af00; text-decoration-color: #00af00">147,584</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">MaxPooling2D</span>)  │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">7</span>, <span style="color: #00af00; text-decoration-color: #00af00">7</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)      │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_3 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">7</span>, <span style="color: #00af00; text-decoration-color: #00af00">7</span>, <span style="color: #00af00; text-decoration-color: #00af00">256</span>)      │       <span style="color: #00af00; text-decoration-color: #00af00">295,168</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_4 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">7</span>, <span style="color: #00af00; text-decoration-color: #00af00">7</span>, <span style="color: #00af00; text-decoration-color: #00af00">256</span>)      │       <span style="color: #00af00; text-decoration-color: #00af00">590,080</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d_2 (<span style="color: #0087ff; text-decoration-color: #0087ff">MaxPooling2D</span>)  │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">3</span>, <span style="color: #00af00; text-decoration-color: #00af00">3</span>, <span style="color: #00af00; text-decoration-color: #00af00">256</span>)      │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ flatten (<span style="color: #0087ff; text-decoration-color: #0087ff">Flatten</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">2304</span>)           │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                   │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)            │       <span style="color: #00af00; text-decoration-color: #00af00">295,040</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)            │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)             │         <span style="color: #00af00; text-decoration-color: #00af00">8,256</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)             │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)             │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_2 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">10</span>)             │           <span style="color: #00af00; text-decoration-color: #00af00">650</span> │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">1,413,834</span> (5.39 MB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">1,413,834</span> (5.39 MB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div></section><section id="convolutional-neural-networks" class="slide level2">
<h2>Convolutional Neural Networks</h2>
<p><a href="../../assets/images/CNN-cpcpd.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17"><img data-src="../../assets/images/CNN-cpcpd.png"></a></p>

<aside><div>
<p>The architecture involves sequentially stacking several convolutional layers, each followed by a ReLU activation layer, and then a pooling layer. As this process continues, the spatial dimensions of the image representation decrease. Concurrently, the number of feature maps increases, as illustrated in our Keras example. At the top of this stack, a standard feedforward neural network is incorporated.</p>
</div></aside></section>
<section id="alexnet" class="slide level2">
<h2>AlexNet</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="../../assets/images/ConvAlex.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-18"><img data-src="../../assets/images/ConvAlex.svg" class="quarto-figure quarto-figure-center" height="400"></a></p>
</figure>
</div>
<p><span class="citation" data-cites="Krizhevsky:2012aa">Krizhevsky, Sutskever, and Hinton (<a href="#/references" role="doc-biblioref" onclick="">2012</a>)</span></p>

<aside class="notes">
<p>AlexNet consists of eight layers with learnable parameters: five convolutional layers followed by three fully connected layers. The architecture also includes max-pooling layers, ReLU activation functions, and dropout to improve training performance and reduce overfitting.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p><strong>Attribution</strong>: <span class="citation" data-cites="prince2023understanding">Prince (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span></p>
</div></aside></section>
<section id="vgg" class="slide level2">
<h2>VGG</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="../../assets/images/ConvVGG.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-19"><img data-src="../../assets/images/ConvVGG.svg" class="quarto-figure quarto-figure-center" height="400"></a></p>
</figure>
</div>
<p><span class="citation" data-cites="Simonyan15">Simonyan and Zisserman (<a href="#/references" role="doc-biblioref" onclick="">2015</a>)</span></p>

<aside class="notes">
<p>Complementary information can be found <a href="https://www.robots.ox.ac.uk/~vgg/research/very_deep/">here</a>.</p>
<blockquote>
<p>Convolutional networks (ConvNets) currently set the state of the art in visual recognition. The aim of this project is to investigate how the ConvNet depth affects their accuracy in the large-scale image recognition setting.</p>
</blockquote>
<blockquote>
<p>Our main contribution is a rigorous evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by increasing the depth to 16-19 weight layers, which is substantially deeper than what has been used in the prior art. To reduce the number of parameters in such very deep networks, we use very small 3×3 filters in all convolutional layers (the convolution stride is set to 1). Please see our publication for more details.</p>
</blockquote>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p><strong>Attribution</strong>: <span class="citation" data-cites="prince2023understanding">Prince (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span></p>
</div></aside></section>
<section id="convnets-performance" class="slide level2">
<h2>ConvNets Performance</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="../../assets/images/ConvImageNetPerformance.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-20"><img data-src="../../assets/images/ConvImageNetPerformance.svg" class="quarto-figure quarto-figure-center" height="400"></a></p>
</figure>
</div>

<aside><div>
<p><strong>Attribution</strong>: <span class="citation" data-cites="prince2023understanding">Prince (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span></p>
</div></aside></section>
<section id="statquest" class="slide level2">
<h2>StatQuest</h2>
<iframe data-external="1" src="https://www.youtube.com/embed/HGwBXDKFk9I" width="711" height="400" title="Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<aside><div>
<p>The video presents a straightforward example that differentiates between images of the letter O and the letter X, utilizing a single filter for this purpose. This approach simplifies the explanation, making it easy to follow. In practical applications, however, each convolutional layer typically contains dozens or even hundreds of filters.</p>
</div></aside></section>
<section id="final-word" class="slide level2">
<h2>Final Word</h2>
<p>As you might expect, the <strong>number of layers</strong> and <strong>filters</strong> are hyperparameters that are optimized through the process of <strong>hyperparameter tuning</strong>.</p>
<div class="columns">
<div class="column" style="width:80%;">

</div><div class="column" style="width:20%;">
<p><a href="../../assets/images/desperate-emoji.png" class="lightbox" data-gallery="quarto-lightbox-gallery-21"><img data-src="../../assets/images/desperate-emoji.png"></a></p>
</div></div>

<aside><div>
<p><strong>Attribution</strong>: <a href="https://emojis.sh/emoji/desperate-and-discouraged-emoji-xiBgGFCW51">@stefaan_cotteni</a></p>
</div></aside></section></section>
<section>
<section id="prologue" class="title-slide slide level1 center">
<h1>Prologue</h1>

</section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
<ul>
<li>Hierarchy of Concepts in Deep Learning</li>
<li>Kernels and Convolution Operations</li>
<li>Receptive Field, Padding, and Stride</li>
<li>Filters and Feature Maps</li>
<li>Convolutional Layers</li>
<li>Pooling Layers</li>
</ul>
<aside class="notes">
<p><strong>Summary</strong></p>
<ul>
<li><strong>Hierarchy of Concepts in Deep Learning</strong>
<ul>
<li>Deep learning models represent data through layers of increasing abstraction.</li>
<li>Each layer learns patterns based on the outputs of preceding layers (“patterns of patterns”).</li>
<li>This hierarchical learning reduces reliance on manual feature engineering.</li>
<li>Deep networks achieve better performance with fewer parameters compared to shallow networks.</li>
</ul></li>
<li><strong>Convolutional Neural Networks (CNNs)</strong>
<ul>
<li>CNNs specialize in processing data with a grid-like topology (e.g., images).</li>
<li>They detect local patterns using convolutional layers with shared weights.</li>
<li>Neurons in convolutional layers connect only within their receptive fields, not fully connected.</li>
<li>This local connectivity and weight sharing significantly reduce the number of parameters.</li>
</ul></li>
<li><strong>Kernels and Convolution Operations</strong>
<ul>
<li>Kernels (filters) are small matrices that slide over the input data to perform convolutions.</li>
<li>The convolution operation involves element-wise multiplication and summation.</li>
<li>Kernels can be designed to detect specific features like edges and textures.</li>
<li>Feature maps are generated, highlighting where certain features are detected in the input.</li>
</ul></li>
<li><strong>Receptive Field, Padding, and Stride</strong>
<ul>
<li><strong>Receptive Field</strong>: The local region of the input that a neuron is sensitive to.</li>
<li><strong>Padding</strong>: Adding zeros around the input to maintain spatial dimensions after convolution.</li>
<li><strong>Stride</strong>: The step size with which the kernel moves over the input data.</li>
<li>These parameters control the output size and computation in convolutional layers.</li>
</ul></li>
<li><strong>Filters and Feature Maps</strong>
<ul>
<li>Filters are learned during training and are crucial for feature detection.</li>
<li>All neurons in a feature map share the same filter parameters.</li>
<li>This sharing leads to efficient parameter usage and consistent feature detection across the input.</li>
</ul></li>
<li><strong>Convolutional Layers</strong>
<ul>
<li>Perform convolutions followed by adding a bias term and applying an activation function (e.g., ReLU).</li>
<li>The activation function introduces non-linearity, allowing the network to learn complex patterns.</li>
<li>The use of shared weights and biases reduces the total number of parameters.</li>
</ul></li>
<li><strong>Pooling Layers</strong>
<ul>
<li><strong>Purpose</strong>: Reduce the spatial dimensions of feature maps to control overfitting and computation.</li>
<li><strong>Types</strong>:
<ul>
<li><strong>Max Pooling</strong>: Takes the maximum value within a pooling window.</li>
<li><strong>Average Pooling</strong>: Computes the average value within a pooling window.</li>
</ul></li>
<li><strong>Benefits</strong>:
<ul>
<li>Dimensionality reduction leads to fewer parameters and faster computation.</li>
<li>Introduces translation invariance, making the network robust to shifts and distortions.</li>
<li>Helps in hierarchical feature learning by focusing on prominent features.</li>
</ul></li>
</ul></li>
<li><strong>Building CNN Architectures</strong>
<ul>
<li>CNNs are built by stacking convolutional and pooling layers.</li>
<li>Spatial dimensions decrease while the number of feature maps increases in deeper layers.</li>
<li>Often culminates with fully connected layers for classification tasks.</li>
<li>Example architectures can be implemented using frameworks like Keras.</li>
</ul></li>
<li><strong>Hyperparameter Tuning</strong>
<ul>
<li>Key hyperparameters include the number of layers, filters, kernel sizes, strides, and padding.</li>
<li>Proper tuning is essential for achieving optimal model performance.</li>
<li>Overfitting can be controlled using techniques like dropout and regularization.</li>
</ul></li>
<li><strong>Future Directions</strong>
<ul>
<li><strong>Feature Attribution</strong>:
<ul>
<li>Techniques like saliency maps and activation maximization help interpret model decisions.</li>
<li>Essential for applications requiring explainability (e.g., self-driving cars).</li>
</ul></li>
<li><strong>Transfer Learning</strong>:
<ul>
<li>Involves using pre-trained models on new tasks to save time and resources.</li>
<li>Particularly useful when labeled data is scarce.</li>
</ul></li>
</ul></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="future-directions" class="slide level2">
<h2>Future Directions</h2>
<p>When integrating CNNs into your projects, consider exploring the following topics:</p>
<ul>
<li><p><strong>Feature Attribution:</strong> Various techniques are available to visualize what the network has learned. For example, in the context of self-driving cars, it is crucial to ensure that the network focuses on relevant features, avoiding distractions.</p></li>
<li><p><strong>Transfer Learning:</strong> This approach enables the reuse of weights from pre-trained networks, which accelerates the learning process, reduces computational demands, and facilitates network training even with a limited number of examples.</p></li>
</ul>
<aside class="notes">
<p>There are also 1D convolutions, which are often applied in bioinformatics.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="further-reading" class="slide level2 smaller">
<h2>Further Reading</h2>
<div class="columns">
<div class="column" style="width:20%;">
<p><a href="https://mit-press-us.imgix.net/covers/9780262048644.jpg?auto=format&amp;w=298&amp;dpr=2&amp;q=80" class="lightbox" data-gallery="quarto-lightbox-gallery-22"><img data-src="https://mit-press-us.imgix.net/covers/9780262048644.jpg?auto=format&amp;w=298&amp;dpr=2&amp;q=80"></a></p>
</div><div class="column" style="width:80%;">
<ul>
<li><p><a href="http://udlbook.com">Understanding Deep Learning</a> <span class="citation" data-cites="prince2023understanding">(<a href="#/references" role="doc-biblioref" onclick="">Prince 2023</a>)</span> is a recently published textbook focused on the foundational concepts of deep learning.</p></li>
<li><p>It begins with fundamental principles and extends to contemporary topics such as transformers, diffusion models, graph neural networks, autoencoders, adversarial networks, and reinforcement learning.</p></li>
<li><p>The textbook aims to help readers comprehend these concepts without delving excessively into theoretical details.</p></li>
<li><p>It includes sixty-eight Python notebook exercises.</p></li>
<li><p>The book follows a “read-first, pay-later” model.</p></li>
</ul>
</div></div>
</section>
<section id="resources" class="slide level2">
<h2>Resources</h2>
<ul>
<li><strong>A guide to convolution arithmetic for deep learning</strong></li>
<li>Authors: Vincent Dumoulin and Francesco Visin</li>
<li>Last revised: 11 Jan 2018
<ul>
<li><a href="https://arxiv.org/abs/1603.07285">arXiv:1603.07285</a></li>
<li><a href="https://github.com/vdumoulin/conv_arithmetic/">GitHub Repository</a></li>
</ul></li>
</ul>
</section>
<section id="next-lecture" class="slide level2">
<h2>Next lecture</h2>
<ul>
<li>We will introduce solution spaces.</li>
</ul>
</section>
<section id="references" class="slide level2 smaller scrollable">
<h2>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-geron2019" class="csl-entry" role="listitem">
Géron, Aurélien. 2019. <em>Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow</em>. 2nd ed. O’Reilly Media.
</div>
<div id="ref-Geron:2022aa" class="csl-entry" role="listitem">
———. 2022. <em>Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow</em>. 3rd ed. O’Reilly Media, Inc.
</div>
<div id="ref-goodfellow:2016" class="csl-entry" role="listitem">
Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. Adaptive Computation and Machine Learning. MIT Press. <a href="https://dblp.org/rec/books/daglib/0040158">https://dblp.org/rec/books/daglib/0040158</a>.
</div>
<div id="ref-Krizhevsky:2012aa" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. <span>“ImageNet Classification with Deep Convolutional Neural Networks.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by F. Pereira, C. J. Burges, L. Bottou, and K. Q. Weinberger. Vol. 25. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a>.
</div>
<div id="ref-lecun:2015dt" class="csl-entry" role="listitem">
LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. 2015. <span>“Deep Learning.”</span> <em>Nature</em> 521 (7553): 436–44. <a href="https://doi.org/10.1038/nature14539">https://doi.org/10.1038/nature14539</a>.
</div>
<div id="ref-Lecun:1998aa" class="csl-entry" role="listitem">
Lecun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998. <span>“Gradient-Based Learning Applied to Document Recognition.”</span> <em>Proceedings of the IEEE</em> 86 (11): 2278–2324. <a href="https://doi.org/10.1109/5.726791">https://doi.org/10.1109/5.726791</a>.
</div>
<div id="ref-prince2023understanding" class="csl-entry" role="listitem">
Prince, Simon J. D. 2023. <em>Understanding Deep Learning</em>. The MIT Press. <a href="http://udlbook.com">http://udlbook.com</a>.
</div>
<div id="ref-Russell:2020aa" class="csl-entry" role="listitem">
Russell, Stuart, and Peter Norvig. 2020. <em>Artificial Intelligence: <span>A</span> Modern Approach</em>. 4th ed. Pearson. <a href="http://aima.cs.berkeley.edu/">http://aima.cs.berkeley.edu/</a>.
</div>
<div id="ref-Simonyan15" class="csl-entry" role="listitem">
Simonyan, Karen, and Andrew Zisserman. 2015. <span>“Very Deep Convolutional Networks for Large-Scale Image Recognition.”</span> In <em>International Conference on Learning Representations</em>.
</div>
</div>
</section>
<section class="slide level2">

<p>Marcel <strong>Turcotte</strong></p>
<p><a href="mailto:Marcel.Turcotte@uOttawa.ca">Marcel.Turcotte@uOttawa.ca</a></p>
<p>School of Electrical Engineering and <strong>Computer Science</strong> (EE<strong>CS</strong>)</p>
<p>University of Ottawa</p>


</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../../assets/images/uottawa_hor_black.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://turcotte.xyz/teaching/csi-4106">turcotte.xyz/teaching/csi-4106</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/turcotte\.xyz\/teaching\/csi-4106");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    <script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
    (function() {
      let previousOnload = window.onload;
      window.onload = () => {
        if (previousOnload) {
          previousOnload();
        }
        lightboxQuarto.on('slide_before_load', (data) => {
          const { slideIndex, slideNode, slideConfig, player, trigger } = data;
          const href = trigger.getAttribute('href');
          if (href !== null) {
            const imgEl = window.document.querySelector(`a[href="${href}"] img`);
            if (imgEl !== null) {
              const srcAttr = imgEl.getAttribute("src");
              if (srcAttr && srcAttr.startsWith("data:")) {
                slideConfig.href = srcAttr;
              }
            }
          } 
        });
      
        lightboxQuarto.on('slide_after_load', (data) => {
          const { slideIndex, slideNode, slideConfig, player, trigger } = data;
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(slideNode);
          }
        });
      
      };
      
    })();
              </script>
    

<script src="https://platform.twitter.com/widgets.js"></script>
</body></html>