<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.24">

  <meta name="author" content="Marcel Turcotte">
  <title>CSI 4106 - Fall 2025 – Cross-entropy, geometric interpretation, and implementation</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-534cd8e3a96973385dffff3f4709048d.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="Cross-entropy, geometric interpretation, and implementation – CSI 4106 - Fall 2025">
<meta property="og:description" content="CSI 4106 - Fall 2025">
<meta property="og:site_name" content="CSI 4106 - Fall 2025">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Cross-entropy, geometric interpretation, and implementation</h1>
  <p class="subtitle">CSI 4106 - Fall 2025</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Marcel Turcotte 
</div>
</div>
</div>

  <p class="date">Version: Sep 18, 2025 10:49</p>
</section>
<section>
<section id="preamble" class="title-slide slide level1 center">
<h1>Preamble</h1>

</section>
<section id="message-of-the-day" class="slide level2 scrollable">
<h2>Message of the Day</h2>
<iframe data-external="1" src="https://www.youtube.com/embed/I_cvKK7LanI" width="889" height="500" title="AI’s 'Significant Effect' on Entry-Level Work" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<aside class="notes">
<p>TIME conducted interviews with the authors of a recent report from the <strong>Stanford Digital Economy Lab</strong>, titled “Canaries in the Coal Mine? Six Facts about the Recent Employment Effects of Artificial Intelligence.” The report is available <a href="https://www.rivista.ai/wp-content/uploads/2025/09/1756729755699.pdf">here</a> and here is the abstract:</p>
<blockquote>
<p>This paper examines changes in the labor market for occupations exposed to generative artificial intelligence using high-frequency administrative data from the largest payroll software provider in the United States. We present six facts that characterize these shifts. We find that since the widespread adoption of generative AI, early-career workers (ages 22-25) in the most AI-exposed occupations have experienced a 13 percent relative decline in employment even after controlling for firm-level shocks. In contrast, employment for workers in less exposed fields and more experienced workers in the same occupations has remained stable or continued to grow. We also find that adjustments occur primarily through employment rather than compensation. Furthermore, employment declines are concentrated in occupations where AI is more likely to automate, ratherthanaugment, humanlabor. Ourresultsarerobusttoalternativeexplanations, such as excluding technology-related firms and excluding occupations amenable to remote work. These six facts provide early, large-scale evidence consistent with the hypothesis that the AI revolution is beginning to have a significant and disproportionate impact on entry-level workers in the American labor market.</p>
</blockquote>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p><a href="https://www.youtube.com/watch?v=I_cvKK7LanI">AI’s “Significant Effect” on Entry-Level Work</a>, TIME, 2025-09-05. (13m 55s)</p>
</div></aside></section>
<section id="message-of-the-day-continued" class="slide level2">
<h2>Message of the Day (continued)</h2>
<iframe data-external="1" src="https://www.youtube.com/embed/p_kF_SDB0-c" width="889" height="500" title="How AI is changing the job market" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<aside><div>
<p><a href="https://www.youtube.com/watch?v=p_kF_SDB0-c">How AI is changing the job market</a>, What in the World podcast, BBC World Service, 2025-09-16.</p>
</div></aside></section>
<section id="learning-outcomes" class="slide level2">
<h2>Learning Outcomes</h2>
<p>By the end of this presentation, you should be able to:</p>
<ul>
<li><strong>Differentiate</strong> between MSE and cross-entropy as loss functions.</li>
<li><strong>Relate</strong> maximum likelihood estimation to parameter learning in logistic regression.</li>
<li><strong>Interpret</strong> the geometric view of logistic regression as a linear decision boundary.</li>
<li><strong>Implement</strong> logistic regression with gradient descent on simple data.</li>
</ul>
</section></section>
<section>
<section id="linear-regression" class="title-slide slide level1 center">
<h1>Linear Regression</h1>

</section>
<section id="problem" class="slide level2">
<h2>Problem</h2>
<ul>
<li class="fragment"><strong>General Case:</strong> <span class="math inline">\(P(y = k \mid x, \theta)\)</span>, where <span class="math inline">\(k\)</span> is a class label.</li>
<li class="fragment"><strong>Binary Case</strong>: <span class="math inline">\(y \in {0,1}\)</span>
<ul>
<li class="fragment"><strong>Predict</strong> <span class="math inline">\(P(y = 1 \mid x, \theta)\)</span></li>
</ul></li>
</ul>

<aside><div>
<p>For a new instance <span class="math inline">\(x_{\textrm{new}}\)</span>, determine the probability that it belongs to class <span class="math inline">\(k\)</span>, denoted as <span class="math inline">\(P(y = k \mid x_{\textrm{new}}, \theta)\)</span>.</p>
</div></aside></section>
<section id="logistic-regression" class="slide level2">
<h2>Logistic Regression</h2>
<p>The <strong>Logistic Regression</strong> model is defined as:</p>
<p><span class="math display">\[
  h_\theta(x_i) = \sigma(\theta x_i) = \frac{1}{1+e^{- \theta x_i}}
  \]</span></p>
<ul>
<li><p><strong>Predictions</strong> are made as follows:</p></li>
<li><p><span class="math inline">\(y_i = 0\)</span>, if <span class="math inline">\(h_\theta(x_i) &lt; 0.5\)</span></p></li>
<li><p><span class="math inline">\(y_i = 1\)</span>, if <span class="math inline">\(h_\theta(x_i) \geq 0.5\)</span></p></li>
</ul>

<aside class="notes">
<p>In the previous lecture, we considered an example wherein <strong>logistic regression</strong> was used to classify <strong>handwritten digits</strong>.</p>
<ul>
<li>The classification problem was addressed using a <strong>one-vs-rest</strong> strategy, which involved training ten separate logistic regression models, each dedicated to recognizing a specific digit.</li>
<li>Each model consisted of <strong>65 parameters</strong>: <strong>one bias</strong> term and <strong>64 weights</strong>. Each <strong>weight</strong> corresponded to a <strong>pixel</strong> (or <strong>attribute</strong>) of a <span class="math inline">\(64 \times 64\)</span> pixel image.</li>
<li>This method demonstrated an excellent performance, achieving an overall accuracy of 0.97.</li>
<li>Analyzing the weights provided insights into the areas of the image to which the model was most responsive (what does it pay attention to?).</li>
</ul>
<p>The model presented above is expressed in its vectorized form, allowing it to be applied to problems involving multiple attributes. In the context of recognizing handwritten digits, the model utilizes 64 attributes, corresponding to individual pixels. The function <span class="math inline">\(\sigma\)</span> employed in this model is the logistic, or sigmoid, function.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p>The problem is formulated as a <strong>binary classification</strong> task, wherein the model presumes that the classes are separable by a <strong>linear function</strong> within the feature space.</p>
</div></aside></section></section>
<section>
<section id="loss-function" class="title-slide slide level1 center">
<h1>Loss Function</h1>

</section>
<section id="model-overview" class="slide level2">
<h2>Model Overview</h2>
<ul>
<li><p>Our model is expressed in a vectorized form as:</p>
<p><span class="math display">\[
h_\theta(x_i) = \sigma(\theta x_i) = \frac{1}{1+e^{- \theta x_i}}
\]</span></p></li>
<li><p><strong>Prediction</strong>:</p>
<ul>
<li>Assign <span class="math inline">\(y_i = 0\)</span>, if <span class="math inline">\(h_\theta(x_i) &lt; 0.5\)</span>; <span class="math inline">\(y_i = 1\)</span>, if <span class="math inline">\(h_\theta(x_i) \geq 0.5\)</span></li>
</ul></li>
</ul>
<ul>
<li class="fragment"><p>The parameter vector <span class="math inline">\(\theta\)</span> is optimized using <strong>gradient descent</strong>.</p></li>
<li class="fragment"><p>Which <strong>loss function</strong> should be used and why?</p></li>
</ul>
<aside class="notes">
<p>In logistic regression, the output is regarded as a probability, with particular emphasis on the interpretation process.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="remarks" class="slide level2">
<h2>Remarks</h2>
<ul>
<li class="fragment"><p>In constructing machine learning models with libraries like <code>scikit-learn</code> or <code>keras</code>, one has to <strong>select a loss function</strong> or <strong>accept the default one</strong>.</p></li>
<li class="fragment"><p>Initially, the <strong>terminology can be confusing</strong>, as identical functions may be referenced by various names.</p></li>
<li class="fragment"><p>Our aim is to <strong>elucidate these complexities</strong>.</p></li>
<li class="fragment"><p>It is actually <strong>not that complicated</strong>!</p></li>
</ul>
</section>
<section id="parameter-estimation" class="slide level2">
<h2>Parameter Estimation</h2>
<ul>
<li class="fragment"><p>Logistic regression is <strong>statistical model</strong>.</p></li>
<li class="fragment"><p>Its output is <span class="math inline">\(\hat{y} = P(y = 1 | x, \theta)\)</span>.</p></li>
<li class="fragment"><p><span class="math inline">\(P(y = 0 | x, \theta) = 1 - \hat{y}\)</span>.</p></li>
<li class="fragment"><p>Assumes that <span class="math inline">\(y\)</span> values come from a <strong>Bernoulli distribution</strong>.</p></li>
<li class="fragment"><p><span class="math inline">\(\theta\)</span> is commonly found by <strong>Maximum Likelihood Estimation</strong>.</p></li>
</ul>

<aside><div>
<p>The expressions <span class="math inline">\(\hat{y}\)</span>, <span class="math inline">\(h_\theta(x_i)\)</span>, and <span class="math inline">\(\sigma(\theta x_i)\)</span> represent the same concept, albeit at varying levels of abstraction and specificity.</p>
</div></aside></section>
<section id="parameter-estimation-1" class="slide level2">
<h2>Parameter Estimation</h2>
<p><strong>Maximum Likelihood Estimation (MLE)</strong> is a statistical method used to estimate the parameters of a probabilistic model.</p>
<p>It identifies the parameter values that maximize the <strong>likelihood function</strong>, which measures how well the model explains the observed data.</p>
</section>
<section id="likelihood-function" class="slide level2">
<h2>Likelihood Function</h2>
<p>Assuming the <span class="math inline">\(y\)</span> values are <em>independent and identically distributed (i.i.d.)</em>, the <strong>likelihood function</strong> is expressed as the <strong>product of individual probabilities</strong>.</p>
<p>In other words, given our data, <span class="math inline">\(\{(x_i, y_i)\}_{i=1}^N\)</span>, the likelihood function is given by this equation. <span class="math display">\[
\mathcal{L}(\theta) = \prod_{i=1}^{N} P(y_i \mid x_i, \theta)
\]</span></p>
</section>
<section id="maximum-likelihood" class="slide level2">
<h2>Maximum Likelihood</h2>
<p><span class="math display">\[
  \hat{\theta} = \underset{\theta \in \Theta}{\arg \max} \mathcal{L}(\theta) =  \underset{\theta \in \Theta}{\arg \max}  \prod_{i=1}^{N} P(y_i \mid x_i, \theta)
\]</span></p>
<ul>
<li class="fragment"><p><strong>Observations</strong>:</p>
<ol type="1">
<li class="fragment"><strong>Maximizing</strong> a function is equivalent to <strong>minimizing its negative.</strong></li>
<li class="fragment">The <strong>logarithm of a product</strong> equals the <strong>sum of its logarithms</strong>.</li>
</ol></li>
</ul>
</section>
<section id="negative-log-likelihood" class="slide level2 smaller">
<h2>Negative Log-Likelihood</h2>
<p><strong>Maximum likelihood</strong> <span class="math display">\[
  \hat{\theta} = \underset{\theta \in \Theta}{\arg \max} \mathcal{L}(\theta) = \underset{\theta \in \Theta}{\arg \max}  \prod_{i=1}^{N} P(y_i \mid x_i, \theta)
\]</span></p>
<p>becomes <strong>negative log-likelihood</strong></p>
<p><span class="math display">\[
\hat{\theta} = \underset{\theta \in \Theta}{\arg \min} - \log \mathcal{L(\theta)} = \underset{\theta \in \Theta}{\arg \min} - \log \prod_{i=1}^{N} P(y_i \mid x_i, \theta) = \underset{\theta \in \Theta}{\arg \min} - \sum_{i=1}^{N} \log P(y_i \mid x_i, \theta)
\]</span></p>
</section>
<section id="mathematical-reformulation" class="slide level2">
<h2>Mathematical Reformulation</h2>
<p>For binary outcomes, the probability <span class="math inline">\(P(y \mid x, \theta)\)</span> is:</p>
<p><span class="math display">\[
P(y \mid x, \theta) =
\begin{cases}
\sigma(\theta x), &amp; \text{if}\ y = 1 \\
1 - \sigma(\theta x), &amp; \text{if}\ y = 0
\end{cases}
\]</span></p>
<div class="fragment">
<p>This can be compactly expressed as:</p>
<p><span class="math display">\[
P(y \mid x, \theta) = \sigma(\theta x)^y (1 - \sigma(\theta x))^{1-y}
\]</span></p>

</div>
<aside><div>
<p>This <strong>“mathematical hack”</strong> validates the rationale for the <strong>label encoding</strong>.</p>
</div></aside></section>
<section id="loss-function-1" class="slide level2">
<h2>Loss Function</h2>
<p>We are now ready to write our <strong>loss function</strong>.</p>
<p><span class="math display">\[
J(\theta) = - \log \mathcal{L(\theta)} = - \sum_{i=1}^{N} \log P(y_i \mid x_i, \theta)
\]</span> where <span class="math inline">\(P(y \mid x, \theta) = \sigma(\theta x)^y (1 - \sigma(\theta x))^{1-y}\)</span>.</p>
<p>Consequently, <span class="math display">\[
J(\theta) = - \sum_{i=1}^{N} \log [ \sigma(\theta x_i)^{y_i} (1 - \sigma(\theta x_i))^{1-y_i} ]
\]</span></p>
</section>
<section id="loss-function-continued" class="slide level2">
<h2>Loss Function (continued)</h2>
<p>Simplifying the equation. <span class="math display">\[
J(\theta) = - \sum_{i=1}^{N} \log [ \sigma(\theta x_i)^{y_i} (1 - \sigma(\theta x_i))^{1-y_i} ]
\]</span> by distributing the <span class="math inline">\(\log\)</span> into the square parenthesis. <span class="math display">\[
J(\theta) = - \sum_{i=1}^{N} [ \log \sigma(\theta x_i)^{y_i} + \log (1 - \sigma(\theta x_i))^{1-y_i} ]
\]</span></p>
</section>
<section id="loss-function-continued-1" class="slide level2">
<h2>Loss Function (continued)</h2>
<p>Simplifying the equation further. <span class="math display">\[
J(\theta) = - \sum_{i=1}^{N} [ \log \sigma(\theta x_i)^{y_i} + \log (1 - \sigma(\theta x_i))^{1-y_i} ]
\]</span> by moving the exponents in front of the <span class="math inline">\(\log\)</span>s.</p>
<p><span class="math display">\[
J(\theta) = - \sum_{i=1}^{N} [ y_i \log \sigma(\theta x_i) + (1-y_i) \log (1 - \sigma(\theta x_i)) ]
\]</span></p>

<aside><div>
<p>The rationale for these additional simplifications will be elucidated shortly.</p>
</div></aside></section>
<section id="one-more-thing" class="slide level2">
<h2>One More Thing</h2>
<ul>
<li>Decision tree algorithms often employ <strong>entropy</strong>, a measure from <strong>information theory</strong>, to evaluate the quality of splits or partitions in decision rules.</li>
<li>Entropy quantifies the uncertainty or impurity associated with the potential outcomes of a random variable.</li>
</ul>
</section>
<section id="entropy" class="slide level2">
<h2>Entropy</h2>
<p><strong>Entropy</strong> in information theory quantifies the <strong>uncertainty</strong> or unpredictability of a random variable’s possible outcomes. It measures the average amount of information produced by a stochastic source of data and is typically expressed in bits for binary systems. The entropy <span class="math inline">\(H\)</span> of a discrete random variable <span class="math inline">\(X\)</span> with possible outcomes <span class="math inline">\(\{x_1, x_2, \ldots, x_n\}\)</span> and probability mass function <span class="math inline">\(P(X)\)</span> is given by:</p>
<p><span class="math display">\[
H(X) = -\sum_{i=1}^n P(x_i) \log_2 P(x_i)
\]</span></p>
</section>
<section id="cross-entropy" class="slide level2">
<h2>Cross-Entropy</h2>
<p><strong>Cross-entropy</strong> quantifies the <strong>difference between two probability distributions</strong>, typically the <strong>true distribution</strong> and a <strong>predicted distribution</strong>.</p>
<p><span class="math display">\[
H(p, q) = -\sum_{i} p(x_i) \log q(x_i)
\]</span> where <span class="math inline">\(p(x_i)\)</span> is the true probability distribution, and <span class="math inline">\(q(x_i)\)</span> is the predicted probability distribution.</p>
</section>
<section id="cross-entropy-1" class="slide level2">
<h2>Cross-Entropy</h2>
<ul>
<li>Consider <span class="math inline">\(y\)</span> as the true probability distribution and <span class="math inline">\(\hat{y}\)</span>&nbsp;as the predicted probability distribution.</li>
<li>Cross-entropy quantifies the <strong>discrepancy</strong> between these two distributions.</li>
</ul>
</section>
<section id="cross-entropy-2" class="slide level2 smaller">
<h2>Cross-Entropy</h2>
<p>Consider the <strong>negative log-likelihood loss</strong> function:</p>
<p><span class="math display">\[
J(\theta) = - \sum_{i=1}^{N} \left[ y_i \log \sigma(\theta x_i) + (1-y_i) \log (1 - \sigma(\theta x_i)) \right]
\]</span></p>
<p>By substituting <span class="math inline">\(\sigma(\theta x_i)\)</span> with <span class="math inline">\(\hat{y_i}\)</span>, the function becomes:</p>
<p><span class="math display">\[
J(\theta) = - \sum_{i=1}^{N} \left[ y_i \log \hat{y_i} + (1-y_i) \log (1 - \hat{y_i}) \right]
\]</span></p>
<p>This expression illustrates that the <strong>negative log-likelihood</strong> is optimized by minimizing the <strong>cross-entropy</strong>.</p>

<aside class="notes">
<p>Interpret the final equation as applying to all examples from 1 to <span class="math inline">\(N\)</span> and all classes from 1 to <span class="math inline">\(k\)</span>. Here, <span class="math inline">\(k=0\)</span> because we are addressing a binary classification problem.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p><strong>Cross-entropy</strong>, <strong>log loss</strong>, and n<strong>egative log-likelihood</strong> refer to the same concept.</p>
</div></aside></section>
<section id="for-each-example" class="slide level2">
<h2>For Each Example</h2>
<div id="31deb6c5" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a></a></span>
<span id="cb1-4"><a></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-5"><a></a></span>
<span id="cb1-6"><a></a><span class="co"># Generate an array of p values from just above 0 to 1</span></span>
<span id="cb1-7"><a></a>p_values <span class="op">=</span> np.linspace(<span class="fl">0.001</span>, <span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="cb1-8"><a></a></span>
<span id="cb1-9"><a></a><span class="co"># Compute the natural logarithm of each p value</span></span>
<span id="cb1-10"><a></a>ln_p_values <span class="op">=</span> <span class="op">-</span> np.log(p_values)</span>
<span id="cb1-11"><a></a></span>
<span id="cb1-12"><a></a><span class="co"># Plot the graph</span></span>
<span id="cb1-13"><a></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">4</span>))</span>
<span id="cb1-14"><a></a>plt.plot(p_values, ln_p_values, label<span class="op">=</span><span class="vs">r'</span><span class="dv">$</span><span class="vs">-</span><span class="er">\</span><span class="vs">log</span><span class="kw">(</span><span class="er">\</span><span class="vs">hat{y}</span><span class="kw">)</span><span class="dv">$</span><span class="vs">'</span>, color<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb1-15"><a></a></span>
<span id="cb1-16"><a></a><span class="co"># Add labels and title</span></span>
<span id="cb1-17"><a></a>plt.xlabel(<span class="vs">r'</span><span class="dv">$</span><span class="er">\</span><span class="vs">hat{y}</span><span class="dv">$</span><span class="vs">'</span>)</span>
<span id="cb1-18"><a></a>plt.ylabel(<span class="vs">r'J'</span>)</span>
<span id="cb1-19"><a></a>plt.title(<span class="vs">r'Graph of </span><span class="dv">$</span><span class="vs">-</span><span class="er">\</span><span class="vs">log</span><span class="kw">(</span><span class="er">\</span><span class="vs">hat{y}</span><span class="kw">)</span><span class="dv">$</span><span class="vs"> for </span><span class="dv">$</span><span class="er">\</span><span class="vs">hat{y}</span><span class="dv">$</span><span class="vs"> from 0 to 1'</span>)</span>
<span id="cb1-20"><a></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb1-21"><a></a>plt.axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'gray'</span>, lw<span class="op">=</span><span class="fl">0.5</span>)  <span class="co"># Add horizontal line at y=0</span></span>
<span id="cb1-22"><a></a>plt.axvline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'gray'</span>, lw<span class="op">=</span><span class="fl">0.5</span>)  <span class="co"># Add vertical line at x=0</span></span>
<span id="cb1-23"><a></a></span>
<span id="cb1-24"><a></a><span class="co"># Display the plot</span></span>
<span id="cb1-25"><a></a>plt.legend()</span>
<span id="cb1-26"><a></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-2-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img data-src="slides_files/figure-revealjs/cell-2-output-1.png" class="quarto-figure quarto-figure-center" width="427" height="384"></a></p>
</figure>
</div>
</div>
</div>

<aside class="notes">
<p>For each example:</p>
<ul>
<li>Only one of the two terms in the summation is not zero.</li>
<li><span class="math inline">\(1 - \hat{y_i}\)</span> is <span class="math inline">\(P(y = 0 \mid x, \theta)\)</span>.</li>
<li>As <span class="math inline">\(\hat{y_i}\)</span> tends to 1.0, <span class="math inline">\(- \log(\hat{y})\)</span> tends to zero.</li>
<li>As <span class="math inline">\(\hat{y_i}\)</span> tends to 0.0, indicating an incorrect prediction, <span class="math inline">\(- \log(\hat{y})\)</span> tends to <span class="math inline">\(\infty\)</span>.</li>
<li>This substantial penalty allows cross-entropy loss to converge more quickly than mean squared error.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p><span class="math display">\[
J(\theta) = - \sum_{i=1}^{N} \left[ y_i \log \hat{y_i} + (1-y_i) \log (1 - \hat{y_i}) \right]
\]</span></p>
</div></aside></section>
<section id="remarks-1" class="slide level2">
<h2>Remarks</h2>
<ul>
<li><p>Cross-entropy loss is particularly well-suited for <strong>probabilistic classification tasks</strong> due to its alignment with maximum likelihood estimation.</p></li>
<li><p>In logistic regression, <strong>cross-entropy loss preserves convexity</strong>, contrasting with the non-convex nature of mean squared error (MSE)<sup>1</sup>.</p></li>
</ul>

<aside class="notes">
<p>If you train <strong>logistic regression</strong> with the <strong>mean squared error (MSE)</strong> loss:</p>
<ul>
<li>The composition of the <strong>sigmoid</strong> (nonlinear, S-shaped) with the <strong>quadratic loss</strong> produces a <strong>non-convex objective</strong>.</li>
<li>This leads to multiple local minima and poor optimization behavior.</li>
<li>By contrast, using the <strong>log-loss (cross-entropy)</strong> yields a <strong>convex objective</strong> in the parameters, making optimization well-behaved with gradient methods.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p>We will revisite cross-entropy loss when studying deep learning, especially in conjunction with the softmax function.</p>
</div><ol class="aside-footnotes"><li id="fn1"><p>In linear regression, mean squared error loss is convex.</p></li></ol></aside></section>
<section id="remarks-2" class="slide level2">
<h2>Remarks</h2>
<ul>
<li><p>For classification problems, cross-entropy loss often achieves <strong>faster convergence</strong> compared to MSE, enhancing model efficiency.</p></li>
<li><p>Within deep learning architectures, MSE can exacerbate the <strong>vanishing gradient problem</strong>, an issue we will address in a subsequent discussion.</p></li>
</ul>
</section>
<section id="why-not-mse-as-a-loss-function" class="slide level2">
<h2>Why not MSE as a Loss Function?</h2>
<iframe data-external="1" src="https://www.youtube.com/embed/m0ZeT1EWjjI" width="889" height="500" title="Cross Entropy vs. MSE as Cost Function for Logistic Regression for Classification [Lecture 2.5]" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</section>
<section id="what-is-the-difference" class="slide level2">
<h2>What is the Difference?</h2>
<iframe data-external="1" src="https://www.youtube.com/embed/ziq967YrSsc" width="889" height="500" title="What is the difference between negative log likelihood and cross entropy? (in neural networks)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</section></section>
<section>
<section id="geometric-interpretation" class="title-slide slide level1 center">
<h1>Geometric Interpretation</h1>

</section>
<section id="geometric-interpretation-1" class="slide level2">
<h2>Geometric Interpretation</h2>
<ul>
<li class="fragment"><p><strong>Do you</strong> recognize this equation? <span class="math display">\[
w_1 x_1 + w_2 x_2 + \ldots + w_D x_2
\]</span></p></li>
<li class="fragment"><p>This is the <strong>dot product</strong> of <span class="math inline">\(\mathbf{w}\)</span> and <span class="math inline">\(\mathbf{x}\)</span>, <span class="math inline">\(\mathbf{w} \cdot \mathbf{x}\)</span>.</p></li>
<li class="fragment"><p>What is the <strong>geometric interpretation</strong> of the dot product?</p></li>
</ul>
<div class="fragment">
<p><span class="math display">\[
\mathbf{w} \cdot \mathbf{x} = \|\mathbf{w}\| \|\mathbf{x}\| \cos \theta
\]</span></p>

</div>
<aside><div>
<p>In certain contexts, it is advantageous to use <span class="math inline">\(w\)</span> in place of <span class="math inline">\(\theta\)</span>.</p>
</div></aside></section>
<section id="geometric-interpretation-2" class="slide level2">
<h2>Geometric Interpretation</h2>
<p><span class="math display">\[
\mathbf{w} \cdot \mathbf{x} = \|\mathbf{w}\| \|\mathbf{x}\| \cos \theta
\]</span></p>
<ul>
<li><p>The <strong>dot product</strong> determines the <strong>angle <span class="math inline">\((\theta)\)</span> between vectors</strong>.</p></li>
<li><p>It <strong>quantifies</strong> how much one vector extends in the direction of another.</p></li>
<li><p>Its value is zero, if the vectors are <strong>perpendicular</strong> <span class="math inline">\((\theta = 90^\circ)\)</span>.</p></li>
</ul>
</section>
<section id="geometric-interpretation-3" class="slide level2">
<h2>Geometric Interpretation</h2>
<ul>
<li class="fragment"><p><strong>Logistic regression</strong> uses a linear combination of the input features, <span class="math inline">\(\mathbf{w} \cdot \mathbf{x} + b\)</span>, as the argument to the sigmoid (logistic) function.</p></li>
<li class="fragment"><p>Geometrically, <span class="math inline">\(\mathbf{w}\)</span> can be viewed as a <strong>vector normal to a hyperplane in the feature space</strong>, and any point <span class="math inline">\(\mathbf{x}\)</span> is projected onto <span class="math inline">\(\mathbf{w}\)</span> via the dot product <span class="math inline">\(\mathbf{w} \cdot \mathbf{x}\)</span>.</p></li>
</ul>
</section>
<section id="geometric-interpretation-4" class="slide level2">
<h2>Geometric Interpretation</h2>
<ul>
<li class="fragment"><p>The <strong>decision boundary</strong> is where this linear combination equals zero, i.e., <span class="math inline">\(\mathbf{w} \cdot \mathbf{x} + b = 0\)</span>.</p></li>
<li class="fragment"><p>Points on one side of the boundary have a <strong>positive dot product</strong> and are more likely to be classified as the positive class (1).</p></li>
<li class="fragment"><p>Points on the other side have a <strong>negative dot product</strong> and are more likely to be in the opposite class (0).</p></li>
<li class="fragment"><p>The <strong>sigmoid function</strong> simply turns this <strong>signed distance</strong> into a <strong>probability</strong> between 0 and 1.</p></li>
</ul>
</section>
<section id="logistic-function" class="slide level2 smaller">
<h2>Logistic Function</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><span class="math display">\[
  \sigma(t) = \frac{1}{1+e^{-t}}
\]</span></p>
<ul>
<li>As <span class="math inline">\(t \to \infty\)</span>, <span class="math inline">\(e^{-t} \to 0\)</span>, so <span class="math inline">\(\sigma(t) \to 1\)</span>.</li>
<li>As <span class="math inline">\(t \to -\infty\)</span>, <span class="math inline">\(e^{-t} \to \infty\)</span>, making the denominator approach infinity, so <span class="math inline">\(\sigma(t) \to 0\)</span>.</li>
<li>When <span class="math inline">\(t = 0\)</span>, <span class="math inline">\(e^{-t} = 0\)</span>, resulting in a denominator of 2, so <span class="math inline">\(\sigma(t) = 0.5\)</span>.</li>
</ul>
</div><div class="column" style="width:50%;">
<div id="09951989" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a><span class="co"># Sigmoid function</span></span>
<span id="cb2-2"><a></a><span class="kw">def</span> sigmoid(t):</span>
<span id="cb2-3"><a></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>t))</span>
<span id="cb2-4"><a></a></span>
<span id="cb2-5"><a></a><span class="co"># Generate t values</span></span>
<span id="cb2-6"><a></a>t <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">1000</span>)</span>
<span id="cb2-7"><a></a></span>
<span id="cb2-8"><a></a><span class="co"># Compute y values for the sigmoid function</span></span>
<span id="cb2-9"><a></a>sigma <span class="op">=</span> sigmoid(t)</span>
<span id="cb2-10"><a></a></span>
<span id="cb2-11"><a></a><span class="co"># Create a figure</span></span>
<span id="cb2-12"><a></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb2-13"><a></a>ax.plot(t, sigma, color<span class="op">=</span><span class="st">'blue'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)  <span class="co"># Keep the curve opaque</span></span>
<span id="cb2-14"><a></a></span>
<span id="cb2-15"><a></a><span class="co"># Draw vertical axis at x = 0</span></span>
<span id="cb2-16"><a></a>ax.axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-17"><a></a></span>
<span id="cb2-18"><a></a><span class="co"># Add labels on the vertical axis</span></span>
<span id="cb2-19"><a></a>ax.set_yticks([<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="fl">1.0</span>])</span>
<span id="cb2-20"><a></a></span>
<span id="cb2-21"><a></a><span class="co"># Add labels to the axes</span></span>
<span id="cb2-22"><a></a>ax.set_xlabel(<span class="st">'t'</span>)</span>
<span id="cb2-23"><a></a>ax.set_ylabel(<span class="vs">r'</span><span class="dv">$\s</span><span class="vs">igma</span><span class="kw">(</span><span class="vs">t</span><span class="kw">)</span><span class="dv">$</span><span class="vs">'</span>)</span>
<span id="cb2-24"><a></a></span>
<span id="cb2-25"><a></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb2-26"><a></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-3-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img data-src="slides_files/figure-revealjs/cell-3-output-1.png" class="quarto-figure quarto-figure-center" width="813" height="429"></a></p>
</figure>
</div>
</div>
</div>
</div></div>
</section>
<section id="whats-special-about-e" class="slide level2 smaller">
<h2>What’s special about e?</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><span class="math display">\[
  \sigma(t) = \frac{1}{1+e^{-t}}
\]</span></p>
<ul>
<li class="fragment"><p>Instead of <span class="math inline">\(e\)</span>, we might have used another constant, say 2.</p></li>
<li class="fragment"><p><strong>Derivative Simplicity</strong>: For the logistic function $(x) = $, the derivative simplifies to $’(x) = (x)(1 - (x))$. This elegant form arises because the exponential base $e$ has the unique property that $ e^x = e^x$, avoiding an extra multiplicative constant.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<div id="0e0ede70" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a><span class="im">import</span> math</span>
<span id="cb3-2"><a></a></span>
<span id="cb3-3"><a></a><span class="kw">def</span> logistic(x, e):</span>
<span id="cb3-4"><a></a>    <span class="co">"""Compute a modified logistic function using b rather than e."""</span></span>
<span id="cb3-5"><a></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.power(e, <span class="op">-</span>x))</span>
<span id="cb3-6"><a></a></span>
<span id="cb3-7"><a></a><span class="co"># Define a range for x values.</span></span>
<span id="cb3-8"><a></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">400</span>)</span>
<span id="cb3-9"><a></a></span>
<span id="cb3-10"><a></a><span class="co"># Plot 1: Varying e.</span></span>
<span id="cb3-11"><a></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb3-12"><a></a>e_values <span class="op">=</span> [<span class="dv">2</span>, math.e, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>]  <span class="co"># different steepness values</span></span>
<span id="cb3-13"><a></a></span>
<span id="cb3-14"><a></a><span class="cf">for</span> e <span class="kw">in</span> e_values:</span>
<span id="cb3-15"><a></a>    plt.plot(x, logistic(x, e), label<span class="op">=</span><span class="ss">f'e = </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb3-16"><a></a>plt.title(<span class="st">'Effect of Varying e'</span>)</span>
<span id="cb3-17"><a></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb3-18"><a></a>plt.ylabel(<span class="vs">r'</span><span class="dv">$</span><span class="ch">\f</span><span class="vs">rac</span><span class="op">{1}</span><span class="vs">{1</span><span class="op">+</span><span class="vs">e</span><span class="dv">^</span><span class="vs">{-x}}</span><span class="dv">$</span><span class="vs">'</span>)</span>
<span id="cb3-19"><a></a>plt.legend()</span>
<span id="cb3-20"><a></a>plt.grid(<span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-4-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img data-src="slides_files/figure-revealjs/cell-4-output-1.png" class="quarto-figure quarto-figure-center" width="671" height="523"></a></p>
</figure>
</div>
</div>
</div>
</div></div>
<aside class="notes">
<p>In the context of logistic regression, the choice of the mathematical constant <span class="math inline">\(e\)</span> is not arbitrary but is supported by several compelling mathematical justifications. These justifications primarily relate to the harmonious integration of the logistic function with other mathematical frameworks. Although our primary focus was to visually demonstrate the potential implications of substituting a different constant, the inherent advantages of using <span class="math inline">\(e\)</span> become evident upon closer examination of its mathematical properties and how they facilitate seamless integration with existing theories and models.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="varying-w" class="slide level2 smaller">
<h2>Varying w</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><span class="math display">\[
  \sigma(wx + b)
\]</span></p>
</div><div class="column" style="width:50%;">
<div id="d528584b" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a><span class="kw">def</span> logistic(x, w, b):</span>
<span id="cb4-2"><a></a>    <span class="co">"""Compute the logistic function with parameters w and b."""</span></span>
<span id="cb4-3"><a></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>(w <span class="op">*</span> x <span class="op">+</span> b)))</span>
<span id="cb4-4"><a></a></span>
<span id="cb4-5"><a></a><span class="co"># Define a range for x values.</span></span>
<span id="cb4-6"><a></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">400</span>)</span>
<span id="cb4-7"><a></a></span>
<span id="cb4-8"><a></a><span class="co"># Plot 1: Varying w (steepness) with b fixed at 0.</span></span>
<span id="cb4-9"><a></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb4-10"><a></a>w_values <span class="op">=</span> [<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">5</span>]  <span class="co"># different steepness values</span></span>
<span id="cb4-11"><a></a>b <span class="op">=</span> <span class="dv">0</span>  <span class="co"># fixed bias</span></span>
<span id="cb4-12"><a></a></span>
<span id="cb4-13"><a></a><span class="cf">for</span> w <span class="kw">in</span> w_values:</span>
<span id="cb4-14"><a></a>    plt.plot(x, logistic(x, w, b), label<span class="op">=</span><span class="ss">f'w = </span><span class="sc">{</span>w<span class="sc">}</span><span class="ss">, b = </span><span class="sc">{</span>b<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb4-15"><a></a>plt.title(<span class="st">'Effect of Varying w (with b = 0)'</span>)</span>
<span id="cb4-16"><a></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb4-17"><a></a>plt.ylabel(<span class="vs">r'</span><span class="dv">$\s</span><span class="vs">igma</span><span class="kw">(</span><span class="vs">wx</span><span class="op">+</span><span class="vs">b</span><span class="kw">)</span><span class="dv">$</span><span class="vs">'</span>)</span>
<span id="cb4-18"><a></a>plt.legend()</span>
<span id="cb4-19"><a></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb4-20"><a></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-5-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img data-src="slides_files/figure-revealjs/cell-5-output-1.png" class="quarto-figure quarto-figure-center" width="664" height="523"></a></p>
</figure>
</div>
</div>
</div>
</div></div>
</section>
<section id="varying-b" class="slide level2 smaller">
<h2>Varying b</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><span class="math display">\[
  \sigma(wx + b)
\]</span></p>
</div><div class="column" style="width:50%;">
<div id="f2b38899" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a></a><span class="co"># Plot 2: Varying b (horizontal shift) with w fixed at 1.</span></span>
<span id="cb5-2"><a></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb5-3"><a></a>w <span class="op">=</span> <span class="dv">1</span>  <span class="co"># fixed steepness</span></span>
<span id="cb5-4"><a></a>b_values <span class="op">=</span> [<span class="op">-</span><span class="dv">5</span>, <span class="op">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">5</span>]  <span class="co"># different bias values</span></span>
<span id="cb5-5"><a></a></span>
<span id="cb5-6"><a></a><span class="cf">for</span> b <span class="kw">in</span> b_values:</span>
<span id="cb5-7"><a></a>    plt.plot(x, logistic(x, w, b), label<span class="op">=</span><span class="ss">f'w = </span><span class="sc">{</span>w<span class="sc">}</span><span class="ss">, b = </span><span class="sc">{</span>b<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb5-8"><a></a>plt.title(<span class="st">'Effect of Varying b (with w = 1)'</span>)</span>
<span id="cb5-9"><a></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb5-10"><a></a>plt.ylabel(<span class="vs">r'</span><span class="dv">$\s</span><span class="vs">igma</span><span class="kw">(</span><span class="vs">wx</span><span class="op">+</span><span class="vs">b</span><span class="kw">)</span><span class="dv">$</span><span class="vs">'</span>)</span>
<span id="cb5-11"><a></a>plt.legend()</span>
<span id="cb5-12"><a></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb5-13"><a></a></span>
<span id="cb5-14"><a></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-6-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img data-src="slides_files/figure-revealjs/cell-6-output-1.png" class="quarto-figure quarto-figure-center" width="664" height="523"></a></p>
</figure>
</div>
</div>
</div>
</div></div>
</section></section>
<section>
<section id="implementation" class="title-slide slide level1 center">
<h1>Implementation</h1>

</section>
<section id="implementation-generating-data" class="slide level2">
<h2>Implementation: Generating Data</h2>
<div id="abff659c" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a><span class="co"># Generate synthetic data for a binary classification problem</span></span>
<span id="cb6-2"><a></a></span>
<span id="cb6-3"><a></a>m <span class="op">=</span> <span class="dv">100</span>  <span class="co"># number of examples</span></span>
<span id="cb6-4"><a></a>d <span class="op">=</span> <span class="dv">2</span>    <span class="co"># number of featues</span></span>
<span id="cb6-5"><a></a></span>
<span id="cb6-6"><a></a>X <span class="op">=</span> np.random.randn(m, d)</span>
<span id="cb6-7"><a></a></span>
<span id="cb6-8"><a></a><span class="co"># Define labels using a linear decision boundary with some noise:</span></span>
<span id="cb6-9"><a></a></span>
<span id="cb6-10"><a></a>noise <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> np.random.randn(m)</span>
<span id="cb6-11"><a></a></span>
<span id="cb6-12"><a></a>y <span class="op">=</span> (X[:, <span class="dv">0</span>] <span class="op">+</span> X[:, <span class="dv">1</span>] <span class="op">+</span> noise <span class="op">&gt;</span> <span class="dv">0</span>).astype(<span class="bu">int</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="implementation-vizualization" class="slide level2">
<h2>Implementation: Vizualization</h2>
<div id="52858478" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a></a><span class="co"># Visualize the decision boundary along with the data points</span></span>
<span id="cb7-2"><a></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb7-3"><a></a>plt.scatter(X[y <span class="op">==</span> <span class="dv">0</span>][:, <span class="dv">0</span>], X[y <span class="op">==</span> <span class="dv">0</span>][:, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Class 0'</span>)</span>
<span id="cb7-4"><a></a>plt.scatter(X[y <span class="op">==</span> <span class="dv">1</span>][:, <span class="dv">0</span>], X[y <span class="op">==</span> <span class="dv">1</span>][:, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Class 1'</span>)</span>
<span id="cb7-5"><a></a></span>
<span id="cb7-6"><a></a>plt.xlabel(<span class="st">"Feature 1"</span>)</span>
<span id="cb7-7"><a></a>plt.ylabel(<span class="st">"Feature 2"</span>)</span>
<span id="cb7-8"><a></a>plt.title(<span class="st">"Data"</span>)</span>
<span id="cb7-9"><a></a>plt.legend()</span>
<span id="cb7-10"><a></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-8-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img data-src="slides_files/figure-revealjs/cell-8-output-1.png" class="quarto-figure quarto-figure-center" width="662" height="523"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="implementation-cost-function" class="slide level2">
<h2>Implementation: Cost Function</h2>
<div id="d7205924" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a></a><span class="co"># Sigmoid function</span></span>
<span id="cb8-2"><a></a><span class="kw">def</span> sigmoid(z):</span>
<span id="cb8-3"><a></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>z))</span>
<span id="cb8-4"><a></a></span>
<span id="cb8-5"><a></a><span class="co"># Cost function: binary cross-entropy</span></span>
<span id="cb8-6"><a></a><span class="kw">def</span> cost_function(theta, X, y):</span>
<span id="cb8-7"><a></a>    m <span class="op">=</span> <span class="bu">len</span>(y)</span>
<span id="cb8-8"><a></a>    h <span class="op">=</span> sigmoid(X.dot(theta))</span>
<span id="cb8-9"><a></a>    epsilon <span class="op">=</span> <span class="fl">1e-5</span>  <span class="co"># avoid log(0)</span></span>
<span id="cb8-10"><a></a>    cost <span class="op">=</span> <span class="op">-</span>(<span class="dv">1</span><span class="op">/</span>m) <span class="op">*</span> np.<span class="bu">sum</span>(y <span class="op">*</span> np.log(h <span class="op">+</span> epsilon) <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> y) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">-</span> h <span class="op">+</span> epsilon))</span>
<span id="cb8-11"><a></a>    <span class="cf">return</span> cost</span>
<span id="cb8-12"><a></a></span>
<span id="cb8-13"><a></a><span class="co"># Gradient of the cost function</span></span>
<span id="cb8-14"><a></a><span class="kw">def</span> gradient(theta, X, y):</span>
<span id="cb8-15"><a></a>    m <span class="op">=</span> <span class="bu">len</span>(y)</span>
<span id="cb8-16"><a></a>    h <span class="op">=</span> sigmoid(X.dot(theta))</span>
<span id="cb8-17"><a></a>    grad <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>m) <span class="op">*</span> X.T.dot(h <span class="op">-</span> y)</span>
<span id="cb8-18"><a></a>    <span class="cf">return</span> grad</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="implementation-logistic-regression" class="slide level2">
<h2>Implementation: Logistic Regression</h2>
<div id="04a6f729" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a></a><span class="co"># Logistic regression training using gradient descent</span></span>
<span id="cb9-2"><a></a><span class="kw">def</span> logistic_regression(X, y, learning_rate<span class="op">=</span><span class="fl">0.1</span>, iterations<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb9-3"><a></a>    m, n <span class="op">=</span> X.shape</span>
<span id="cb9-4"><a></a>    theta <span class="op">=</span> np.zeros(n)</span>
<span id="cb9-5"><a></a>    cost_history <span class="op">=</span> []</span>
<span id="cb9-6"><a></a>    </span>
<span id="cb9-7"><a></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iterations):</span>
<span id="cb9-8"><a></a>        theta <span class="op">-=</span> learning_rate <span class="op">*</span> gradient(theta, X, y)</span>
<span id="cb9-9"><a></a>        cost_history.append(cost_function(theta, X, y))</span>
<span id="cb9-10"><a></a>        </span>
<span id="cb9-11"><a></a>    <span class="cf">return</span> theta, cost_history</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="training" class="slide level2">
<h2>Training</h2>
<div id="1fc9fe99" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a></a><span class="co"># Add intercept term (bias)</span></span>
<span id="cb10-2"><a></a>X_with_intercept <span class="op">=</span> np.hstack([np.ones((m, <span class="dv">1</span>)), X])</span>
<span id="cb10-3"><a></a></span>
<span id="cb10-4"><a></a><span class="co"># Train the logistic regression model</span></span>
<span id="cb10-5"><a></a>theta, cost_history <span class="op">=</span> logistic_regression(X_with_intercept, y, learning_rate<span class="op">=</span><span class="fl">0.1</span>, iterations<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb10-6"><a></a></span>
<span id="cb10-7"><a></a><span class="bu">print</span>(<span class="st">"Optimized theta:"</span>, theta)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Optimized theta: [-0.28840995  2.80390104  2.45238752]</code></pre>
</div>
</div>
</section>
<section id="cost-function-convergence" class="slide level2">
<h2>Cost Function Convergence</h2>
<div id="fb428cbb" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb12-2"><a></a>plt.plot(cost_history, label<span class="op">=</span><span class="st">"Cost"</span>)</span>
<span id="cb12-3"><a></a>plt.xlabel(<span class="st">"Iteration"</span>)</span>
<span id="cb12-4"><a></a>plt.ylabel(<span class="st">"Cost"</span>)</span>
<span id="cb12-5"><a></a>plt.title(<span class="st">"Cost Function Convergence"</span>)</span>
<span id="cb12-6"><a></a>plt.legend()</span>
<span id="cb12-7"><a></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-12-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img data-src="slides_files/figure-revealjs/cell-12-output-1.png" class="quarto-figure quarto-figure-center" width="672" height="523"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="decision-boundary-and-data-points" class="slide level2">
<h2>Decision Boundary and Data Points</h2>
<div id="c719052f" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb13-2"><a></a>plt.scatter(X[y <span class="op">==</span> <span class="dv">0</span>][:, <span class="dv">0</span>], X[y <span class="op">==</span> <span class="dv">0</span>][:, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Class 0'</span>)</span>
<span id="cb13-3"><a></a>plt.scatter(X[y <span class="op">==</span> <span class="dv">1</span>][:, <span class="dv">0</span>], X[y <span class="op">==</span> <span class="dv">1</span>][:, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Class 1'</span>)</span>
<span id="cb13-4"><a></a></span>
<span id="cb13-5"><a></a><span class="co"># Decision boundary: theta0 + theta1*x1 + theta2*x2 = 0</span></span>
<span id="cb13-6"><a></a>x_vals <span class="op">=</span> np.array([<span class="bu">min</span>(X[:, <span class="dv">0</span>]) <span class="op">-</span> <span class="dv">1</span>, <span class="bu">max</span>(X[:, <span class="dv">0</span>]) <span class="op">+</span> <span class="dv">1</span>])</span>
<span id="cb13-7"><a></a>y_vals <span class="op">=</span> <span class="op">-</span>(theta[<span class="dv">0</span>] <span class="op">+</span> theta[<span class="dv">1</span>] <span class="op">*</span> x_vals) <span class="op">/</span> theta[<span class="dv">2</span>]</span>
<span id="cb13-8"><a></a>plt.plot(x_vals, y_vals, label<span class="op">=</span><span class="st">'Decision Boundary'</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb13-9"><a></a>plt.xlabel(<span class="st">"Feature 1"</span>)</span>
<span id="cb13-10"><a></a>plt.ylabel(<span class="st">"Feature 2"</span>)</span>
<span id="cb13-11"><a></a>plt.title(<span class="st">"Logistic Regression Decision Boundary"</span>)</span>
<span id="cb13-12"><a></a>plt.legend()</span>
<span id="cb13-13"><a></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-13-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img data-src="slides_files/figure-revealjs/cell-13-output-1.png" class="quarto-figure quarto-figure-center" width="662" height="523"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="implementation-continued" class="slide level2">
<h2>Implementation (continued)</h2>
<div id="16967a20" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a></a><span class="co"># Predict function: returns class labels and probabilities for new data</span></span>
<span id="cb14-2"><a></a><span class="kw">def</span> predict(theta, X, threshold<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb14-3"><a></a>    probs <span class="op">=</span> sigmoid(X.dot(theta))</span>
<span id="cb14-4"><a></a>    <span class="cf">return</span> (probs <span class="op">&gt;=</span> threshold).astype(<span class="bu">int</span>), probs</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="predictions" class="slide level2">
<h2>Predictions</h2>
<div id="81228ed7" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a></a><span class="co"># New examples must include the intercept term.</span></span>
<span id="cb15-2"><a></a></span>
<span id="cb15-3"><a></a><span class="co"># Negative example (likely class 0): Choose a point far in the negative quadrant.</span></span>
<span id="cb15-4"><a></a>example_neg <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="op">-</span><span class="dv">3</span>, <span class="op">-</span><span class="dv">3</span>])</span>
<span id="cb15-5"><a></a></span>
<span id="cb15-6"><a></a><span class="co"># Positive example (likely class 1): Choose a point far in the positive quadrant.</span></span>
<span id="cb15-7"><a></a>example_pos <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>])</span>
<span id="cb15-8"><a></a></span>
<span id="cb15-9"><a></a><span class="co"># Near decision boundary: Choose x1 = 0 and compute x2 from the decision boundary equation.</span></span>
<span id="cb15-10"><a></a>x1_near <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb15-11"><a></a>x2_near <span class="op">=</span> <span class="op">-</span>(theta[<span class="dv">0</span>] <span class="op">+</span> theta[<span class="dv">1</span>] <span class="op">*</span> x1_near) <span class="op">/</span> theta[<span class="dv">2</span>]</span>
<span id="cb15-12"><a></a>example_near <span class="op">=</span> np.array([<span class="dv">1</span>, x1_near, x2_near])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<aside class="notes">
<p>In the given example, each data point is characterized by two primary features. However, the representation includes three components. Why?</p>
<p>This discrepancy arises from an earlier discussed mathematical technique, where each instance is augmented with an additional term, <span class="math inline">\(x_i^{(0)} = 1\)</span>. This augmentation facilitates the expression of the model in a vectorized format, enhancing computational efficiency and simplicity.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="predictions-continued" class="slide level2">
<h2>Predictions (continued)</h2>
<div id="56971646" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a></a><span class="co"># Combine the examples into one array for prediction.</span></span>
<span id="cb16-2"><a></a>new_examples <span class="op">=</span> np.vstack([example_neg, example_pos, example_near])</span>
<span id="cb16-3"><a></a></span>
<span id="cb16-4"><a></a>labels, probabilities <span class="op">=</span> predict(theta, new_examples)</span>
<span id="cb16-5"><a></a></span>
<span id="cb16-6"><a></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Predictions on new examples:"</span>)</span>
<span id="cb16-7"><a></a></span>
<span id="cb16-8"><a></a><span class="bu">print</span>(<span class="st">"Negative example </span><span class="sc">{}</span><span class="st"> -&gt; Prediction: </span><span class="sc">{}</span><span class="st"> (Probability: </span><span class="sc">{:.4f}</span><span class="st">)"</span>.<span class="bu">format</span>(example_neg[<span class="dv">1</span>:], labels[<span class="dv">0</span>], probabilities[<span class="dv">0</span>]))</span>
<span id="cb16-9"><a></a></span>
<span id="cb16-10"><a></a><span class="bu">print</span>(<span class="st">"Positive example </span><span class="sc">{}</span><span class="st"> -&gt; Prediction: </span><span class="sc">{}</span><span class="st"> (Probability: </span><span class="sc">{:.4f}</span><span class="st">)"</span>.<span class="bu">format</span>(example_pos[<span class="dv">1</span>:], labels[<span class="dv">1</span>], probabilities[<span class="dv">1</span>]))</span>
<span id="cb16-11"><a></a></span>
<span id="cb16-12"><a></a><span class="bu">print</span>(<span class="st">"Near-boundary example </span><span class="sc">{}</span><span class="st"> -&gt; Prediction: </span><span class="sc">{}</span><span class="st"> (Probability: </span><span class="sc">{:.4f}</span><span class="st">)"</span>.<span class="bu">format</span>(example_near[<span class="dv">1</span>:], labels[<span class="dv">2</span>], probabilities[<span class="dv">2</span>]))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Predictions on new examples:
Negative example [-3 -3] -&gt; Prediction: 0 (Probability: 0.0000)
Positive example [3 3] -&gt; Prediction: 1 (Probability: 1.0000)
Near-boundary example [0.         0.11760374] -&gt; Prediction: 1 (Probability: 0.5000)</code></pre>
</div>
</div>
</section>
<section id="visualizing-the-weight-vector" class="slide level2">
<h2>Visualizing the Weight Vector</h2>
<p>In the previous lecture, we established that <strong>logistic regression</strong> determines a <strong>weight vector</strong> that is <strong>orthogonal</strong> to the <strong>decision boundary</strong>.</p>
<p>Conversely, the <strong>decision boundary</strong> itself is <strong>orthogonal</strong> to the <strong>weight vector</strong>, which is derived through gradient descent optimization.</p>
</section>
<section id="visualizing-the-weight-vector-1" class="slide level2">
<h2>Visualizing the Weight Vector</h2>
<div id="bacd7ca5" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a></a><span class="co"># Plot decision boundary and data points</span></span>
<span id="cb18-2"><a></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb18-3"><a></a>plt.scatter(X[y <span class="op">==</span> <span class="dv">0</span>][:, <span class="dv">0</span>], X[y <span class="op">==</span> <span class="dv">0</span>][:, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Class 0'</span>)</span>
<span id="cb18-4"><a></a>plt.scatter(X[y <span class="op">==</span> <span class="dv">1</span>][:, <span class="dv">0</span>], X[y <span class="op">==</span> <span class="dv">1</span>][:, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Class 1'</span>)</span>
<span id="cb18-5"><a></a></span>
<span id="cb18-6"><a></a><span class="co"># Decision boundary: theta0 + theta1*x1 + theta2*x2 = 0</span></span>
<span id="cb18-7"><a></a>x_vals <span class="op">=</span> np.array([<span class="bu">min</span>(X[:, <span class="dv">0</span>]) <span class="op">-</span> <span class="dv">1</span>, <span class="bu">max</span>(X[:, <span class="dv">0</span>]) <span class="op">+</span> <span class="dv">1</span>])</span>
<span id="cb18-8"><a></a>y_vals <span class="op">=</span> <span class="op">-</span>(theta[<span class="dv">0</span>] <span class="op">+</span> theta[<span class="dv">1</span>] <span class="op">*</span> x_vals) <span class="op">/</span> theta[<span class="dv">2</span>]</span>
<span id="cb18-9"><a></a>plt.plot(x_vals, y_vals, label<span class="op">=</span><span class="st">'Decision Boundary'</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb18-10"><a></a></span>
<span id="cb18-11"><a></a><span class="co"># --- Draw the normal vector ---</span></span>
<span id="cb18-12"><a></a><span class="co"># The normal vector is (theta[1], theta[2]).</span></span>
<span id="cb18-13"><a></a><span class="co"># Choose a reference point on the decision boundary. Here, we use x1 = 0:</span></span>
<span id="cb18-14"><a></a>x_ref <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-15"><a></a>y_ref <span class="op">=</span> <span class="op">-</span>theta[<span class="dv">0</span>] <span class="op">/</span> theta[<span class="dv">2</span>]  <span class="co"># when x1=0, theta0 + theta2*x2=0  =&gt;  x2=-theta0/theta2</span></span>
<span id="cb18-16"><a></a></span>
<span id="cb18-17"><a></a><span class="co"># Create the normal vector from (theta[1], theta[2]).</span></span>
<span id="cb18-18"><a></a>normal <span class="op">=</span> np.array([theta[<span class="dv">1</span>], theta[<span class="dv">2</span>]])</span>
<span id="cb18-19"><a></a></span>
<span id="cb18-20"><a></a><span class="co"># Normalize and scale for display</span></span>
<span id="cb18-21"><a></a>normal_norm <span class="op">=</span> np.linalg.norm(normal)</span>
<span id="cb18-22"><a></a><span class="cf">if</span> normal_norm <span class="op">!=</span> <span class="dv">0</span>:</span>
<span id="cb18-23"><a></a>    normal_unit <span class="op">=</span> normal <span class="op">/</span> normal_norm</span>
<span id="cb18-24"><a></a><span class="cf">else</span>:</span>
<span id="cb18-25"><a></a>    normal_unit <span class="op">=</span> normal</span>
<span id="cb18-26"><a></a>scale <span class="op">=</span> <span class="dv">2</span>  <span class="co"># adjust scale as needed</span></span>
<span id="cb18-27"><a></a>normal_display <span class="op">=</span> normal_unit <span class="op">*</span> scale</span>
<span id="cb18-28"><a></a></span>
<span id="cb18-29"><a></a><span class="co"># Draw an arrow starting at the reference point</span></span>
<span id="cb18-30"><a></a>plt.arrow(x_ref, y_ref, normal_display[<span class="dv">0</span>], normal_display[<span class="dv">1</span>],</span>
<span id="cb18-31"><a></a>          head_width<span class="op">=</span><span class="fl">0.1</span>, head_length<span class="op">=</span><span class="fl">0.2</span>, fc<span class="op">=</span><span class="st">'black'</span>, ec<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb18-32"><a></a>plt.text(x_ref <span class="op">+</span> normal_display[<span class="dv">0</span>]<span class="op">*</span><span class="fl">1.1</span>, y_ref <span class="op">+</span> normal_display[<span class="dv">1</span>]<span class="op">*</span><span class="fl">1.1</span>, </span>
<span id="cb18-33"><a></a>         <span class="vs">r'</span><span class="dv">$</span><span class="kw">(</span><span class="ch">\t</span><span class="vs">heta_1, </span><span class="ch">\t</span><span class="vs">heta_2</span><span class="kw">)</span><span class="dv">$</span><span class="vs">'</span>, color<span class="op">=</span><span class="st">'black'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb18-34"><a></a></span>
<span id="cb18-35"><a></a>plt.xlabel(<span class="st">"Feature 1"</span>)</span>
<span id="cb18-36"><a></a>plt.ylabel(<span class="st">"Feature 2"</span>)</span>
<span id="cb18-37"><a></a>plt.title(<span class="st">"Logistic Regression Decision Boundary and Normal Vector"</span>)</span>
<span id="cb18-38"><a></a>plt.legend()</span>
<span id="cb18-39"><a></a>plt.gca().set_aspect(<span class="st">'equal'</span>, adjustable<span class="op">=</span><span class="st">'box'</span>)</span>
<span id="cb18-40"><a></a>plt.ylim(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb18-41"><a></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-17-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img data-src="slides_files/figure-revealjs/cell-17-output-1.png" class="quarto-figure quarto-figure-center" width="596" height="523"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="near-the-decision-boundary" class="slide level2">
<h2>Near the Decision Boundary</h2>
<div id="7607aebe" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a></a><span class="co"># --- Visualization Setup ---</span></span>
<span id="cb19-2"><a></a><span class="co"># Create a grid over the feature space</span></span>
<span id="cb19-3"><a></a>x1_range <span class="op">=</span> np.linspace(X[:, <span class="dv">0</span>].<span class="bu">min</span>()<span class="op">-</span><span class="dv">1</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>()<span class="op">+</span><span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb19-4"><a></a>x2_range <span class="op">=</span> np.linspace(X[:, <span class="dv">1</span>].<span class="bu">min</span>()<span class="op">-</span><span class="dv">1</span>, X[:, <span class="dv">1</span>].<span class="bu">max</span>()<span class="op">+</span><span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb19-5"><a></a>xx1, xx2 <span class="op">=</span> np.meshgrid(x1_range, x2_range)</span>
<span id="cb19-6"><a></a></span>
<span id="cb19-7"><a></a><span class="co"># Construct the grid input (with intercept) for predictions</span></span>
<span id="cb19-8"><a></a>grid <span class="op">=</span> np.c_[np.ones(xx1.ravel().shape), xx1.ravel(), xx2.ravel()]</span>
<span id="cb19-9"><a></a><span class="co"># Compute predicted probabilities over the grid</span></span>
<span id="cb19-10"><a></a>probs <span class="op">=</span> sigmoid(grid.dot(theta)).reshape(xx1.shape)</span>
<span id="cb19-11"><a></a><span class="co"># --- Approach 2: 2D Contour (Heatmap) Plot ---</span></span>
<span id="cb19-12"><a></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb19-13"><a></a>contour <span class="op">=</span> plt.contourf(xx1, xx2, probs, cmap<span class="op">=</span><span class="st">'spring'</span>, levels<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb19-14"><a></a>plt.colorbar(contour)</span>
<span id="cb19-15"><a></a>plt.xlabel(<span class="st">'Feature x1'</span>)</span>
<span id="cb19-16"><a></a>plt.ylabel(<span class="st">'Feature x2'</span>)</span>
<span id="cb19-17"><a></a>plt.title(<span class="st">'Contour Plot (Heatmap) of Predicted Probabilities'</span>)</span>
<span id="cb19-18"><a></a><span class="co"># Overlay training data</span></span>
<span id="cb19-19"><a></a>plt.scatter(X[y <span class="op">==</span> <span class="dv">0</span>][:, <span class="dv">0</span>], X[y <span class="op">==</span> <span class="dv">0</span>][:, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'red'</span>, edgecolor<span class="op">=</span><span class="st">'k'</span>, label<span class="op">=</span><span class="st">'Class 0'</span>)</span>
<span id="cb19-20"><a></a>plt.scatter(X[y <span class="op">==</span> <span class="dv">1</span>][:, <span class="dv">0</span>], X[y <span class="op">==</span> <span class="dv">1</span>][:, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'blue'</span>, edgecolor<span class="op">=</span><span class="st">'k'</span>, label<span class="op">=</span><span class="st">'Class 1'</span>)</span>
<span id="cb19-21"><a></a>plt.legend()</span>
<span id="cb19-22"><a></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-18-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img data-src="slides_files/figure-revealjs/cell-18-output-1.png" class="quarto-figure quarto-figure-center" width="634" height="523"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="near-the-decision-boundary-1" class="slide level2">
<h2>Near the Decision Boundary</h2>
<div id="b2f9c7a4" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a></a><span class="co"># --- Approach 1: 3D Surface Plot ---</span></span>
<span id="cb20-2"><a></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb20-3"><a></a>ax <span class="op">=</span> fig.add_subplot(<span class="dv">111</span>, projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb20-4"><a></a>surface <span class="op">=</span> ax.plot_surface(xx1, xx2, probs, cmap<span class="op">=</span><span class="st">'spring'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb20-5"><a></a>ax.set_xlabel(<span class="st">'Feature x1'</span>)</span>
<span id="cb20-6"><a></a>ax.set_ylabel(<span class="st">'Feature x2'</span>)</span>
<span id="cb20-7"><a></a>ax.set_zlabel(<span class="st">'Probability'</span>)</span>
<span id="cb20-8"><a></a>ax.set_title(<span class="st">'3D Surface Plot of Logistic Regression Model'</span>)</span>
<span id="cb20-9"><a></a>fig.colorbar(surface, shrink<span class="op">=</span><span class="fl">0.5</span>, aspect<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb20-10"><a></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-19-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11"><img data-src="slides_files/figure-revealjs/cell-19-output-1.png" class="quarto-figure quarto-figure-center" width="737" height="631"></a></p>
</figure>
</div>
</div>
</div>
</section></section>
<section>
<section id="prologue" class="title-slide slide level1 center">
<h1>Prologue</h1>

</section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
<p>In this presentation, we:</p>
<ul>
<li>Derived the <strong>likelihood</strong> and <strong>negative log-likelihood</strong> formulations.</li>
<li>Illustrated the <strong>geometric interpretation</strong> of decision boundaries and weight vectors.</li>
<li><strong>Implemented</strong> logistic regression with <strong>gradient descent</strong> and visualized results.</li>
</ul>
</section>
<section id="next-lecture" class="slide level2">
<h2>Next lecture</h2>
<ul>
<li>Performance measures and cross-evaluation</li>
</ul>
</section>
<section id="references" class="slide level2 smaller scrollable">
<h2>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Russell:2020aa" class="csl-entry" role="listitem">
Russell, Stuart, and Peter Norvig. 2020. <em>Artificial Intelligence: <span>A</span> Modern Approach</em>. 4th ed. Pearson. <a href="http://aima.cs.berkeley.edu/">http://aima.cs.berkeley.edu/</a>.
</div>
</div>
</section>
<section class="slide level2">

<p>Marcel <strong>Turcotte</strong></p>
<p><a href="mailto:Marcel.Turcotte@uOttawa.ca">Marcel.Turcotte@uOttawa.ca</a></p>
<p>School of Electrical Engineering and <strong>Computer Science</strong> (EE<strong>CS</strong>)</p>
<p>University of Ottawa</p>


</section></section>

    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../../assets/images/uottawa_hor_black.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://turcotte.xyz/teaching/csi-4106">turcotte.xyz/teaching/csi-4106</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/turcotte\.xyz\/teaching\/csi-4106");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    <script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
    (function() {
      let previousOnload = window.onload;
      window.onload = () => {
        if (previousOnload) {
          previousOnload();
        }
        lightboxQuarto.on('slide_before_load', (data) => {
          const { slideIndex, slideNode, slideConfig, player, trigger } = data;
          const href = trigger.getAttribute('href');
          if (href !== null) {
            const imgEl = window.document.querySelector(`a[href="${href}"] img`);
            if (imgEl !== null) {
              const srcAttr = imgEl.getAttribute("src");
              if (srcAttr && srcAttr.startsWith("data:")) {
                slideConfig.href = srcAttr;
              }
            }
          } 
        });
      
        lightboxQuarto.on('slide_after_load', (data) => {
          const { slideIndex, slideNode, slideConfig, player, trigger } = data;
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(slideNode);
          }
        });
      
      };
      
    })();
              </script>
    

</body></html>