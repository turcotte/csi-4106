<!DOCTYPE html>
<html lang="en"><head>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.33">

  <meta name="author" content="Marcel Turcotte">
  <title>CSI 4106 - Fall 2025 – Linear regression and gradient descent</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-f563837468303362081e247dddd440d0.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
  
  <script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<meta property="og:title" content="Linear regression and gradient descent – CSI 4106 - Fall 2025">
<meta property="og:description" content="CSI 4106 - Fall 2025">
<meta property="og:site_name" content="CSI 4106 - Fall 2025">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Linear regression and gradient descent</h1>
  <p class="subtitle">CSI 4106 - Fall 2025</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Marcel Turcotte 
</div>
</div>
</div>

  <p class="date">Version: Sep 16, 2025 17:58</p>
</section>
<section>
<section id="preamble" class="title-slide slide level1 center">
<h1>Preamble</h1>

</section>
<section id="message-of-the-day" class="slide level2">
<h2>Message of the Day</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="https://ichef.bbci.co.uk/news/1536/cpsprodpb/d40c/live/2c052910-9004-11f0-86c4-a304e538bef3.jpg.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img data-src="https://ichef.bbci.co.uk/news/1536/cpsprodpb/d40c/live/2c052910-9004-11f0-86c4-a304e538bef3.jpg.webp" class="quarto-figure quarto-figure-center" height="475"></a></p>
</figure>
</div>

<aside class="notes">
<p>Albania has appointed an <strong>AI minister</strong>, Diella, to the role of minister for public procurement. While the appointment is symbolic due to constitutional requirements, Diella aims to <strong>eliminate corruption</strong> in public tenders by leveraging AI for faster, more efficient, and accountable processes. The initiative has received mixed reactions, with some viewing it as a publicity stunt while others see potential for improving transparency and trust in public procurement.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p><a href="https://www.bbc.com/news/articles/cm2znzgwj3xo">World’s first AI minister will eliminate corruption, says Albania’s PM</a>, by Guy Delauney, BBC, 2025-09-12.</p>
</div></aside></section>
<section id="learning-objectives" class="slide level2">
<h2>Learning Objectives</h2>
<ul>
<li><strong>Differentiate</strong> regression tasks from classification tasks.</li>
<li><strong>Articulate</strong> the training methodology for linear regression models.</li>
<li><strong>Interpret</strong> the function of optimization algorithms in addressing linear regression.</li>
<li><strong>Detail</strong> the significance of partial derivatives within the <em>gradient descent algorithm</em>.</li>
<li><strong>Contrast</strong> the <strong>batch</strong>, <strong>stochastic</strong>, and <strong>mini-batch</strong> gradient descent methods.</li>
</ul>
<aside class="notes">
<p>In the previous lecture, we examined two distinct learning algorithms: <span class="math inline">\(k\)</span>-nearest neighbors (KNN) and decision trees, each employing unique methods for model training and representation. KNN does not involve explicit learning; instead, the data itself constitutes the model. Conversely, decision trees utilize a greedy algorithm that begins with an empty tree and the full training dataset. The algorithm incrementally adds decision nodes, partitioning the parent dataset into subsets to achieve greater homogeneity (or purity) within each resulting classification compared to the parent node. This recursive process terminates when a node’s data satisfies predefined stopping criteria, such as achieving a single class, reaching a minimum purity level, or attaining a maximum tree depth. Once the decision tree is constructed, the original data is no longer needed. This lecture aimed to demonstrate that supervised learning models can be represented in various ways, with each “learning” algorithm tailored to its specific model.</p>
<p>In today’s lecture, we will explore a training algorithm that is applicable to a diverse range of models, including neural networks.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="linear-regression" class="title-slide slide level1 center">
<h1>Linear Regression</h1>

</section>
<section id="rationale" class="slide level2 smaller">
<h2>Rationale</h2>
<p><strong>Linear regression</strong> is introduced to conveniently present a well-known training algorithm, <strong>gradient descent</strong>. Additionally, it serves as a foundation for introducing <strong>logistic regression</strong>–a classification algorithm—which further facilitates discussions on <strong>artificial neural networks</strong>.</p>
<ul>
<li>Linear Regression
<ul>
<li>Gradient Descent</li>
<li>Logistic Regression
<ul>
<li>Neural Networks</li>
</ul></li>
</ul></li>
</ul>

<aside class="notes">
<p>The training algorithms for machine learning models can vary significantly depending on the model (e.g., decision trees, SVMs, etc.). In order to fit our schedule, we will concentrate on this specific sequence.</p>
<p>The concept of linear regression can be traced back to the early work of Sir Francis Galton in the late 19th century. Galton introduced the idea of “regression” in his 1886 paper, which focused on the relationship between the heights of parents and their children. He observed that children’s heights tended to regress towards the average, which led to the term “regression.”</p>
<p>However, the mathematical formulation of linear regression is closely associated with the work of Karl Pearson, who in the early 20th century extended Galton’s ideas to create the method of least squares for fitting a linear model. The method itself, though, was developed earlier in 1805 by Adrien-Marie Legendre and independently by Carl Friedrich Gauss for astronomical data analysis.</p>
<p><strong>See:</strong> <span class="citation" data-cites="Stanton:2001aa">Stanton (<a href="#/references" role="doc-biblioref" onclick="">2001</a>)</span>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p>From 2 to <strong>2 trillion</strong> parameters!</p>
</div></aside></section>
<section id="supervised-learning---regression" class="slide level2">
<h2>Supervised Learning - Regression</h2>
<ul>
<li class="fragment">The <strong>training data</strong> is a collection of <strong>labelled</strong> examples.
<ul>
<li class="fragment"><span class="math inline">\(\{(x_i,y_i)\}_{i=1}^N\)</span>
<ul>
<li class="fragment">Each <span class="math inline">\(x_i\)</span> is a <strong>feature vector</strong> with <span class="math inline">\(D\)</span> dimensions.</li>
<li class="fragment"><span class="math inline">\(x_i^{(j)}\)</span> is the value of the <strong>feature</strong> <span class="math inline">\(j\)</span> of the example <span class="math inline">\(i\)</span>, for <span class="math inline">\(j \in 1 \ldots D\)</span> and <span class="math inline">\(i \in 1 \ldots N\)</span>.</li>
</ul></li>
<li class="fragment">The <strong>label</strong> <span class="math inline">\(y_i\)</span> is a <strong>real number</strong>.</li>
</ul></li>
<li class="fragment"><strong>Problem</strong>: Given the data set as input, create a <strong>model</strong> that can be used to predict the value of <span class="math inline">\(y\)</span> for an unseen <span class="math inline">\(x\)</span>.</li>
</ul>
<aside class="notes">
<p>Can you think of examples of regression tasks?</p>
<ol type="1">
<li><strong>House Price Prediction</strong>:
<ul>
<li><strong>Application</strong>: Estimating the market value of residential properties based on features such as location, size, number of bedrooms, age, and amenities.</li>
</ul></li>
<li><strong>Stock Market Forecasting</strong>:
<ul>
<li><strong>Application</strong>: Predicting future prices of stocks or indices based on historical data, financial indicators, and economic variables.</li>
</ul></li>
<li><strong>Weather Prediction</strong>:
<ul>
<li><strong>Application</strong>: Estimating future temperatures, rainfall, and other weather conditions using historical weather data and atmospheric variables.</li>
</ul></li>
<li><strong>Sales Forecasting</strong>:
<ul>
<li><strong>Application</strong>: Predicting future sales volumes for products or services by analyzing past sales data, market trends, and seasonal patterns.</li>
</ul></li>
<li><strong>Energy Consumption Prediction</strong>:
<ul>
<li><strong>Application</strong>: Forecasting future energy usage for households, industries, or cities based on historical consumption data, weather conditions, and economic factors.</li>
</ul></li>
<li><strong>Medical Cost Estimation</strong>:
<ul>
<li><strong>Application</strong>: Predicting healthcare costs for patients based on their medical history, demographic information, and treatment plans.</li>
</ul></li>
<li><strong>Traffic Flow Prediction</strong>:
<ul>
<li><strong>Application</strong>: Estimating future traffic volumes and congestion levels on roads and highways using historical traffic data and real-time sensor inputs.</li>
</ul></li>
<li><strong>Customer Lifetime Value (CLV) Estimation</strong>:
<ul>
<li><strong>Application</strong>: Predicting the total revenue a business can expect from a customer over the duration of their relationship, based on purchasing behavior and demographic data.</li>
</ul></li>
<li><strong>Economic Indicators Forecasting</strong>:
<ul>
<li><strong>Application</strong>: Predicting key economic indicators such as GDP growth, unemployment rates, and inflation using historical economic data and market trends.</li>
</ul></li>
<li><strong>Demand Forecasting</strong>:
<ul>
<li><strong>Application</strong>: Estimating future demand for products or services in various industries like retail, manufacturing, and logistics to optimize inventory and supply chain management.</li>
</ul></li>
<li><strong>Real Estate Valuation</strong>:
<ul>
<li><strong>Application</strong>: Assessing the market value of commercial properties like office buildings, malls, and industrial spaces based on location, size, and market conditions.</li>
</ul></li>
<li><strong>Insurance Risk Assessment</strong>:
<ul>
<li><strong>Application</strong>: Predicting the risk associated with insuring individuals or properties, which helps in determining premium rates, based on historical claims data, and demographic factors.</li>
</ul></li>
<li><strong>Ad Click-Through Rate (CTR) Prediction</strong>:
<ul>
<li><strong>Application</strong>: Estimating the likelihood that a user will click on an online advertisement based on user behavior, ad characteristics, and contextual factors.</li>
</ul></li>
<li><strong>Loan Default Prediction</strong>:
<ul>
<li><strong>Application</strong>: Predicting the probability of a borrower defaulting on a loan based on credit history, income, loan amount, and other financial indicators.</li>
</ul></li>
</ol>
<p>Focusing on applications possibly running on a mobile device.</p>
<ol type="1">
<li><strong>Battery Life Prediction</strong>:
<ul>
<li><strong>Application</strong>: Estimating remaining battery life based on usage patterns, running applications, and device settings.</li>
</ul></li>
<li><strong>Health and Fitness Tracking</strong>:
<ul>
<li><strong>Application</strong>: Predicting calorie burn, heart rate, or sleep quality based on user activity, biometrics, and historical health data.</li>
</ul></li>
<li><strong>Personal Finance Management</strong>:
<ul>
<li><strong>Application</strong>: Forecasting future expenses or savings based on spending habits, income patterns, and budget goals.</li>
</ul></li>
<li><strong>Weather Forecasting</strong>:
<ul>
<li><strong>Application</strong>: Providing personalized weather forecasts based on current location and historical weather data.</li>
</ul></li>
<li><strong>Traffic and Commute Time Estimation</strong>:
<ul>
<li><strong>Application</strong>: Predicting travel times and suggesting optimal routes based on historical traffic data, real-time conditions, and user behavior.</li>
</ul></li>
<li><strong>Image and Video Quality Enhancement</strong>:
<ul>
<li><strong>Application</strong>: Adjusting image or video quality settings (e.g., brightness, contrast) based on lighting conditions and user preferences.</li>
</ul></li>
<li><strong>Fitness Goal Achievement</strong>:
<ul>
<li><strong>Application</strong>: Estimating the time needed to achieve fitness goals such as weight loss or muscle gain based on user activity and dietary input.</li>
</ul></li>
<li><strong>Mobile Device Performance Optimization</strong>:
<ul>
<li><strong>Application</strong>: Predicting the optimal settings for device performance and battery life based on usage patterns and app activity.</li>
</ul></li>
</ol>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="old-faithful-eruptions" class="slide level2">
<h2>Old Faithful Eruptions</h2>
<div id="16522a67" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a></a></span>
<span id="cb1-3"><a></a>WOLFRAM_CSV <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/turcotte/csi4106-f25/refs/heads/main/datasets/old_faithful_eruptions/Sample-Data-Old-Faithful-Eruptions.csv"</span></span>
<span id="cb1-4"><a></a>df <span class="op">=</span> pd.read_csv(WOLFRAM_CSV)</span>
<span id="cb1-5"><a></a></span>
<span id="cb1-6"><a></a><span class="co"># Renaming the columns</span></span>
<span id="cb1-7"><a></a>df <span class="op">=</span> df.rename(columns<span class="op">=</span>{<span class="st">"Duration"</span>: <span class="st">"eruptions"</span>, <span class="st">"WaitingTime"</span>: <span class="st">"waiting"</span>})</span>
<span id="cb1-8"><a></a><span class="bu">print</span>(df.shape)</span>
<span id="cb1-9"><a></a>df.head(<span class="dv">6</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


<aside class="notes">
<p>The dataset used in this presentation was sourced from the Wolfram Research Data Repository, with its initial publication detailed in <span class="citation" data-cites="Azzalini:1990aa">Azzalini and Bowman (<a href="#/references" role="doc-biblioref" onclick="">1990</a>)</span>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p><strong>Attribution:</strong> Wolfram Research, “Sample Data: Old Faithful Eruptions” from the Wolfram Data Repository (2016) <a href="https://doi.org/10.24097/wolfram.50727.data">doi: 10.24097/wolfram.50727.data</a></p>
</div></aside></section>
<section id="old-faithful-eruptions-output" class="slide level2 output-location-slide"><h2>Old Faithful Eruptions</h2><div class="cell output-location-slide" data-execution_count="1">
<div class="cell-output cell-output-stdout">
<pre><code>(272, 2)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="1">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe caption-top" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">eruptions</th>
<th data-quarto-table-cell-role="th">waiting</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>3.600</td>
<td>79</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1.800</td>
<td>54</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>3.333</td>
<td>74</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>2.283</td>
<td>62</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>4.533</td>
<td>85</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>2.883</td>
<td>55</td>
</tr>
</tbody>
</table>

</div>
</div>
</div></section><section id="old-faithful-geyser" class="slide level2">
<h2>Old Faithful Geyser</h2>
<iframe data-external="1" src="https://www.youtube.com/embed/Qxf3xzirBrs" width="100%" height="450" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<aside class="notes">
<p>Old Faithful, situated in Yellowstone National Park, is renowned as the world’s most famous geyser. It can reach eruption heights of up to 140 feet. Notably, its eruption intervals range between 60 and 110 minutes, contingent upon the duration of the preceding eruption.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p><strong>Attribution:</strong> Yellowstone National Park Trips</p>
</div></aside></section>
<section id="quick-visualization" class="slide level2">
<h2>Quick Visualization</h2>
<div id="46f1a5ef" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-2"><a></a></span>
<span id="cb3-3"><a></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">4</span>))</span>
<span id="cb3-4"><a></a>plt.scatter(df[<span class="st">"eruptions"</span>], df[<span class="st">"waiting"</span>], s<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb3-5"><a></a>plt.xlabel(<span class="st">"Eruption duration (min)"</span>)</span>
<span id="cb3-6"><a></a>plt.ylabel(<span class="st">"Waiting time to next eruption (min)"</span>)</span>
<span id="cb3-7"><a></a>plt.title(<span class="st">"Old Faithful: eruptions vs waiting"</span>)</span>
<span id="cb3-8"><a></a>plt.tight_layout()</span>
<span id="cb3-9"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-3-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img data-src="slides_files/figure-revealjs/cell-3-output-1.png" class="quarto-figure quarto-figure-center" width="565" height="372"></a></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<p>The duration of the current eruption appears to have a linear relationship with the subsequent wait time: shorter eruption durations tend to precede shorter wait times, while longer eruption durations are associated with longer wait times.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="problem" class="slide level2">
<h2>Problem</h2>
<ul>
<li>Predict the <strong>waiting time until the next eruption (min)</strong>, <span class="math inline">\(y\)</span>, based on the <strong>duration of the current eruption (min)</strong>, <span class="math inline">\(x\)</span>.</li>
</ul>
<aside class="notes">
<p>Selecting a problem characterized by a single attribute allows us to better focus our discussion and enhance the clarity of visualization.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<iframe data-external="1" src="https://www.youtube.com/embed/qxo8p8PtFeA?si=Buy1DF-T1qPsVE2S" width="100%" height="85%" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</section>
<section id="linear-regression-1" class="slide level2">
<h2>Linear Regression</h2>
<p>A <strong>linear model</strong> assumes that the value of the <strong>label</strong>, <span class="math inline">\(\hat{y_i}\)</span>, can be expressed as a <strong>linear combination</strong> of the feature values, <span class="math inline">\(x_i^{(j)}\)</span>: <span class="math display">\[
  \hat{y_i} = \theta_0 + \theta_1 x_i^{(1)} + \theta_2 x_i^{(2)} + \ldots + \theta_D x_i^{(D)}
\]</span></p>
<div class="fragment">
<p>Here, <span class="math inline">\(\theta_{j}\)</span> is the <span class="math inline">\(j\)</span>th parameter of the (linear) <strong>model</strong>, with <span class="math inline">\(\theta_0\)</span> being the <strong>bias</strong> term/parameter, and <span class="math inline">\(\theta_1 \ldots \theta_D\)</span> being the <strong>feature weights</strong>.</p>

<aside class="notes">
<p>In statistical contexts, the notation <span class="math inline">\(\hat{y_i}\)</span> is employed to denote the estimator of the true value <span class="math inline">\(y_i\)</span>. This represents the predicted or estimated outcome based on a given model.</p>
<p>Conversely, in machine learning, the notation <span class="math inline">\(h(x_i)\)</span> is used, where <span class="math inline">\(h\)</span> represents the hypothesis function or model applied to the input data <span class="math inline">\(x_i\)</span>. The hypothesis function <span class="math inline">\(h\)</span> is derived from a predefined hypothesis space, which encompasses the set of all possible models that can be used to map input data to predicted outcomes.</p>
<p>The parameter <span class="math inline">\(\theta_0\)</span> is called the <strong>bias term</strong> (also “intercept”) because:</p>
<ul>
<li>It <strong>shifts the prediction independently of the inputs</strong>.</li>
<li>Geometrically, it moves the regression hyperplane up or down (or left/right in classification), so the model is not forced to pass through the origin.</li>
<li>In machine learning terms, it acts like a <strong>constant offset</strong>, compensating for systematic effects not explained by the features.</li>
</ul>
<p>So it’s called “bias” because it introduces a fixed baseline to which the contributions of the other parameters are added.</p>
<p>In a machine learning model, the <strong>parameters</strong> are the <strong>weights</strong> and the <strong>biases</strong>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div>
<aside><div>
<p>In my presentations, I use <span class="math inline">\(\hat{y_i}\)</span> and <span class="math inline">\(h(x_i)\)</span> synonymously.</p>
</div></aside></section>
<section id="definition" class="slide level2">
<h2>Definition</h2>
<p><strong>Problem</strong>: find values for all the model parameters so that the model <strong>“best fits”</strong> the training data.</p>
<div class="fragment">
<ul>
<li>The <strong>Root Mean Square Error</strong> is a common performance measure for regression problems.</li>
</ul>
<p><span class="math display">\[
    \sqrt{\frac{1}{N}\sum_1^N [h(x_i) - y_i]^2}
\]</span></p>
<aside class="notes">
<p>In practical implementation, algorithms typically optimize the mean squared error (MSE) due to its mathematical tractability and the fact that it converges to the same parameter estimates as other error measures. While our current focus is on using MSE, the algorithm can be adapted to work with a range of objective functions.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div>
</section>
<section id="minimizing-rmse" class="slide level2">
<h2>Minimizing RMSE</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Linear_least_squares_example2.svg/2085px-Linear_least_squares_example2.svg.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img data-src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Linear_least_squares_example2.svg/2085px-Linear_least_squares_example2.svg.png" class="quarto-figure quarto-figure-center" height="500"></a></p>
</figure>
</div>

<aside><div>
<p><strong>Attribution</strong>: <a href="https://commons.wikimedia.org/wiki/File:Linear_least_squares_example2.svg">Krishnavedala</a>, <a href="https://creativecommons.org/licenses/by-sa/3.0">CC BY-SA 3.0</a>, via Wikimedia Commons</p>
</div></aside></section>
<section id="learning" class="slide level2">
<h2>Learning</h2>
<div id="39eafd0c" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> SGDRegressor</span>
<span id="cb4-2"><a></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb4-3"><a></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, r2_score</span>
<span id="cb4-4"><a></a></span>
<span id="cb4-5"><a></a><span class="co"># Prepare data</span></span>
<span id="cb4-6"><a></a>X <span class="op">=</span> df[[<span class="st">"eruptions"</span>]].values  <span class="co"># shape (n_samples, 1)</span></span>
<span id="cb4-7"><a></a>y <span class="op">=</span> df[<span class="st">"waiting"</span>].values      <span class="co"># shape (n_samples,)</span></span>
<span id="cb4-8"><a></a></span>
<span id="cb4-9"><a></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb4-10"><a></a>    X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb4-11"><a></a>)</span>
<span id="cb4-12"><a></a></span>
<span id="cb4-13"><a></a><span class="co"># Fit via SGDRegressor — linear model via gradient descent</span></span>
<span id="cb4-14"><a></a>sgd <span class="op">=</span> SGDRegressor(</span>
<span id="cb4-15"><a></a>    loss<span class="op">=</span><span class="st">"squared_error"</span>,</span>
<span id="cb4-16"><a></a>    penalty<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb4-17"><a></a>    learning_rate<span class="op">=</span><span class="st">"constant"</span>,</span>
<span id="cb4-18"><a></a>    eta0<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb4-19"><a></a>    max_iter<span class="op">=</span><span class="dv">2000</span>,</span>
<span id="cb4-20"><a></a>    tol<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb4-21"><a></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb4-22"><a></a>)</span>
<span id="cb4-23"><a></a></span>
<span id="cb4-24"><a></a>sgd.fit(X_train, y_train)</span>
<span id="cb4-25"><a></a></span>
<span id="cb4-26"><a></a><span class="bu">print</span>(<span class="st">"Learned parameters:"</span>)</span>
<span id="cb4-27"><a></a><span class="bu">print</span>(<span class="ss">f"  intercept = </span><span class="sc">{</span>sgd<span class="sc">.</span>intercept_[<span class="dv">0</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb4-28"><a></a><span class="bu">print</span>(<span class="ss">f"  slope     = </span><span class="sc">{</span>sgd<span class="sc">.</span>coef_[<span class="dv">0</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb4-29"><a></a></span>
<span id="cb4-30"><a></a>y_pred <span class="op">=</span> sgd.predict(X_test)</span>
<span id="cb4-31"><a></a><span class="bu">print</span>(<span class="ss">f"Test MSE = </span><span class="sc">{</span>mean_squared_error(y_test, y_pred)<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb4-32"><a></a><span class="bu">print</span>(<span class="ss">f"Test R²  = </span><span class="sc">{</span>r2_score(y_test, y_pred)<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Learned parameters:
  intercept = 32.910
  slope     = 10.503
Test MSE = 43.02
Test R²  = 0.671</code></pre>
</div>
</div>
</section>
<section id="visualization" class="slide level2">
<h2>Visualization</h2>
<div id="a15a3032" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a></a></span>
<span id="cb6-3"><a></a><span class="co"># Scatter the data</span></span>
<span id="cb6-4"><a></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">4</span>))</span>
<span id="cb6-5"><a></a>plt.scatter(X, y, color<span class="op">=</span><span class="st">"steelblue"</span>, s<span class="op">=</span><span class="dv">30</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="st">"data"</span>)</span>
<span id="cb6-6"><a></a></span>
<span id="cb6-7"><a></a><span class="co"># Plot the fitted line</span></span>
<span id="cb6-8"><a></a>x_line <span class="op">=</span> np.linspace(<span class="dv">0</span>, X.<span class="bu">max</span>(), <span class="dv">100</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb6-9"><a></a>y_line <span class="op">=</span> sgd.predict(x_line)</span>
<span id="cb6-10"><a></a>plt.plot(x_line, y_line, color<span class="op">=</span><span class="st">"red"</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">"fitted line"</span>)</span>
<span id="cb6-11"><a></a></span>
<span id="cb6-12"><a></a>plt.xlabel(<span class="st">"Eruption duration (min)"</span>)</span>
<span id="cb6-13"><a></a>plt.ylabel(<span class="st">"Waiting time to next eruption (min)"</span>)</span>
<span id="cb6-14"><a></a>plt.title(<span class="st">"Old Faithful: Linear regression via SGD"</span>)</span>
<span id="cb6-15"><a></a>plt.legend()</span>
<span id="cb6-16"><a></a>plt.tight_layout()</span>
<span id="cb6-17"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><a href="slides_files/figure-revealjs/cell-5-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img data-src="slides_files/figure-revealjs/cell-5-output-1.png" width="565" height="372"></a></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<p>In the plot above, the line <span class="math inline">\(x=0\)</span> is included to facilitate the visualization of the intercept, which is given by <span class="math inline">\(\theta_0 = 32.910\)</span>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="characteristics" class="slide level2 smaller">
<h2>Characteristics</h2>
<p>A typical <strong>learning algorithm</strong> comprises the following components:</p>
<ol type="1">
<li class="fragment">A <strong>model</strong>, often consisting of a set of <strong>parameters</strong> whose values will be <strong>“learnt”</strong>.</li>
<li class="fragment">An <strong>objective function</strong>.
<ul>
<li class="fragment">In the case of <strong>regression</strong>, this is often a <strong>loss function</strong>, a function that quantifies misclassification. The <strong>Root Mean Square Error</strong> is a common loss function for regression problems. <span class="math display">\[
\sqrt{\frac{1}{N}\sum_1^N [h(x_i) - y_i]^2}
\]</span></li>
</ul></li>
<li class="fragment"><strong>Optimization</strong> algorithm</li>
</ol>
</section>
<section id="optimization" class="slide level2">
<h2>Optimization</h2>
<p><strong>Until</strong> some termination criteria is met<sup>1</sup>:</p>
<ul>
<li class="fragment"><strong>Evaluate</strong> the loss function, comparing <span class="math inline">\(h(x_i)\)</span> to <span class="math inline">\(y_i\)</span>.</li>
<li class="fragment"><strong>Make small changes to the parameters</strong>, in a way that reduces the value of the loss function.</li>
</ul>
<aside><ol class="aside-footnotes"><li id="fn1"><p>E.g. the value of the <strong>loss function no longer decreases</strong> or the <strong>maximum number of iterations</strong>.</p></li></ol></aside></section>
<section id="remarks" class="slide level2">
<h2>Remarks</h2>
<ul>
<li class="fragment">It is important to separate the <strong>optimization algorithm</strong> from the <strong>problem</strong> it addresses.</li>
<li class="fragment">For <strong>linear regression</strong>, an exact analytical solution exists, but it presents certain limitations.</li>
<li class="fragment"><strong>Gradient descent</strong> serves as a general algorithm applicable not only to linear regression, but also to logistic regression, deep learning, t-SNE (t-distributed Stochastic Neighbor Embedding), among various other problems.</li>
<li class="fragment">There exists a diverse range of optimization algorithms that <strong>do not rely on gradient-based methods</strong>.</li>
</ul>
</section>
<section id="optimization-single-feature" class="slide level2">
<h2>Optimization — single feature</h2>
<ul>
<li class="fragment"><p><strong>Model</strong> (hypothesis):<br>
<span class="math display">\[
h(x_i; \theta) = \theta_0 + \theta_1 x_i^{(1)}
\]</span></p></li>
<li class="fragment"><p><strong>Loss/cost function</strong>:<br>
<span class="math display">\[
J(\theta_0, \theta_1) = \frac{1}{N}\sum_{i=1}^N [h(x_i;\theta) - y_i]^2
\]</span></p></li>
</ul>

<aside class="notes">
<p>This screen is paramount for our presentation, and it is essential to grasp its content fully, as it often causes confusion.</p>
<p>In machine learning, an algorithm generates a model, denoted as <span class="math inline">\(h\)</span>. This model is derived from the training data, represented by <span class="math inline">\(X\)</span>. After the model is trained, it can be utilized to predict outcomes for new, unseen data points, denoted by <span class="math inline">\(x_{\mathrm{new}}\)</span>. Once trained, the model’s parameters, <span class="math inline">\(\theta\)</span>, become fixed. The model functions by mapping each input <span class="math inline">\(x_i\)</span> to a predicted output <span class="math inline">\(\hat{y}_i\)</span>, <span class="math inline">\(x_i \mapsto \hat{y}_i\)</span>.</p>
<p>The cost function is used to determine the optimal parameter values, <span class="math inline">\(\theta\)</span>, for the model <span class="math inline">\(h\)</span>. On this screen, the loss is specified as the mean squared error. Our goal is to identify <span class="math inline">\(\theta\)</span> such that the model <span class="math inline">\(h\)</span> minimizes its error on the training dataset <span class="math inline">\(X\)</span>. The loss function maps the parameter pair <span class="math inline">\((\theta_0, \theta_1)\)</span> to a non-negative real number <span class="math inline">\(\mathbb{R}_{\ge0}\)</span>, <span class="math inline">\((\theta_0,\theta_1)\mapsto\mathbb{R}_{\ge0}\)</span>, which aggregates errors across all data points.</p>
<p>Gradient descent is a process that operates in the parameter space, where it adjusts <span class="math inline">\(\theta\)</span> to minimize the loss, rather than in the feature space of the data.</p>
<p>The notation <span class="math inline">\(h(x_i; \theta)\)</span> indicates that the value of the function <span class="math inline">\(h\)</span> (of the model) depends on the input example <span class="math inline">\(x_i\)</span> as well as the parameters <span class="math inline">\(\theta\)</span>. The semicolon is used to semantically differentiate these two sets of values. This convention comes from the field of statistics. In machine learning, we also use the notation <span class="math inline">\(h_{\theta}(x_i)\)</span>, which can be considered more appropriate. In this context, we refer to an indexed family of functions, where <span class="math inline">\(\theta\)</span> specifically determines which function <span class="math inline">\(h_\theta\)</span> should be used. This notation is read as follows: “the function <span class="math inline">\(h\)</span>, parameterized by <span class="math inline">\(\theta\)</span>, applied to the input <span class="math inline">\(x_i\)</span>.”</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p><strong>Goal</strong>: find <span class="math inline">\(\theta_0, \theta_1\)</span> that minimize <span class="math inline">\(J\)</span>, by iteratively updating the parameters.</p>
</div></aside></section>
<section id="hypothesis-vs-parameter-space" class="slide level2">
<h2>Hypothesis vs Parameter Space</h2>
<div id="c4bcc7cd" class="cell" data-execution_count="5">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-6-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img data-src="slides_files/figure-revealjs/cell-6-output-1.png" class="quarto-figure quarto-figure-center" width="1124" height="450"></a></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<p>In this example, we use the Old Faithful geyser eruption dataset.</p>
<p>The figure on the left illustrates the data within its feature space, where each line represents a distinct hypothesis. For instance, the horizontal blue line corresponds to the hypothesis defined by parameters <span class="math inline">\((\theta_0=0, \theta_1=0)\)</span>, while the orange line corresponds to <span class="math inline">\((\theta_0=10, \theta_1=2)\)</span>. The sequence of model parameters, or hypotheses, has been deliberately designed to elucidate the underlying concepts.</p>
<p>Conversely, the figure on the right depicts the parameter space, which is where optimization is performed. In this space, the vertical axis indicates the Mean Squared Error (MSE) for all combinations of <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span>. Specifically, a model characterized by these parameters would incur this level of error with the specified training data. During the training process, the data remains constant, whereas the parameters <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span> are adjusted to minimize the error.</p>
<p>In the figure on the right, the progression of model parameters, denoted as (<span class="math inline">\(\theta_0, \theta_1\)</span>) with the sequence <span class="math inline">\(\{(0.000, 0.000), (10.000, 2.000), (20.000, 4.000), (30.000, 7.000), (32.910, 10.503)\}\)</span>, illustrates a reduction in the mean squared error (MSE), <span class="math inline">\(J(\theta)\)</span>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="hypothesis-vs-parameter-space-1" class="slide level2">
<h2>Hypothesis vs Parameter Space</h2>
<div id="32b83cef" class="cell" data-execution_count="6">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-7-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img data-src="slides_files/figure-revealjs/cell-7-output-1.png" class="quarto-figure quarto-figure-center" width="1141" height="417"></a></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<p>Thes figures illustrate the same concept, but using a contour plot.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="derivative" class="title-slide slide level1 center">
<h1>Derivative</h1>

</section>
<section id="derivative-1" class="slide level2">
<h2>Derivative</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div id="c95a2c76" class="cell" data-execution_count="7">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-8-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img data-src="slides_files/figure-revealjs/cell-8-output-1.png" class="quarto-figure quarto-figure-center" width="470" height="467"></a></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li class="fragment">We will start with a <strong>single-variable function</strong>.</li>
<li class="fragment">Think of this as our <strong>loss function</strong>, which we aim to minimize; to reduce the average discrepancy between expected and predicted values.</li>
<li class="fragment">Here, I am using <span class="math inline">\(t\)</span> to avoid any confusion with the attributes of our training examples.</li>
</ul>
</div></div>
</section>
<section id="source-code" class="slide level2">
<h2>Source code</h2>
<div id="710ea04d" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a></a><span class="im">from</span> sympy <span class="im">import</span> <span class="op">*</span></span>
<span id="cb7-2"><a></a></span>
<span id="cb7-3"><a></a>x <span class="op">=</span> symbols(<span class="st">'t'</span>)</span>
<span id="cb7-4"><a></a></span>
<span id="cb7-5"><a></a>f <span class="op">=</span> t<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">4</span><span class="op">*</span>t <span class="op">+</span> <span class="dv">7</span></span>
<span id="cb7-6"><a></a></span>
<span id="cb7-7"><a></a>plot(f)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>

<aside><div>
<p>On the previous slide, I’ve used <a href="https://www.sympy.org/"><strong>SymPy</strong></a>, a library for <strong>symbolic mathematics</strong>.</p>
</div></aside></section>
<section id="derivative-2" class="slide level2">
<h2>Derivative</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div id="18c54f6a" class="cell" data-execution_count="9">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-10-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img data-src="slides_files/figure-revealjs/cell-10-output-1.png" class="quarto-figure quarto-figure-center" width="802" height="442"></a></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li class="fragment"><p>The graph of the <strong>derivative</strong>, <span class="math inline">\(f^{'}(t)\)</span>, is depicted in <strong>red</strong>.</p></li>
<li class="fragment"><p>The <strong>derivative</strong> indicates how changes in the input affect the output, <span class="math inline">\(f(t)\)</span>.</p></li>
<li class="fragment"><p>The magnitude of the <strong>derivative</strong> at <span class="math inline">\(t = -2\)</span> is <span class="math inline">\(0\)</span>.</p></li>
<li class="fragment"><p>This point corresponds to the <strong>minimum</strong> of our function.</p></li>
</ul>
</div></div>
<aside class="notes">
<p>Near <span class="math inline">\(t = -2\)</span>, variations in <span class="math inline">\(t\)</span> have minimal impact on the output, <span class="math inline">\(f(t)\)</span>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="derivative-3" class="slide level2">
<h2>Derivative</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div id="c7fbebda" class="cell" data-execution_count="10">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-11-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img data-src="slides_files/figure-revealjs/cell-11-output-1.png" class="quarto-figure quarto-figure-center" width="800" height="442"></a></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li class="fragment">When evaluated at a <strong>specific point</strong>, the derivative indicates the <strong>slope</strong> of the <strong>tangent line</strong> to the graph of the function at that point.</li>
<li class="fragment">At <span class="math inline">\(t= -2\)</span>, the <strong>slope</strong> of the <strong>tangent line</strong> is 0.</li>
</ul>
</div></div>
</section>
<section id="derivative-4" class="slide level2">
<h2>Derivative</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div id="10147fca" class="cell" data-execution_count="11">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-12-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img data-src="slides_files/figure-revealjs/cell-12-output-1.png" class="quarto-figure quarto-figure-center" width="800" height="442"></a></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li class="fragment"><p>A <strong>positive derivative</strong> indicates that <strong>increasing the input variable</strong> will <strong>increase the output value</strong>.</p></li>
<li class="fragment"><p>Additionally, the <strong>magnitude</strong> of the derivative quantifies how <strong>rapidly</strong> the output changes.</p></li>
</ul>
</div></div>
</section>
<section id="derivative-5" class="slide level2">
<h2>Derivative</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div id="b4d87f40" class="cell" data-execution_count="12">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-13-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11"><img data-src="slides_files/figure-revealjs/cell-13-output-1.png" class="quarto-figure quarto-figure-center" width="800" height="442"></a></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li class="fragment"><p>A <strong>negative derivative</strong> indicates that <strong>increasing the input variable</strong> will <strong>decrease the output value</strong>.</p></li>
<li class="fragment"><p>Additionally, the <strong>magnitude</strong> of the derivative quantifies how <strong>rapidly</strong> the output changes.</p></li>
</ul>
</div></div>
</section>
<section id="source-code-1" class="slide level2">
<h2>Source code</h2>
<div id="9a3c0ecc" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb8-2"><a></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-3"><a></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-4"><a></a></span>
<span id="cb8-5"><a></a><span class="co"># Define the variable and function</span></span>
<span id="cb8-6"><a></a>t <span class="op">=</span> sp.symbols(<span class="st">'t'</span>)</span>
<span id="cb8-7"><a></a>f <span class="op">=</span> t<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">4</span><span class="op">*</span>t <span class="op">+</span> <span class="dv">7</span></span>
<span id="cb8-8"><a></a></span>
<span id="cb8-9"><a></a><span class="co"># Compute the derivative</span></span>
<span id="cb8-10"><a></a>f_prime <span class="op">=</span> sp.diff(f, t)</span>
<span id="cb8-11"><a></a></span>
<span id="cb8-12"><a></a><span class="co"># Lambdify the functions for numerical plotting</span></span>
<span id="cb8-13"><a></a>f_func <span class="op">=</span> sp.lambdify(t, f, <span class="st">"numpy"</span>)</span>
<span id="cb8-14"><a></a>f_prime_func <span class="op">=</span> sp.lambdify(t, f_prime, <span class="st">"numpy"</span>)</span>
<span id="cb8-15"><a></a></span>
<span id="cb8-16"><a></a><span class="co"># Generate t values for plotting</span></span>
<span id="cb8-17"><a></a>t_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">400</span>)</span>
<span id="cb8-18"><a></a></span>
<span id="cb8-19"><a></a><span class="co"># Get y values for the function and its derivative</span></span>
<span id="cb8-20"><a></a>f_vals <span class="op">=</span> f_func(t_vals)</span>
<span id="cb8-21"><a></a>f_prime_vals <span class="op">=</span> f_prime_func(t_vals)</span>
<span id="cb8-22"><a></a></span>
<span id="cb8-23"><a></a><span class="co"># Plot the function and its derivative</span></span>
<span id="cb8-24"><a></a>plt.plot(t_vals, f_vals, label<span class="op">=</span><span class="vs">r'</span><span class="dv">$</span><span class="vs">f</span><span class="kw">(</span><span class="vs">t</span><span class="kw">)</span><span class="vs"> = t</span><span class="dv">^</span><span class="vs">2 </span><span class="op">+</span><span class="vs"> 4t </span><span class="op">+</span><span class="vs"> 7</span><span class="dv">$</span><span class="vs">'</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb8-25"><a></a>plt.plot(t_vals, f_prime_vals, label<span class="op">=</span><span class="vs">r"</span><span class="dv">$</span><span class="vs">f'</span><span class="kw">(</span><span class="vs">t</span><span class="kw">)</span><span class="vs"> = 2t </span><span class="op">+</span><span class="vs"> 4</span><span class="dv">$</span><span class="vs">"</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb8-26"><a></a></span>
<span id="cb8-27"><a></a><span class="co"># Fill the area below the derivative where it's negative</span></span>
<span id="cb8-28"><a></a>plt.fill_between(t_vals, f_prime_vals, where<span class="op">=</span>(f_prime_vals <span class="op">&gt;</span> <span class="dv">0</span>), color<span class="op">=</span><span class="st">'red'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-29"><a></a></span>
<span id="cb8-30"><a></a><span class="co"># Add labels and legend</span></span>
<span id="cb8-31"><a></a>plt.axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>,linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-32"><a></a>plt.axvline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>,linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-33"><a></a>plt.title(<span class="st">'Function and Derivative'</span>)</span>
<span id="cb8-34"><a></a>plt.xlabel(<span class="st">'t'</span>)</span>
<span id="cb8-35"><a></a>plt.ylabel(<span class="st">'y'</span>)</span>
<span id="cb8-36"><a></a>plt.legend()</span>
<span id="cb8-37"><a></a></span>
<span id="cb8-38"><a></a><span class="co"># Show the plot</span></span>
<span id="cb8-39"><a></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb8-40"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section></section>
<section>
<section id="gradient-descent" class="title-slide slide level1 center">
<h1>Gradient Descent</h1>

</section>
<section id="gradient-descent-single-feature" class="slide level2">
<h2>Gradient Descent — Single Feature</h2>
<ul>
<li class="fragment"><p><strong>Model</strong> (hypothesis):<br>
<span class="math display">\[
h(x_i; \theta) = \theta_0 + \theta_1 x_i^{(1)}
\]</span></p></li>
<li class="fragment"><p><strong>Loss/cost function</strong>:<br>
<span class="math display">\[
J(\theta_0, \theta_1) = \frac{1}{N}\sum_{i=1}^N [h(x_i;\theta) - y_i]^2
\]</span></p></li>
</ul>
</section>
<section id="gradient-descent---intuition" class="slide level2">
<h2>Gradient descent - intuition</h2>
<iframe data-external="1" src="https://www.youtube.com/embed/i62czvwDlsw" width="889" height="500" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</section>
<section id="gradient-descent---step-by-step" class="slide level2">
<h2>Gradient Descent - Step-by-Step</h2>
<iframe data-external="1" src="https://www.youtube.com/embed/sDv4f4s2SB8" width="889" height="500" title="DGradient Descent, Step-by-Step" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</section>
<section id="gradient-descent---single-value" class="slide level2">
<h2>Gradient descent - single value</h2>
<ul>
<li class="fragment"><strong>Initialization:</strong> <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span> - either with random values or zeros.</li>
<li class="fragment"><strong>Loop:</strong>
<ul>
<li class="fragment">repeat until convergence: <span class="math display">\[
\theta_j := \theta_j - \alpha \frac {\partial}{\partial \theta_j}J(\theta_0, \theta_1) , \text{for } j=0 \text{ and } j=1
\]</span></li>
</ul></li>
<li class="fragment"><span class="math inline">\(\alpha\)</span> is called the <strong>learning rate</strong> - this is the size of each step.</li>
<li class="fragment"><span class="math inline">\(\frac {\partial}{\partial \theta_j}J(\theta_0, \theta_1)\)</span> is the <strong>partial derivative</strong> with respect to <span class="math inline">\(\theta_j\)</span>.</li>
</ul>
<aside class="notes">
<p>A <strong>partial derivative</strong> represents the rate of change of a multivariable function <strong>with respect to one of its variables</strong>, while <strong>keeping the other variables constant</strong>.</p>
<p>For the algorithm to be mathematically sound, all the <span class="math inline">\(\theta_j\)</span> must be updated <strong>simultaneously</strong>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="gradient-descent---single-value-1" class="slide level2">
<h2>Gradient Descent - Single Value</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div id="2ee657fd" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb9-2"><a></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-3"><a></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-4"><a></a></span>
<span id="cb9-5"><a></a><span class="co"># Define the variable and function</span></span>
<span id="cb9-6"><a></a>t <span class="op">=</span> sp.symbols(<span class="st">'t'</span>)</span>
<span id="cb9-7"><a></a>f <span class="op">=</span> t<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">4</span><span class="op">*</span>t <span class="op">+</span> <span class="dv">7</span></span>
<span id="cb9-8"><a></a></span>
<span id="cb9-9"><a></a><span class="co"># Compute the derivative</span></span>
<span id="cb9-10"><a></a>f_prime <span class="op">=</span> sp.diff(f, t)</span>
<span id="cb9-11"><a></a></span>
<span id="cb9-12"><a></a><span class="co"># Lambdify the functions for numerical plotting</span></span>
<span id="cb9-13"><a></a>f_func <span class="op">=</span> sp.lambdify(t, f, <span class="st">"numpy"</span>)</span>
<span id="cb9-14"><a></a>f_prime_func <span class="op">=</span> sp.lambdify(t, f_prime, <span class="st">"numpy"</span>)</span>
<span id="cb9-15"><a></a></span>
<span id="cb9-16"><a></a><span class="co"># Generate t values for plotting</span></span>
<span id="cb9-17"><a></a>t_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">400</span>)</span>
<span id="cb9-18"><a></a></span>
<span id="cb9-19"><a></a><span class="co"># Get y values for the function and its derivative</span></span>
<span id="cb9-20"><a></a>f_vals <span class="op">=</span> f_func(t_vals)</span>
<span id="cb9-21"><a></a>f_prime_vals <span class="op">=</span> f_prime_func(t_vals)</span>
<span id="cb9-22"><a></a></span>
<span id="cb9-23"><a></a><span class="co"># Plot the function and its derivative</span></span>
<span id="cb9-24"><a></a>plt.plot(t_vals, f_vals, label<span class="op">=</span><span class="vs">r'</span><span class="dv">$</span><span class="vs">J</span><span class="dv">$</span><span class="vs">'</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb9-25"><a></a>plt.plot(t_vals, f_prime_vals, label<span class="op">=</span><span class="vs">r"</span><span class="dv">$</span><span class="ch">\f</span><span class="vs">rac {</span><span class="er">\</span><span class="vs">partial}{</span><span class="er">\</span><span class="vs">partial </span><span class="ch">\t</span><span class="vs">heta_j}J</span><span class="kw">(</span><span class="ch">\t</span><span class="vs">heta</span><span class="kw">)</span><span class="dv">$</span><span class="vs">"</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb9-26"><a></a></span>
<span id="cb9-27"><a></a><span class="co"># Add labels and legend</span></span>
<span id="cb9-28"><a></a>plt.axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>,linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-29"><a></a>plt.axvline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>,linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-30"><a></a>plt.title(<span class="st">'Function and Derivative'</span>)</span>
<span id="cb9-31"><a></a>plt.xlabel(<span class="vs">r'</span><span class="dv">$</span><span class="ch">\t</span><span class="vs">heta_j</span><span class="dv">$</span><span class="vs">'</span>)</span>
<span id="cb9-32"><a></a>plt.ylabel(<span class="vs">r'</span><span class="dv">$</span><span class="vs">J</span><span class="dv">$</span><span class="vs">'</span>)</span>
<span id="cb9-33"><a></a>plt.legend()</span>
<span id="cb9-34"><a></a></span>
<span id="cb9-35"><a></a><span class="co"># Show the plot</span></span>
<span id="cb9-36"><a></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb9-37"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-15-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img data-src="slides_files/figure-revealjs/cell-15-output-1.png" class="quarto-figure quarto-figure-center" width="803" height="446"></a></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li class="fragment"><p>When the value of <span class="math inline">\(\theta_j\)</span> is in the range <span class="math inline">\([- \inf, -2)\)</span>, <span class="math inline">\(\frac {\partial}{\partial \theta_j}J(\theta)\)</span> has a <strong>negative</strong> value.</p></li>
<li class="fragment"><p>Therefore, <span class="math inline">\(- \alpha \frac {\partial}{\partial \theta_j}J(\theta)\)</span> is <strong>positive</strong>.</p></li>
<li class="fragment"><p>Accordingly, the value of <span class="math inline">\(\theta_j\)</span> is <strong>increased</strong>.</p></li>
</ul>
</div></div>

<aside><div>
<p>Updating rule: <span class="math inline">\(\theta_j := \theta_j - \alpha \frac {\partial}{\partial \theta_j}J(\theta_0, \theta_1) , \text{for } j=0 \text{ and } j=1\)</span>.</p>
</div></aside></section>
<section id="gradient-descent---single-value-2" class="slide level2">
<h2>Gradient Descent - Single Value</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div id="cd0bedf2" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb10-2"><a></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-3"><a></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-4"><a></a></span>
<span id="cb10-5"><a></a><span class="co"># Define the variable and function</span></span>
<span id="cb10-6"><a></a>t <span class="op">=</span> sp.symbols(<span class="st">'t'</span>)</span>
<span id="cb10-7"><a></a>f <span class="op">=</span> t<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">4</span><span class="op">*</span>t <span class="op">+</span> <span class="dv">7</span></span>
<span id="cb10-8"><a></a></span>
<span id="cb10-9"><a></a><span class="co"># Compute the derivative</span></span>
<span id="cb10-10"><a></a>f_prime <span class="op">=</span> sp.diff(f, t)</span>
<span id="cb10-11"><a></a></span>
<span id="cb10-12"><a></a><span class="co"># Lambdify the functions for numerical plotting</span></span>
<span id="cb10-13"><a></a>f_func <span class="op">=</span> sp.lambdify(t, f, <span class="st">"numpy"</span>)</span>
<span id="cb10-14"><a></a>f_prime_func <span class="op">=</span> sp.lambdify(t, f_prime, <span class="st">"numpy"</span>)</span>
<span id="cb10-15"><a></a></span>
<span id="cb10-16"><a></a><span class="co"># Generate t values for plotting</span></span>
<span id="cb10-17"><a></a>t_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">400</span>)</span>
<span id="cb10-18"><a></a></span>
<span id="cb10-19"><a></a><span class="co"># Get y values for the function and its derivative</span></span>
<span id="cb10-20"><a></a>f_vals <span class="op">=</span> f_func(t_vals)</span>
<span id="cb10-21"><a></a>f_prime_vals <span class="op">=</span> f_prime_func(t_vals)</span>
<span id="cb10-22"><a></a></span>
<span id="cb10-23"><a></a><span class="co"># Plot the function and its derivative</span></span>
<span id="cb10-24"><a></a>plt.plot(t_vals, f_vals, label<span class="op">=</span><span class="vs">r'</span><span class="dv">$</span><span class="vs">J</span><span class="dv">$</span><span class="vs">'</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb10-25"><a></a>plt.plot(t_vals, f_prime_vals, label<span class="op">=</span><span class="vs">r"</span><span class="dv">$</span><span class="ch">\f</span><span class="vs">rac {</span><span class="er">\</span><span class="vs">partial}{</span><span class="er">\</span><span class="vs">partial </span><span class="ch">\t</span><span class="vs">heta_j}J</span><span class="kw">(</span><span class="ch">\t</span><span class="vs">heta</span><span class="kw">)</span><span class="dv">$</span><span class="vs">"</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb10-26"><a></a></span>
<span id="cb10-27"><a></a><span class="co"># Add labels and legend</span></span>
<span id="cb10-28"><a></a>plt.axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>,linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-29"><a></a>plt.axvline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>,linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-30"><a></a>plt.title(<span class="st">'Function and Derivative'</span>)</span>
<span id="cb10-31"><a></a>plt.xlabel(<span class="vs">r'</span><span class="dv">$</span><span class="ch">\t</span><span class="vs">heta_j</span><span class="dv">$</span><span class="vs">'</span>)</span>
<span id="cb10-32"><a></a>plt.ylabel(<span class="vs">r'</span><span class="dv">$</span><span class="vs">J</span><span class="dv">$</span><span class="vs">'</span>)</span>
<span id="cb10-33"><a></a>plt.legend()</span>
<span id="cb10-34"><a></a></span>
<span id="cb10-35"><a></a><span class="co"># Show the plot</span></span>
<span id="cb10-36"><a></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb10-37"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-16-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img data-src="slides_files/figure-revealjs/cell-16-output-1.png" class="quarto-figure quarto-figure-center" width="803" height="446"></a></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li class="fragment"><p>When the value of <span class="math inline">\(\theta_j\)</span> is in the range <span class="math inline">\((-2, \infty]\)</span>, <span class="math inline">\(\frac {\partial}{\partial \theta_j}J(\theta)\)</span> has a <strong>positive</strong> value.</p></li>
<li class="fragment"><p>Therefore, <span class="math inline">\(- \alpha \frac {\partial}{\partial \theta_j}J(\theta)\)</span> is <strong>negative</strong>.</p></li>
<li class="fragment"><p>Accordingly, the value of <span class="math inline">\(\theta_j\)</span> is <strong>decreased</strong>.</p></li>
</ul>
</div></div>

<aside><div>
<p>Updating rule: <span class="math inline">\(\theta_j := \theta_j - \alpha \frac {\partial}{\partial \theta_j}J(\theta_0, \theta_1) , \text{for } j=0 \text{ and } j=1\)</span>.</p>
</div></aside></section>
<section id="partial-derivatives" class="slide level2 scrollable smaller">
<h2>Partial derivatives</h2>
<p>Given</p>
<p><span class="math display">\[
J(\theta_0, \theta_1) = \frac{1}{N}\sum_1^N [h(x_i) - y_i]^2 = \frac{1}{N}\sum_1^N [\theta_0 + \theta_1 x_i - y_i]^2
\]</span></p>
<div class="fragment">
<p>We have</p>
<p><span class="math display">\[
\frac {\partial}{\partial \theta_0}J(\theta_0, \theta_1) = \frac{2}{N} \sum\limits_{i=1}^{N} [\theta_0 - \theta_1 x_i - y_{i}]
\]</span></p>
</div>
<div class="fragment">
<p>and</p>
<p><span class="math display">\[
\frac {\partial}{\partial \theta_1}J(\theta_0, \theta_1) = \frac{2}{N} \sum\limits_{i=1}^{N} x_{i} [\theta_0 + \theta_1 x_i - y_{i}]
\]</span></p>
</div>
</section>
<section id="partial-derivate-sympy" class="slide level2">
<h2>Partial derivate (SymPy)</h2>
<div id="71835982" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a></a><span class="im">from</span> IPython.display <span class="im">import</span> Math, display</span>
<span id="cb11-2"><a></a><span class="im">from</span> sympy <span class="im">import</span> <span class="op">*</span></span>
<span id="cb11-3"><a></a></span>
<span id="cb11-4"><a></a><span class="co"># Define the symbols</span></span>
<span id="cb11-5"><a></a></span>
<span id="cb11-6"><a></a>theta_0, theta_1, x_i, y_i <span class="op">=</span> symbols(<span class="st">'theta_0 theta_1 x_i y_i'</span>)</span>
<span id="cb11-7"><a></a></span>
<span id="cb11-8"><a></a><span class="co"># Define the hypothesis function:</span></span>
<span id="cb11-9"><a></a></span>
<span id="cb11-10"><a></a>h <span class="op">=</span> theta_0 <span class="op">+</span> theta_1 <span class="op">*</span> x_i</span>
<span id="cb11-11"><a></a></span>
<span id="cb11-12"><a></a><span class="bu">print</span>(<span class="st">"Hypothesis function:"</span>)</span>
<span id="cb11-13"><a></a></span>
<span id="cb11-14"><a></a>display(Math(<span class="st">'h(x) = '</span> <span class="op">+</span> latex(h)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Hypothesis function:</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p><span class="math inline">\(\displaystyle h(x) = \theta_{0} + \theta_{1} x_{i}\)</span></p>
</div>
</div>
</section>
<section id="partial-derivate-sympy-1" class="slide level2">
<h2>Partial derivate (SymPy)</h2>
<div id="200caede" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a></a>N <span class="op">=</span> Symbol(<span class="st">'N'</span>, integer<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-2"><a></a></span>
<span id="cb13-3"><a></a><span class="co"># Define the loss function (mean squared error)</span></span>
<span id="cb13-4"><a></a></span>
<span id="cb13-5"><a></a>J <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>N) <span class="op">*</span> Sum((h <span class="op">-</span> y_i)<span class="op">**</span><span class="dv">2</span>, (x_i, <span class="dv">1</span>, N))</span>
<span id="cb13-6"><a></a></span>
<span id="cb13-7"><a></a><span class="bu">print</span>(<span class="st">"Loss function:"</span>)</span>
<span id="cb13-8"><a></a></span>
<span id="cb13-9"><a></a>display(Math(<span class="st">'J = '</span> <span class="op">+</span> latex(J)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss function:</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p><span class="math inline">\(\displaystyle J = \frac{\sum_{x_{i}=1}^{N} \left(\theta_{0} + \theta_{1} x_{i} - y_{i}\right)^{2}}{N}\)</span></p>
</div>
</div>
</section>
<section id="partial-derivate-sympy-2" class="slide level2">
<h2>Partial derivate (SymPy)</h2>
<div id="501e7529" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a></a><span class="co"># Calculate the partial derivative with respect to theta_0</span></span>
<span id="cb15-2"><a></a></span>
<span id="cb15-3"><a></a>partial_derivative_theta_0 <span class="op">=</span> diff(J, theta_0)</span>
<span id="cb15-4"><a></a></span>
<span id="cb15-5"><a></a><span class="bu">print</span>(<span class="st">"Partial derivative with respect to theta_0:"</span>)</span>
<span id="cb15-6"><a></a></span>
<span id="cb15-7"><a></a>display(Math(latex(partial_derivative_theta_0)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Partial derivative with respect to theta_0:</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p><span class="math inline">\(\displaystyle \frac{\sum_{x_{i}=1}^{N} \left(2 \theta_{0} + 2 \theta_{1} x_{i} - 2 y_{i}\right)}{N}\)</span></p>
</div>
</div>
</section>
<section id="partial-derivate-sympy-3" class="slide level2">
<h2>Partial derivate (SymPy)</h2>
<div id="2fad3ba3" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a></a><span class="co"># Calculate the partial derivative with respect to theta_1</span></span>
<span id="cb17-2"><a></a></span>
<span id="cb17-3"><a></a>partial_derivative_theta_1 <span class="op">=</span> diff(J, theta_1)</span>
<span id="cb17-4"><a></a></span>
<span id="cb17-5"><a></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Partial derivative with respect to theta_1:"</span>)</span>
<span id="cb17-6"><a></a></span>
<span id="cb17-7"><a></a>display(Math(latex(partial_derivative_theta_1)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Partial derivative with respect to theta_1:</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p><span class="math inline">\(\displaystyle \frac{\sum_{x_{i}=1}^{N} 2 x_{i} \left(\theta_{0} + \theta_{1} x_{i} - y_{i}\right)}{N}\)</span></p>
</div>
</div>
</section>
<section id="multivariate-linear-regression" class="slide level2">
<h2>Multivariate linear regression</h2>
<p><span class="math display">\[
h (x_i) = \theta_0 + \theta_1 x_i^{(1)} + \theta_2 x_i^{(2)} + \theta_3 x_i^{(3)} + \cdots + \theta_D x_i^{(D)}
\]</span></p>
<p><span class="math display">\[
\begin{align*}
  x_i^{(j)} &amp;= \text{value of the feature } j \text{ in the } i \text{th example} \\
  D &amp;= \text{the number of features}
\end{align*}
\]</span></p>
</section>
<section id="gradient-descent---multivariate" class="slide level2">
<h2>Gradient descent - multivariate</h2>
<p>The new <strong>loss function</strong> is</p>
<p><span class="math display">\[
J(\theta_0, \theta_1,\ldots,\theta_D) =  \dfrac {1}{N} \displaystyle \sum _{i=1}^N [h(x_{i}) - y_i ]^2
\]</span></p>
<p>Its <strong>partial derivative</strong>:</p>
<p><span class="math display">\[
\frac {\partial}{\partial \theta_j}J(\theta) = \frac{2}{N} \sum\limits_{i=1}^N x_i^{(j)} [\theta x_i - y_i ]
\]</span></p>
<p><strong>where</strong> <span class="math inline">\(\theta\)</span>, <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span> are vectors, and <span class="math inline">\(\theta x_i\)</span> is a vector operation!</p>
</section>
<section id="gradient-vector" class="slide level2 smaller">
<h2>Gradient vector</h2>
<p>The vector containing the partial derivative of <span class="math inline">\(J\)</span> (with respect to <span class="math inline">\(\theta_j\)</span>, for <span class="math inline">\(j \in \{0, 1\ldots D\}\)</span>) is called the <strong>gradient vector</strong>.</p>
<p><span class="math display">\[
\nabla_\theta J(\theta) = \begin{pmatrix}
  \frac {\partial}{\partial \theta_0}J(\theta) \\
  \frac {\partial}{\partial \theta_1}J(\theta) \\
  \vdots  \\
  \frac {\partial}{\partial \theta_D}J(\theta)\\
\end{pmatrix}
\]</span></p>
<div class="fragment">
<ul>
<li>This vector gives the direction of the <strong>steepest ascent</strong>.</li>
<li>It gives its name to the <strong>gradient descent</strong> algorithm:</li>
</ul>
<p><span class="math display">\[
\theta' = \theta - \alpha \nabla_\theta J(\theta)
\]</span></p>
</div>
</section>
<section id="gradient-descent---multivariate-1" class="slide level2">
<h2>Gradient descent - multivariate</h2>
<p>The gradient descent algorithm becomes:</p>
<p><strong>Repeat until convergence:</strong></p>
<p><span class="math display">\[
\begin{aligned}
\{ &amp; \\
\theta_j := &amp; \theta_j -  \alpha  \frac {\partial}{\partial \theta_j}J(\theta_0, \theta_1, \ldots, \theta_D) \\
&amp;\text{for } j \in [0, \ldots, D] \textbf{ (update simultaneously)} \\
\} &amp;
\end{aligned}
\]</span></p>
</section>
<section id="gradient-descent---multivariate-2" class="slide level2 smaller">
<h2>Gradient descent - multivariate</h2>
<p><strong>Repeat until convergence:</strong></p>
<p><span class="math display">\[
\begin{aligned}
\; \{ &amp; \\
\; &amp; \theta_0 := \theta_0 - \alpha \frac{2}{N} \sum\limits_{i=1}^{N}  x^{0}_i[h(x_i) - y_i] \\
\; &amp; \theta_1 := \theta_1 - \alpha \frac{2}{N} \sum\limits_{i=1}^{N}  x^{1}_i[h(x_i) - y_i]  \\
\; &amp; \theta_2 := \theta_2 - \alpha \frac{2}{N} \sum\limits_{i=1}^{N}  x^{2}_i[h(x_i) - y_i] \\
   &amp; \cdots \\
\} &amp;
\end{aligned}
\]</span></p>

<aside><div>
<p>Where <span class="math inline">\(x_0 = 1\)</span>.</p>
</div></aside></section>
<section id="assumptions" class="slide level2">
<h2>Assumptions</h2>
<p>What were our <strong>assumptions</strong>?</p>
<div class="fragment">
<ul>
<li>The (objective/loss) function is <strong>differentiable</strong>.</li>
</ul>
</div>
</section>
<section id="local-vs.-global" class="slide level2 smaller">
<h2>Local vs.&nbsp;global</h2>
<ul>
<li class="fragment"><p>A function is <strong>convex</strong> if for any pair of points on the graph of the function, the line connecting these two points lies above or on the graph.</p>
<ul>
<li class="fragment">A <strong>convex</strong> function has a <strong>single</strong> minimum.
<ul>
<li class="fragment">The loss function for the linear regression (MSE) is convex.</li>
</ul></li>
</ul></li>
<li class="fragment"><p>For functions that are not convex, the gradient descent algorithm converges to a <strong>local</strong> minimum.</p></li>
<li class="fragment"><p>The loss function generally used with linear or logistic regressions, and Support Vector Machines (SVM) are convex, but not the ones for artificial neural networks.</p></li>
</ul>
<aside class="notes">
<p>A function would be convex downward or concave if those lines were below or on the graph of the function.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="local-vs.-global-1" class="slide level2">
<h2>Local vs.&nbsp;global</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="https://upload.wikimedia.org/wikipedia/commons/1/1e/Extrema_example.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-14"><img data-src="https://upload.wikimedia.org/wikipedia/commons/1/1e/Extrema_example.svg" class="quarto-figure quarto-figure-center" style="height:55.0%"></a></p>
</figure>
</div>

<aside><div>
<p><strong>Attribution:</strong> <a href="https://commons.wikimedia.org/wiki/File:Extrema_example.svg">commons.wikimedia.org/wiki/File:Extrema_example.svg</a></p>
</div></aside></section>
<section id="convergence" class="slide level2">
<h2>Convergence</h2>
<div id="f0727870" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a></a><span class="co"># 1. Define the symbolic variable and the function</span></span>
<span id="cb19-2"><a></a>x <span class="op">=</span> sp.Symbol(<span class="st">'x'</span>, real<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-3"><a></a>f_expr <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>x<span class="op">**</span><span class="dv">3</span> <span class="op">+</span> <span class="dv">4</span><span class="op">*</span>x<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">5</span><span class="op">*</span>x <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb19-4"><a></a></span>
<span id="cb19-5"><a></a><span class="co"># 2. Compute the derivative of f</span></span>
<span id="cb19-6"><a></a>f_prime_expr <span class="op">=</span> sp.diff(f_expr, x)</span>
<span id="cb19-7"><a></a></span>
<span id="cb19-8"><a></a><span class="co"># 3. Convert symbolic expressions to Python functions</span></span>
<span id="cb19-9"><a></a>f <span class="op">=</span> sp.lambdify(x, f_expr, <span class="st">'numpy'</span>)</span>
<span id="cb19-10"><a></a>f_prime <span class="op">=</span> sp.lambdify(x, f_prime_expr, <span class="st">'numpy'</span>)</span>
<span id="cb19-11"><a></a></span>
<span id="cb19-12"><a></a><span class="co"># 4. Generate a range of x-values</span></span>
<span id="cb19-13"><a></a>x_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1000</span>)</span>
<span id="cb19-14"><a></a></span>
<span id="cb19-15"><a></a><span class="co"># 5. Compute f and f' over this range</span></span>
<span id="cb19-16"><a></a>y_vals <span class="op">=</span> f(x_vals)</span>
<span id="cb19-17"><a></a>y_prime_vals <span class="op">=</span> f_prime(x_vals)</span>
<span id="cb19-18"><a></a></span>
<span id="cb19-19"><a></a><span class="co"># 6. Prepare LaTeX strings for legend</span></span>
<span id="cb19-20"><a></a>f_label <span class="op">=</span> <span class="vs">rf'</span><span class="dv">$</span><span class="vs">f</span><span class="kw">(</span><span class="vs">x</span><span class="kw">)</span><span class="vs"> = </span><span class="sc">{</span>sp<span class="sc">.</span>latex(f_expr)<span class="sc">}</span><span class="dv">$</span><span class="vs">'</span></span>
<span id="cb19-21"><a></a>f_prime_label <span class="op">=</span> <span class="vs">rf'</span><span class="dv">$</span><span class="vs">f</span><span class="dv">^</span><span class="er">\</span><span class="vs">prime</span><span class="kw">(</span><span class="vs">x</span><span class="kw">)</span><span class="vs"> = </span><span class="sc">{</span>sp<span class="sc">.</span>latex(f_prime_expr)<span class="sc">}</span><span class="dv">$</span><span class="vs">'</span></span>
<span id="cb19-22"><a></a></span>
<span id="cb19-23"><a></a><span class="co"># 7. Plot f and f', with equations in the legend</span></span>
<span id="cb19-24"><a></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb19-25"><a></a>plt.plot(x_vals, y_vals, label<span class="op">=</span>f_label)</span>
<span id="cb19-26"><a></a>plt.plot(x_vals, y_prime_vals, label<span class="op">=</span>f_prime_label)</span>
<span id="cb19-27"><a></a></span>
<span id="cb19-28"><a></a><span class="co"># 8. Shade the region between x-axis and f'(x) for the entire domain</span></span>
<span id="cb19-29"><a></a>plt.fill_between(x_vals, y_prime_vals, <span class="dv">0</span>, color<span class="op">=</span><span class="st">'gray'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, interpolate<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb19-30"><a></a>                 label<span class="op">=</span><span class="st">'Region between 0 and f</span><span class="ch">\'</span><span class="st">(x)'</span>)</span>
<span id="cb19-31"><a></a></span>
<span id="cb19-32"><a></a><span class="co"># 9. Add reference line, labels, legend, etc.</span></span>
<span id="cb19-33"><a></a>plt.axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb19-34"><a></a>plt.title(<span class="vs">rf'Function and its Derivative with Shading for </span><span class="dv">$</span><span class="vs">f</span><span class="dv">^</span><span class="er">\</span><span class="vs">prime</span><span class="kw">(</span><span class="vs">x</span><span class="kw">)</span><span class="dv">$</span><span class="vs">'</span>)</span>
<span id="cb19-35"><a></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb19-36"><a></a>plt.ylabel(<span class="st">'y'</span>)</span>
<span id="cb19-37"><a></a>plt.legend()</span>
<span id="cb19-38"><a></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb19-39"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-21-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15"><img data-src="slides_files/figure-revealjs/cell-21-output-1.png" class="quarto-figure quarto-figure-center" width="659" height="370"></a></p>
</figure>
</div>
</div>
</div>

<aside class="notes">
<ul>
<li><p>The first objective of this example is to illustrate that gradient descent is applicable to functions of arbitrary complexity, provided that the gradient can be computed or approximated at each iteration.</p></li>
<li><p>Furthermore, the function must possesses at least one local minimum within the interval of interest.</p></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p>For <strong>functions lacking a global minimum</strong>, gradient descent can <strong>continue descending indefinitely</strong>, preventing convergence.</p>
</div></aside></section>
<section id="learning-rate" class="slide level2">
<h2>Learning Rate</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div id="54b4d679" class="cell" data-execution_count="21">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-22-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16"><img data-src="slides_files/figure-revealjs/cell-22-output-1.png" class="quarto-figure quarto-figure-center" width="470" height="469"></a></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li class="fragment"><strong>Small steps</strong>, low values for <span class="math inline">\(\alpha\)</span>, will make the algorithm <strong>converge slowly</strong>.</li>
<li class="fragment"><strong>Large steps</strong> might cause the algorithm to <strong>diverge</strong>.</li>
<li class="fragment">Notice how the algorithm <strong>slows down</strong> naturally when approaching a minimum.</li>
</ul>
</div></div>
</section>
<section id="learning-rate-1" class="slide level2 scrollable">
<h2>Learning Rate</h2>
<div id="9294e8c4" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-2"><a></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-3"><a></a></span>
<span id="cb20-4"><a></a><span class="kw">def</span> f(x):</span>
<span id="cb20-5"><a></a>    <span class="cf">return</span> x<span class="op">**</span><span class="dv">2</span></span>
<span id="cb20-6"><a></a></span>
<span id="cb20-7"><a></a><span class="kw">def</span> grad_f(x):</span>
<span id="cb20-8"><a></a>    <span class="cf">return</span> <span class="dv">2</span><span class="op">*</span>x</span>
<span id="cb20-9"><a></a></span>
<span id="cb20-10"><a></a><span class="co"># Initial guess, learning rate, and number of gradient-descent steps</span></span>
<span id="cb20-11"><a></a>x_current <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb20-12"><a></a>learning_rate <span class="op">=</span> <span class="fl">1.1</span>  <span class="co"># Too large =&gt; divergence</span></span>
<span id="cb20-13"><a></a>num_iterations <span class="op">=</span> <span class="dv">5</span>   <span class="co"># We'll do five updates</span></span>
<span id="cb20-14"><a></a></span>
<span id="cb20-15"><a></a><span class="co"># Store each x value in a list (trajectory) for plotting</span></span>
<span id="cb20-16"><a></a>trajectory <span class="op">=</span> [x_current]</span>
<span id="cb20-17"><a></a></span>
<span id="cb20-18"><a></a><span class="co"># Perform gradient descent</span></span>
<span id="cb20-19"><a></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_iterations):</span>
<span id="cb20-20"><a></a>    g <span class="op">=</span> grad_f(x_current)</span>
<span id="cb20-21"><a></a>    x_current <span class="op">=</span> x_current <span class="op">-</span> learning_rate <span class="op">*</span> g</span>
<span id="cb20-22"><a></a>    trajectory.append(x_current)</span>
<span id="cb20-23"><a></a></span>
<span id="cb20-24"><a></a><span class="co"># Prepare data for plotting</span></span>
<span id="cb20-25"><a></a>x_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">1000</span>)</span>
<span id="cb20-26"><a></a>y_vals <span class="op">=</span> f(x_vals)</span>
<span id="cb20-27"><a></a></span>
<span id="cb20-28"><a></a><span class="co"># Plot the function f(x)</span></span>
<span id="cb20-29"><a></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">5</span>))</span>
<span id="cb20-30"><a></a>plt.plot(x_vals, y_vals, label<span class="op">=</span><span class="vs">r"</span><span class="dv">$</span><span class="vs">f</span><span class="kw">(</span><span class="vs">x</span><span class="kw">)</span><span class="vs"> = x</span><span class="dv">^</span><span class="vs">2</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb20-31"><a></a>plt.axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb20-32"><a></a></span>
<span id="cb20-33"><a></a><span class="co"># Plot the trajectory, labeling each iteration</span></span>
<span id="cb20-34"><a></a><span class="cf">for</span> i, x_t <span class="kw">in</span> <span class="bu">enumerate</span>(trajectory):</span>
<span id="cb20-35"><a></a>    y_t <span class="op">=</span> f(x_t)</span>
<span id="cb20-36"><a></a>    <span class="co"># Plot the point</span></span>
<span id="cb20-37"><a></a>    plt.plot(x_t, y_t, <span class="st">'ro'</span>)</span>
<span id="cb20-38"><a></a>    <span class="co"># Label the iteration number</span></span>
<span id="cb20-39"><a></a>    plt.text(x_t, y_t, <span class="ss">f"  </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb20-40"><a></a>    <span class="co"># Connect consecutive points</span></span>
<span id="cb20-41"><a></a>    <span class="cf">if</span> i <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb20-42"><a></a>        x_prev <span class="op">=</span> trajectory[i <span class="op">-</span> <span class="dv">1</span>]</span>
<span id="cb20-43"><a></a>        y_prev <span class="op">=</span> f(x_prev)</span>
<span id="cb20-44"><a></a>        plt.plot([x_prev, x_t], [y_prev, y_t], <span class="st">'r--'</span>)</span>
<span id="cb20-45"><a></a></span>
<span id="cb20-46"><a></a><span class="co"># Final touches</span></span>
<span id="cb20-47"><a></a>plt.title(<span class="st">"Gradient Descent Divergence with a Large Learning Rate"</span>)</span>
<span id="cb20-48"><a></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb20-49"><a></a>plt.ylabel(<span class="st">"f(x)"</span>)</span>
<span id="cb20-50"><a></a>plt.legend()</span>
<span id="cb20-51"><a></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb20-52"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-23-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17"><img data-src="slides_files/figure-revealjs/cell-23-output-1.png" class="quarto-figure quarto-figure-center" width="502" height="442"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="batch-gradient-descent" class="slide level2">
<h2>Batch gradient descent</h2>
<ul>
<li>To be more precise, this algorithm is known as <strong>batch gradient descent</strong> since for each iteration, it processes the “whole batch” of training examples.</li>
</ul>
<div class="fragment">
<ul>
<li>Literature suggests that the algorithm might take more time to converge if the features are on different scales.</li>
</ul>
</div>
</section>
<section id="batch-gradient-descent---drawback" class="slide level2">
<h2>Batch gradient descent - drawback</h2>
<ul>
<li>The <strong>batch gradient descent</strong> algorithm becomes very <strong>slow</strong> as the <strong>number of training examples increases</strong>.</li>
</ul>
<div class="fragment">
<ul>
<li>This is because <strong>all</strong> the training data is seen at <strong>each iteration</strong>. The algorithm is generally run for a fixed number of iterations, say 1000.</li>
</ul>
</div>
</section>
<section id="stochastic-gradient-descent" class="slide level2">
<h2>Stochastic Gradient Descent</h2>
<p>The <strong>stochastic gradient descent</strong> algorithm randomly selects <strong>one</strong> training instance to calculate its gradient.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a></a>epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb21-2"><a></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb21-3"><a></a>   <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb21-4"><a></a>         selection <span class="op">=</span> np.random.randint(N)</span>
<span id="cb21-5"><a></a>         <span class="co"># Calculate the gradient using selection</span></span>
<span id="cb21-6"><a></a>         <span class="co"># Update the parameters</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li class="fragment">This allows it to work with large training sets.</li>
<li class="fragment">Its trajectory is not as regular as the batch algorithm.
<ul>
<li class="fragment">Because of its bumpy trajectory, it is often better at finding the <strong>global minima</strong>, when compared to batch.</li>
<li class="fragment">Its bumpy trajectory makes it bounce around the local minima.</li>
</ul></li>
</ul>
<aside class="notes">
<p>To mitigate the issue of oscillating around local minima, it is advisable to progressively reduce the learning rate as the number of epochs increases. This technique, known as a <strong>learning schedule</strong>, helps achieve more stable convergence.</p>
<p>It important that the examples are either <strong>selected randomly</strong> or <strong>shuffled</strong> before running the algorithm to make sure that the algorithm converges towards the global minima.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="mini-batch-gradient-descent" class="slide level2">
<h2>Mini-batch gradient descent</h2>
<ul>
<li class="fragment">At each step, rather than selecting one training example as SGD does, <strong>mini-batch gradient descent</strong> randomly selects a <strong>small number</strong> of training examples to compute the gradients.</li>
<li class="fragment">Its trajectory is more regular compared to SGD.
<ul>
<li class="fragment">As the size of the mini-batches increases, the algorithm becomes increasingly similar to batch gradient descent, which uses all the examples at each step.</li>
</ul></li>
<li class="fragment">It can take advantage of the hardware acceleration of matrix operations, particularly with GPUs.</li>
</ul>
</section>
<section id="quick-visualization-1" class="slide level2">
<h2>Quick Visualization</h2>
<div id="833b71f3" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb22-2"><a></a></span>
<span id="cb22-3"><a></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">4</span>))</span>
<span id="cb22-4"><a></a>plt.scatter(df[<span class="st">"eruptions"</span>], df[<span class="st">"waiting"</span>], s<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb22-5"><a></a>plt.xlabel(<span class="st">"Eruption duration (min)"</span>)</span>
<span id="cb22-6"><a></a>plt.ylabel(<span class="st">"Waiting time to next eruption (min)"</span>)</span>
<span id="cb22-7"><a></a>plt.title(<span class="st">"Old Faithful: eruptions vs waiting"</span>)</span>
<span id="cb22-8"><a></a>plt.tight_layout()</span>
<span id="cb22-9"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="slides_files/figure-revealjs/cell-24-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18"><img data-src="slides_files/figure-revealjs/cell-24-output-1.png" class="quarto-figure quarto-figure-center" width="566" height="373"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="stochastic-mini-batch-batch" class="slide level2">
<h2>Stochastic, Mini-Batch, Batch</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="../../assets/images/geron_2022-f4_10.png" class="lightbox" data-gallery="quarto-lightbox-gallery-19"><img data-src="../../assets/images/geron_2022-f4_10.png" class="quarto-figure quarto-figure-center" height="450"></a></p>
</figure>
</div>

<aside><div>
<p><strong>Attribution:</strong> <span class="citation" data-cites="Geron:2022aa">Géron (<a href="#/references" role="doc-biblioref" onclick="">2022</a>)</span>, Figure 4.10, <a href="https://github.com/ageron/handson-ml3/blob/main/04_training_linear_models.ipynb">04_training_linear_models.ipynb</a></p>
</div></aside></section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
<ul>
<li class="fragment"><p><strong>Batch gradient descent</strong> is inherently <strong>slow</strong> and impractical for large datasets requiring <strong>out-of-core</strong> support, though it is capable of handling a substantial number of features.</p></li>
<li class="fragment"><p><strong>Stochastic gradient descent</strong> is <strong>fast</strong> and well-suited for processing a large volume of examples efficiently.</p></li>
<li class="fragment"><p><strong>Mini-batch gradient descent</strong> combines the benefits of both batch and stochastic methods; it is <strong>fast</strong>, capable of managing large datasets, and leverages hardware acceleration, particularly with GPUs.</p></li>
</ul>
<aside class="notes">
<p>The typical size of a mini-batch when applying stochastic gradient descent (SGD) can vary depending on the specific application and dataset, but common sizes often range between 32 and 512 samples. Here are some common mini-batch sizes used in practice:</p>
<ol type="1">
<li><strong>Small Mini-Batches</strong>: Sizes such as 16, 32, or 64 are often used when working with smaller datasets or when memory constraints are a concern.</li>
<li><strong>Medium Mini-Batches</strong>: Sizes like 128, 256, or 512 are commonly used and can provide a good balance between computational efficiency and convergence speed.</li>
<li><strong>Large Mini-Batches</strong>: Sizes like 1024, 2048, or larger might be used in large-scale machine learning tasks, especially when sufficient computational resources are available.</li>
</ol>
<p>The choice of mini-batch size can influence several factors such as:</p>
<ul>
<li><strong>Training Speed</strong>: Larger mini-batches can make better use of parallel processing capabilities, potentially speeding up training.</li>
<li><strong>Convergence</strong>: Smaller mini-batches can introduce more noise in the gradient estimation, which can sometimes help escape local minima and improve generalization.</li>
<li><strong>Memory Usage</strong>: Larger mini-batches require more memory, which might be a limiting factor, especially on GPUs with limited VRAM.</li>
</ul>
<p>Ultimately, the optimal mini-batch size is task-specific and often determined empirically through experimentation.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>

<aside><div>
<p>All three are implemented by <code>SGDRegressor</code> in <code>Scikit-Learn</code>.</p>
</div></aside></section>
<section id="optimization-and-deep-nets" class="slide level2">
<h2>Optimization and deep nets</h2>
<p>We will briefly revisit the subject when discussing <strong>deep artificial neural networks</strong>, for which <strong>specialized optimization algorithms</strong> exist.</p>
<ul>
<li>Momentum Optimization</li>
<li>Nesterov Accelerated Gradient</li>
<li>AdaGrad</li>
<li>RMSProp</li>
<li>Adam and Nadam</li>
</ul>
</section>
<section id="final-word" class="slide level2">
<h2>Final word</h2>
<ul>
<li>Optimization is a vast subject. Other algorithms exist and are used in other contexts.
<ul>
<li>Including:
<ul>
<li>Particle swarm optimization (PSO), genetic algorithms (GAs), and artificial bee colony (ABC) algorithms.</li>
</ul></li>
</ul></li>
</ul>
</section></section>
<section>
<section id="prologue" class="title-slide slide level1 center">
<h1>Prologue</h1>

</section>
<section id="linear-regression---summary" class="slide level2">
<h2>Linear regression - summary</h2>
<ul>
<li class="fragment">A <strong>linear model</strong> assumes that the value of the label, <span class="math inline">\(\hat{y_i}\)</span>, can be expressed as a <strong>linear combination</strong> of the feature values, <span class="math inline">\(x_i^{(j)}\)</span>: <span class="math inline">\(\hat{y_i} = h(x_i) = \theta_0 + \theta_1 x_i^{(1)} + \theta_2 x_i^{(2)} + \ldots + \theta_D x_i^{(D)}\)</span></li>
<li class="fragment">The <strong>Mean Squared Error (MSE)</strong> is: <span class="math inline">\(\frac{1}{N}\sum_1^N [h(x_i) - y_i]^2\)</span></li>
<li class="fragment"><strong>Batch</strong>, <strong>stochastic</strong>, or <strong>mini-batch gradient descent</strong> can be used to find “optimal” values for the parameters, <span class="math inline">\(\theta_j\)</span> for <span class="math inline">\(j \in 0, 1, \ldots, D\)</span>.</li>
<li class="fragment">The result is a <strong>regressor</strong>, a function that can be used to predict the <span class="math inline">\(y\)</span> value (the label) for some unseen example <span class="math inline">\(x\)</span>.</li>
</ul>
</section>
<section id="andrew-ng" class="slide level2">
<h2>Andrew Ng</h2>
<div class="columns">
<div class="column" style="width:40%;">
<p><a href="../../assets/images/gradient_descent_andrew_ng-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-20"><img data-src="../../assets/images/gradient_descent_andrew_ng-1.png"></a></p>
</div><div class="column" style="width:60%;">
<ul>
<li><a href="https://youtu.be/sOou4izGINg?si=_Fz1V1tbGk8usJR0">Gradient Descent (Math)</a> <br> (11:30 m)</li>
<li><a href="https://youtu.be/DS83GeqWQqs?si=kOfDpHT_4t8hl_YL">Intuition</a> <br>(11:51 m)</li>
<li><a href="https://www.youtube.com/watch?v=nOMy9LIcIkI&amp;list=PLb0Gp98iu3OyY9zWJfSMq26nmkNKztNhA&amp;index=6">Linear Regression</a> <br>(10:20 m)</li>
<li><a href="https://www.youtube.com/playlist?list=PLoR5VjrKytrCv-Vxnhp5UyS1UjZsXP0Kj">ML-005 | Stanford | Andrew Ng</a><br> (19 videos)</li>
</ul>
</div></div>

<aside class="notes">
<p>Andrew Ng is Founder of <a href="https://www.deeplearning.ai/">DeepLearning.AI</a>, Founder &amp; CEO of <a href="https://landing.ai/">Landing AI</a>, General Partner at <a href="https://aifund.ai/">AI Fund</a>, Chairman and Co-Founder of <a href="https://www.coursera.org/">Coursera</a> and an Adjunct Professor at Stanford University’s Computer Science Department.</p>
<p>Ng was also a cofounder and head of <a href="https://en.wikipedia.org/wiki/Google_Brain" title="Google Brain">Google Brain</a> and was the former Chief Scientist at <a href="https://en.wikipedia.org/wiki/Baidu" title="Baidu">Baidu</a>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p><a href="https://www.andrewng.org">Andrew Ng</a> is presenting the gradient descent algorithm using a linear regression with one variable.</p>
</div></aside></section>
<section id="fundamentals-by-herman-kamper" class="slide level2">
<h2>Fundamentals by Herman Kamper</h2>
<iframe data-external="1" src="https://www.youtube.com/embed/BlnLoqn3ZBo" width="889" height="500" title="Gradient descent 1: Fundamentals" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</section></section>
<section>
<section id="mathematics" class="title-slide slide level1 center">
<h1>Mathematics</h1>

</section>
<section id="blue1brown" class="slide level2">
<h2>3Blue1Brown</h2>
<ul>
<li><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of <strong>linear algebra</strong></a>
<ul>
<li>A series of 16 videos (10 to 15 minutes per video) providing “a geometric understanding of matrices, determinants, eigen-stuffs and more.”
<ul>
<li>6,662,732 views as of September 30, 2019.</li>
</ul></li>
</ul></li>
<li><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr">Essence of <strong>calculus</strong></a>
<ul>
<li>A series of 12 videos (15 to 20 minutes per video): “The goal here is to make calculus feel like something that you yourself could have discovered.”
<ul>
<li>2,309,726 views as of September 30, 2019.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="next-lecture" class="slide level2">
<h2>Next lecture</h2>
<ul>
<li>Logistic regression</li>
</ul>
</section></section>
<section>
<section id="appendix" class="title-slide slide level1 center">
<h1>Appendix</h1>

</section>
<section id="linearregression" class="slide level2">
<h2>LinearRegression</h2>
<div id="000ec4d7" class="cell" data-execution_count="24">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><a href="slides_files/figure-revealjs/cell-25-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-21"><img data-src="slides_files/figure-revealjs/cell-25-output-1.png" width="800" height="438"></a></p>
</figure>
</div>
</div>
</div>
<div id="bc5da807" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb23-2"><a></a></span>
<span id="cb23-3"><a></a>lin_reg <span class="op">=</span> LinearRegression()</span>
<span id="cb23-4"><a></a>lin_reg.fit(X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<aside class="notes">
<p>Herein, we examine a straightforward model involving a single variable.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="source-code-2" class="slide level2">
<h2>Source code</h2>
<div id="eb909431" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-2"><a></a></span>
<span id="cb24-3"><a></a>X <span class="op">=</span> <span class="dv">6</span> <span class="op">*</span> np.random.rand(<span class="dv">100</span>, <span class="dv">1</span>) <span class="op">-</span> <span class="dv">4</span></span>
<span id="cb24-4"><a></a>y <span class="op">=</span> X <span class="op">**</span> <span class="dv">2</span> <span class="op">-</span> <span class="dv">4</span> <span class="op">*</span> X <span class="op">+</span> <span class="dv">5</span> <span class="op">+</span> np.random.randn(<span class="dv">100</span>, <span class="dv">1</span>)</span>
<span id="cb24-5"><a></a></span>
<span id="cb24-6"><a></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb24-7"><a></a></span>
<span id="cb24-8"><a></a>lin_reg <span class="op">=</span> LinearRegression()</span>
<span id="cb24-9"><a></a>lin_reg.fit(X, y)</span>
<span id="cb24-10"><a></a></span>
<span id="cb24-11"><a></a>X_new <span class="op">=</span> np.array([[<span class="op">-</span><span class="dv">4</span>], [<span class="dv">2</span>]])</span>
<span id="cb24-12"><a></a>y_pred <span class="op">=</span> lin_reg.predict(X_new)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>

<aside class="notes">
<p>The data have been generated from a quadratic model, <span class="math inline">\(x^2 - 4x + 5\)</span>, with Gaussian noise added to make the synthetic data more realistic.</p>
<p>This example illustrates that, in practice, we often have limited knowledge about the underlying model that generated the data.</p>
<p>Despite this uncertainty, we make assumptions—in this case, assuming linearity—when building our models.</p>
<p>Later, we will refer to this as <strong>bias</strong> (distinct from the bias parameter in our regression model). Both bias and variance influence the performance of our models.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p>This slide contains an anomaly. Can you identify it?</p>
</div></aside></section>
<section id="gpt-4o-interactions-2024-09-14" class="slide level2 scrollable">
<h2>GPT 4o interactions (2024-09-14)</h2>
<p><strong>User</strong>: Explain the code on the previous slide line by line.</p>
<div class="fragment">
<p><strong>AI</strong>: Certainly! Here is a line-by-line explanation of the provided code:</p>
<div id="2e588255" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><strong>Explanation</strong>: This line imports the NumPy library, which is a fundamental package for numerical computations in Python. It is commonly used for working with arrays and matrices, as well as for performing a variety of mathematical operations.</li>
</ul>
<div id="98adcb07" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a></a>X <span class="op">=</span> <span class="dv">6</span> <span class="op">*</span> np.random.rand(<span class="dv">100</span>, <span class="dv">1</span>) <span class="op">-</span> <span class="dv">4</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><strong>Explanation</strong>: This line generates a NumPy array <code>X</code> containing 100 rows and 1 column of random numbers. The <code>np.random.rand(100, 1)</code> function generates uniformly distributed random numbers between 0 and 1. Multiplying by 6 scales these numbers to the range [0, 6), and subtracting 4 shifts the range to [-4, 2).</li>
</ul>
<div id="c0c1a3ca" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a></a>y <span class="op">=</span> X <span class="op">**</span> <span class="dv">2</span> <span class="op">-</span> <span class="dv">4</span> <span class="op">*</span> X <span class="op">+</span> <span class="dv">5</span> <span class="op">+</span> np.random.randn(<span class="dv">100</span>, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><strong>Explanation</strong>: This line generates the target variable <code>y</code> based on a quadratic equation with some added noise. Specifically:
<ul>
<li><code>X ** 2</code> computes the square of each element in <code>X</code>.</li>
<li><code>- 4 * X</code> subtracts four times each element in <code>X</code>.</li>
<li><code>+ 5</code> adds a constant term of 5.</li>
<li><code>+ np.random.randn(100, 1)</code> adds Gaussian noise with a mean of 0 and standard deviation of 1 to each element, making the data more realistic by introducing some randomness.</li>
</ul></li>
</ul>
<div id="34fb942d" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><strong>Explanation</strong>: This line imports the <code>LinearRegression</code> class from the <code>sklearn.linear_model</code> module, which is part of the Scikit-Learn library. Scikit-Learn is widely used for machine learning in Python.</li>
</ul>
<div id="1f3e5a15" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a></a>lin_reg <span class="op">=</span> LinearRegression()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><strong>Explanation</strong>: This line creates an instance of the <code>LinearRegression</code> class and assigns it to the variable <code>lin_reg</code>. This object will be used to fit the linear regression model to the data.</li>
</ul>
<div id="e535b866" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a></a>lin_reg.fit(X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><strong>Explanation</strong>: This line fits the linear regression model to the data by finding the best-fitting line through the points <code>(X, y)</code>. The <code>fit</code> method trains the model using the provided features <code>X</code> and target variable <code>y</code>.</li>
</ul>
<div id="60457834" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a></a>X_new <span class="op">=</span> np.array([[<span class="op">-</span><span class="dv">4</span>], [<span class="dv">2</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><strong>Explanation</strong>: This line creates a new NumPy array <code>X_new</code> with two values: -4 and 2. These values represent new data points for which we want to predict the target variable using the trained model.</li>
</ul>
<div id="25655ea1" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a></a>y_pred <span class="op">=</span> lin_reg.predict(X_new)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><strong>Explanation</strong>: This line uses the trained linear regression model to predict the target variable <code>y_pred</code> for the new data points in <code>X_new</code>. The <code>predict</code> method returns the predicted values based on the fitted model.</li>
</ul>
<p>In summary, this script generates synthetic data, fits a linear regression model to it, and then uses the model to make predictions on new data points.</p>
</div>
</section>
<section id="source-code-continued" class="slide level2">
<h2>Source code (continued)</h2>
<div id="72fbac40" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a></a><span class="im">import</span> os</span>
<span id="cb33-2"><a></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb33-3"><a></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb33-4"><a></a></span>
<span id="cb33-5"><a></a><span class="kw">def</span> save_fig(fig_id, tight_layout<span class="op">=</span><span class="va">True</span>, fig_extension<span class="op">=</span><span class="st">"pdf"</span>, resolution<span class="op">=</span><span class="dv">300</span>):</span>
<span id="cb33-6"><a></a>    path <span class="op">=</span> os.path.join(fig_id <span class="op">+</span> <span class="st">"."</span> <span class="op">+</span> fig_extension)</span>
<span id="cb33-7"><a></a>    <span class="bu">print</span>(<span class="st">"Saving figure"</span>, fig_id)</span>
<span id="cb33-8"><a></a>    <span class="cf">if</span> tight_layout:</span>
<span id="cb33-9"><a></a>        plt.tight_layout()</span>
<span id="cb33-10"><a></a>    plt.savefig(path, <span class="bu">format</span><span class="op">=</span>fig_extension, dpi<span class="op">=</span>resolution)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="source-code-continued-1" class="slide level2">
<h2>Source code (continued)</h2>
<div id="a5cecfd8" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a></a>plt.plot(X, y, <span class="st">"b."</span>)</span>
<span id="cb34-2"><a></a>plt.plot(X_new, y_pred, <span class="st">"r-"</span>)</span>
<span id="cb34-3"><a></a>plt.xlabel(<span class="st">"$x$"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb34-4"><a></a>plt.ylabel(<span class="st">"$y$"</span>, rotation<span class="op">=</span><span class="dv">0</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb34-5"><a></a>plt.axis([<span class="op">-</span><span class="dv">4</span>, <span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">35</span>])</span>
<span id="cb34-6"><a></a>save_fig(<span class="st">"regression_linear-01"</span>)</span>
<span id="cb34-7"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="references" class="slide level2 smaller scrollable">
<h2>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Azzalini:1990aa" class="csl-entry" role="listitem">
Azzalini, A., and A. W. Bowman. 1990. <span>“A Look at Some Data on the Old Faithful Geyser.”</span> <em>Journal of the Royal Statistical Society Series C: Applied Statistics</em> 39 (3): 357–65. <a href="https://doi.org/10.2307/2347385">https://doi.org/10.2307/2347385</a>.
</div>
<div id="ref-Geron:2022aa" class="csl-entry" role="listitem">
Géron, Aurélien. 2022. <em>Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow</em>. 3rd ed. O’Reilly Media, Inc.
</div>
<div id="ref-Russell:2020aa" class="csl-entry" role="listitem">
Russell, Stuart, and Peter Norvig. 2020. <em>Artificial Intelligence: <span>A</span> Modern Approach</em>. 4th ed. Pearson. <a href="http://aima.cs.berkeley.edu/">http://aima.cs.berkeley.edu/</a>.
</div>
<div id="ref-Stanton:2001aa" class="csl-entry" role="listitem">
Stanton, Jeffrey M. 2001. <span>“Galton, Pearson, and the Peas: A Brief History of Linear Regression for Statistics Instructors.”</span> <em>Journal of Statistics Education</em> 9 (3). <a href="https://doi.org/10.1080/10691898.2001.11910537">https://doi.org/10.1080/10691898.2001.11910537</a>.
</div>
</div>
</section>
<section class="slide level2">

<p>Marcel <strong>Turcotte</strong></p>
<p><a href="mailto:Marcel.Turcotte@uOttawa.ca">Marcel.Turcotte@uOttawa.ca</a></p>
<p>School of Electrical Engineering and <strong>Computer Science</strong> (EE<strong>CS</strong>)</p>
<p>University of Ottawa</p>


</section></section>

    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../../assets/images/uottawa_hor_black.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://turcotte.xyz/teaching/csi-4106">turcotte.xyz/teaching/csi-4106</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/turcotte\.xyz\/teaching\/csi-4106");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    <script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
    (function() {
      let previousOnload = window.onload;
      window.onload = () => {
        if (previousOnload) {
          previousOnload();
        }
        lightboxQuarto.on('slide_before_load', (data) => {
          const { slideIndex, slideNode, slideConfig, player, trigger } = data;
          const href = trigger.getAttribute('href');
          if (href !== null) {
            const imgEl = window.document.querySelector(`a[href="${href}"] img`);
            if (imgEl !== null) {
              const srcAttr = imgEl.getAttribute("src");
              if (srcAttr && srcAttr.startsWith("data:")) {
                slideConfig.href = srcAttr;
              }
            }
          } 
        });
      
        lightboxQuarto.on('slide_after_load', (data) => {
          const { slideIndex, slideNode, slideConfig, player, trigger } = data;
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(slideNode);
          }
        });
      
      };
      
    })();
              </script>
    

</body></html>