<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.32">

  <meta name="author" content="Marcel Turcotte">
  <title>CSI 4106 - Fall 2025 – Training Artificial Neural Networks (Part 1)</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-f563837468303362081e247dddd440d0.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="Training Artificial Neural Networks (Part 1) – CSI 4106 - Fall 2025">
<meta property="og:description" content="CSI 4106 - Fall 2025">
<meta property="og:site_name" content="CSI 4106 - Fall 2025">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Training Artificial Neural Networks (Part 1)</h1>
  <p class="subtitle">CSI 4106 - Fall 2025</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Marcel Turcotte 
</div>
</div>
</div>

  <p class="date">Version: Jul 10, 2025 16:50</p>
</section>
<section>
<section id="preamble" class="title-slide slide level1 center">
<h1>Preamble</h1>

</section>
<section id="quote-of-the-day" class="slide level2 scrollable">
<h2>Quote of the Day</h2>
<p></p><div id="tweet-47541"></div><script>tweet={"url":"https:\/\/twitter.com\/demishassabis\/status\/1839354651206160563","author_name":"Demis Hassabis","author_url":"https:\/\/twitter.com\/demishassabis","html":"\u003Cblockquote class=\"twitter-tweet\" align=\"center\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003EFeedback loop: train SOTA chip design model (AlphaChip) -&gt; use it to design better AI chips -&gt; use them to train better models -&gt; to design better chips... part of the reason why our TPU stack is so good. Congrats \u003Ca href=\"https:\/\/twitter.com\/Azaliamirh?ref_src=twsrc%5Etfw\"\u003E@Azaliamirh\u003C\/a\u003E, \u003Ca href=\"https:\/\/twitter.com\/annadgoldie?ref_src=twsrc%5Etfw\"\u003E@annadgoldie\u003C\/a\u003E, \u003Ca href=\"https:\/\/twitter.com\/JeffDean?ref_src=twsrc%5Etfw\"\u003E@JeffDean\u003C\/a\u003E &amp; the AlphaChip team! \u003Ca href=\"https:\/\/t.co\/MLwaAP79Tg\"\u003Ehttps:\/\/t.co\/MLwaAP79Tg\u003C\/a\u003E\u003C\/p\u003E&mdash; Demis Hassabis (@demishassabis) \u003Ca href=\"https:\/\/twitter.com\/demishassabis\/status\/1839354651206160563?ref_src=twsrc%5Etfw\"\u003ESeptember 26, 2024\u003C\/a\u003E\u003C\/blockquote\u003E\n\u003Cscript async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"\u003E\u003C\/script\u003E\n\n","width":550,"height":null,"type":"rich","cache_age":"3153600000","provider_name":"Twitter","provider_url":"https:\/\/twitter.com","version":"1.0"};document.getElementById("tweet-47541").innerHTML = tweet["html"];</script><p></p>
<aside class="notes">
<p><a href="https://www.linkedin.com/in/demishassabis/?originalSubdomain=uk">Sir Demis Hassabis</a> is the Co-founder and CEO of <a href="https://deepmind.google">Google DeepMind</a>, a leading company dedicated to addressing some of the most complex scientific and engineering challenges of our era to propel scientific advancement. A chess prodigy from the age of four, Hassabis achieved master-level proficiency by 13 and served as the captain for several England junior chess teams. In 2024, he was awarded the Nobel Prize in <a href="https://www.nobelprize.org/prizes/chemistry/2024/summary/">Chemistry</a> for his contributions to the development of <a href="https://deepmind.google/technologies/alphafold/">AlphaFold</a>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="learning-objectives" class="slide level2">
<h2>Learning objectives</h2>
<ul>
<li><strong>Explain</strong> the architecture and function of feedforward neural networks (FNNs).</li>
<li><strong>Describe</strong> the backpropagation algorithm and its role in training neural networks.</li>
<li><strong>Identify</strong> common activation functions and understand their impact on network performance.</li>
<li><strong>Understand</strong> the vanishing gradient problem and strategies to mitigate it.</li>
</ul>
<aside class="notes">
<p>As with Assignment 2, I have compiled the important concepts for the next assignment into a single lecture.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="summary" class="title-slide slide level1 center">
<h1>Summary</h1>

</section>
<section id="blue1brown" class="slide level2">
<h2>3Blue1Brown</h2>
<iframe data-external="1" src="https://www.youtube.com/embed/aircAruvnKk" width="889" height="500" title="But what is a neural network? | Chapter 1, Deep learning" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<aside class="notes">
<p>It is highly recommended that you watch this video. While it covers the concepts we have already explored, it presents the material in a manner that is challenging to replicate in a classroom setting.</p>
<ul>
<li>Provides a clear explanation of the <strong>intuition</strong> behind the effectiveness of neural networks, detailing the <strong>hierarchy of concepts</strong> briefly mentioned in the last lecture.</li>
<li>Offers a compelling rationale for the necessity of a <strong>bias term</strong>.</li>
<li>Similarly, elucidates the concept of <strong>activation functions</strong> and the importance of a squashing function.</li>
<li>The segment beginning at 13m 26s offers a visual explanation of the <strong>linear algebra</strong> involved: <span class="math inline">\(\sigma(W X^T + b)\)</span>.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p>In my opinion, this is an <strong>excellent</strong> and <strong>informative</strong> video.</p>
</div></aside></section>
<section id="summary---dl" class="slide level2">
<h2>Summary - DL</h2>
<ul>
<li class="fragment"><p><strong>Deep learning (DL)</strong> is a <strong>machine learning</strong> technique that can be applied to <strong>supervised learning</strong> (including <strong>regression</strong> and <strong>classification</strong>), <strong>unsupervised learning</strong>, and <strong>reinforcement learning</strong>.</p></li>
<li class="fragment"><p>Inspired from the structure and function of <strong>biological neural networks</strong> found in animals.</p></li>
<li class="fragment"><p>Comprises <strong>interconnected neurons</strong> (or units) arranged into <strong>layers</strong>.</p></li>
</ul>
</section>
<section id="summary---fnn" class="slide level2">
<h2>Summary - FNN</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="../../assets/images/ann_mlp-07-01.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img data-src="../../assets/images/ann_mlp-07-01.png" class="quarto-figure quarto-figure-center" height="425"></a></p>
</figure>
</div>

<aside class="notes">
<p>Neural networks have <strong>inputs</strong> and <strong>outputs</strong>.</p>
<p>The network consists of <strong>three layers</strong>: input, hidden, and output. The <strong>input layer</strong> contains two nodes, the <strong>hidden layer</strong> comprises three nodes, and the <strong>output layer</strong> has two nodes. Additional hidden layers and nodes per layer can be added, which will be discussed later.</p>
<p>It is often useful to include explicit input nodes that do not perform calculations, known as <strong>input units</strong> or <strong>input neurons</strong>. These nodes act as placeholders to introduce input features into the network, passing data directly to the next layer without transformation. In the network diagram, these are the light blue nodes on the left. Typically, <strong>the number of input units corresponds to the number of features</strong>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p>Information in this architecture flows unidirectionally—from left to right, moving from input to output. Consequently, it is termed a <strong>feedforward neural network (FNN)</strong>.</p>
</div></aside></section>
<section id="summary---fnn-1" class="slide level2">
<h2>Summary - FNN</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="../../assets/images/ann_mlp-14-00.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img data-src="../../assets/images/ann_mlp-14-00.png" class="quarto-figure quarto-figure-center" height="475"></a></p>
</figure>
</div>

<aside class="notes">
<p>Neural networks can have a significantly large number of input nodes, often in the hundreds or thousands, depending on the complexity of the data. Additionally, they may contain numerous hidden layers. For instance, ResNet, which won the ILSVRC 2015 image classification task, features 152 layers. The authors of ResNet have demonstrated results for networks with 100 and even 1000 layers&nbsp;<span class="citation" data-cites="He:2016aa">(<a href="#/references" role="doc-biblioref" onclick="">He et al. 2016</a>)</span>. However, the number of output nodes tends to be relatively small. In regression problems, there is typically one output node, while in classification tasks (whether multiclass or multilabel), the number of output nodes corresponds to the number of classes.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p>The number of <strong>layers</strong> and <strong>nodes</strong> can vary based on the specific requirements.</p>
</div></aside></section>
<section id="summary---units" class="slide level2">
<h2>Summary - units</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="../../assets/images/ann_threhold_unit-03-en.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img data-src="../../assets/images/ann_threhold_unit-03-en.png" class="quarto-figure quarto-figure-center" height="450"></a></p>
</figure>
</div>

<aside class="notes">
<p>In the diagram above, it is important to clarify that the inputs and output pertain specifically to this individual unit, rather than to the entire network’s global inputs and output.</p>
<p>The name <strong>activation</strong> originates from the function’s role in determining whether a neuron should be “activated” or “fired” based on its input.</p>
<p>Historically, the concept was inspired by biological neurons, where a neuron activates and transmits a signal to other neurons if its input exceeds a certain threshold. In artificial neural networks, the activation function serves a similar purpose by introducing non-linearity into the model. This non-linearity is crucial because it enables the network to learn complex patterns and representations in the data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p>Introducing a fictitious input <span class="math inline">\(x^{(0)} = 1\)</span> is a <em>hack</em> that simplifies the expression <span class="math inline">\(x^T\theta + b\)</span>.</p>
</div></aside></section>
<section id="common-activation-functions" class="slide level2">
<h2>Common Activation Functions</h2>
<div id="18fbff38" class="cell" data-execution_count="1">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><a href="slides_files/figure-revealjs/cell-2-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img data-src="slides_files/figure-revealjs/cell-2-output-1.png" width="867" height="291"></a></p>
</figure>
</div>
</div>
</div>

<aside class="notes">
<p>Consider the following observations:</p>
<ul>
<li>The sigmoid function produces outputs within the open interval <span class="math inline">\((0, 1)\)</span>.</li>
<li>The hyperbolic tangent function (<span class="math inline">\(\tanh\)</span>) has an image spanning the open interval <span class="math inline">\((-1, 1)\)</span>.</li>
<li>The Rectified Linear Unit (ReLU) function outputs values in the interval <span class="math inline">\([0, \infty)\)</span>.</li>
</ul>
<p>Additionally, note:</p>
<ul>
<li>The maximum derivative value of the sigmoid function is 0.25.</li>
<li>The maximum derivative value of the <span class="math inline">\(\tanh\)</span> function is 1.</li>
<li>The derivative of the ReLU function is 0 for negative inputs and 1 for positive inputs.</li>
</ul>
<p>Furthermore:</p>
<ul>
<li>A node employing ReLU as its activation function generates outputs within the range <span class="math inline">\([0, \infty)\)</span>. However, its derivative, utilized in gradient descent during backpropagation, is constant, taking values of either 0 or 1.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p><span class="citation" data-cites="Geron:2022aa">Géron (<a href="#/references" role="doc-biblioref" onclick="">2022</a>)</span> – <a href="https://github.com/ageron/handson-ml3/blob/main/10_neural_nets_with_keras.ipynb">10_neural_nets_with_keras.ipynb</a></p>
</div></aside></section>
<section id="universal-approximation" class="slide level2">
<h2>Universal Approximation</h2>
<p>The <strong>universal approximation theorem</strong> states that a feedforward neural network with a single hidden layer containing a finite number of neurons can <strong>approximate any continuous function</strong> on a compact subset of <span class="math inline">\(\mathbb{R}^n\)</span>, given appropriate weights and activation functions.</p>

<aside><div>
<p><span class="citation" data-cites="Cybenko:1989aa">Cybenko (<a href="#/references" role="doc-biblioref" onclick="">1989</a>)</span>; <span class="citation" data-cites="Hornik:1989aa">Hornik, Stinchcombe, and White (<a href="#/references" role="doc-biblioref" onclick="">1989</a>)</span></p>
</div></aside></section></section>
<section>
<section id="notation" class="title-slide slide level1 center">
<h1>Notation</h1>

</section>
<section id="notation-1" class="slide level2">
<h2>Notation</h2>
<div class="columns">
<div class="column" style="width:40%;">
<p>A <strong>two-layer</strong> perceptron computes:</p>
<p><span class="math display">\[
  y = \phi_2(\phi_1(X))
\]</span></p>
<p>where</p>
<p><span class="math display">\[
  \phi_l(Z) = \phi(W_lZ_l + b_l)
\]</span></p>
</div><div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="../../assets/images/ann_mlp-07-01.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img data-src="../../assets/images/ann_mlp-07-01.png" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div></div>

<aside class="notes">

<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p>Where <span class="math inline">\(\phi\)</span> is an activation function, <span class="math inline">\(W\)</span> a weight matrix, <span class="math inline">\(X\)</span> an input matrix, and <span class="math inline">\(b\)</span> a bias vector.</p>
</div></aside></section>
<section id="notation-2" class="slide level2">
<h2>Notation</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>A <strong>3-layer</strong> perceptron computes:</p>
<p><span class="math display">\[
  y = \phi_3(\phi_2(\phi_1(X)))
\]</span></p>
<p>where</p>
<p><span class="math display">\[
  \phi_l(Z) = \phi(W_lZ_l + b_l)
\]</span></p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="../../assets/images/ann_mlp-14-01.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img data-src="../../assets/images/ann_mlp-14-01.png" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div></div>
</section>
<section id="notation-3" class="slide level2">
<h2>Notation</h2>
<p>A <strong><span class="math inline">\(k\)</span>-layer</strong> perceptron computes:</p>
<p><span class="math display">\[
  y = \phi_k( \ldots \phi_2(\phi_1(X)) \ldots )
\]</span></p>
<p>where</p>
<p><span class="math display">\[
  \phi_l(Z) = \phi(W_lZ_l + b_l)
\]</span></p>

<aside><div>
<p>A feedforward network exhibits a consistent structure, where each layer executes the same type of computation on varying inputs. Specifically, the input to layer <span class="math inline">\(l\)</span> is the output from layer <span class="math inline">\(l-1\)</span>.</p>
</div></aside></section></section>
<section>
<section id="back-propagation" class="title-slide slide level1 center">
<h1>Back-propagation</h1>

</section>
<section id="blue1brown-1" class="slide level2">
<h2>3Blue1Brown</h2>
<iframe data-external="1" src="https://www.youtube.com/embed/Ilg3gGewQ5U" width="889" height="500" title="What is backpropagation really doing? | Chapter 3, Deep learning" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</section>
<section id="back-propagation-1" class="slide level2 smaller">
<h2>Back-propagation</h2>
<p><strong>Learning representations by back-propagating errors</strong></p>
<p><em>David E. Rumelhart, <u>Geoffrey E. Hinton</u> &amp; Ronald J. Williams</em></p>
<p>We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure <strong>repeatedly adjusts the weights</strong> of the connections in the network so as to <strong>minimize a measure of the difference between the actual output vector of the net and the desired output vector</strong>. As a result of the weight adjustments, internal ‘<strong>hidden</strong>’ units which are not part of the input or output come to <strong>represent important features of the task domain</strong>, and the regularities in the task are captured by the interactions of these units. The <strong>ability to create useful new features</strong> distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure.</p>

<aside class="notes">
<p>I am presenting here the abstract from the seminal Nature publication where Hinton and colleagues introduced the backpropagation algorithm. This abstract is both elegant and informative, effectively capturing the core principles of modern neural networks: the concept of a loss function, the iterative adjustment of weights through the gradient descent algorithm, and the critical role of hidden layers in generating useful task-dependent features.</p>
<p>Nature is a prestigious journal, and it only occasionally publishes content related to computer science.</p>
<p>At the time of this publication, Hinton was affiliated with Carnegie Mellon University. As a reminder, Hinton received the Nobel Prize in Physics in 2024 for his contributions to developing foundational methods in modern machine learning.</p>
<p>The abstract highlights the rationale for using hidden layers in neural networks. The initial hidden layers learn simple representations directly from the input data, while subsequent layers identify associations among these representations. Each layer builds upon the knowledge of previous layers, culminating in the network’s final output.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p><span class="citation" data-cites="10.1038/323533a0">Rumelhart, Hinton, and Williams (<a href="#/references" role="doc-biblioref" onclick="">1986</a>)</span></p>
</div></aside></section>
<section id="before-the-back-propagation" class="slide level2">
<h2>Before the back-propagation</h2>
<ul>
<li class="fragment"><p>Limitations, such as the <strong>inability to solve the XOR classification task</strong>, essentially <strong>stalled</strong> research on neural networks.</p></li>
<li class="fragment"><p>The perceptron was <strong>limited to a single layer</strong>, and there was <strong>no known method for training a multi-layer perceptron</strong>.</p></li>
<li class="fragment"><p>Single-layer perceptrons are limited to solving classification tasks that are <strong>linearly separable</strong>.</p></li>
</ul>
</section>
<section id="back-propagation-contributions" class="slide level2">
<h2>Back-propagation: contributions</h2>
<ul>
<li class="fragment"><p>The model employs <strong>mean squared error</strong> as its <strong>loss function</strong>.</p></li>
<li class="fragment"><p><strong>Gradient descent</strong> is used to minimize <strong>loss</strong>.</p></li>
<li class="fragment"><p>A <strong>sigmoid activation function</strong> is used instead of a step function, as its <strong>derivative</strong> provides valuable information for gradient descent.</p></li>
<li class="fragment"><p>Shows how updating <strong>internal weights</strong> using a <strong>two-pass</strong> algorithm consisting of a <strong>forward</strong> pass and a <strong>backward</strong> pass.</p></li>
<li class="fragment"><p><strong>Enables training multi-layer perceptrons.</strong></p></li>
</ul>
</section>
<section id="backpropagation-top-level" class="slide level2">
<h2>Backpropagation: top level</h2>
<ol type="1">
<li><p><strong>Initialization</strong></p></li>
<li><p><strong>Forward Pass</strong></p></li>
<li><p><strong>Compute Loss</strong></p></li>
<li><p><strong>Backward Pass (Backpropagation)</strong></p></li>
<li><p><strong>Repeat 2 to 5</strong>.</p></li>
</ol>

<aside><div>
<p>The algorithm <strong>stops</strong> either after a <strong>predefined number of epochs</strong> or when c<strong>onvergence criteria are satisfied</strong>.</p>
</div></aside></section>
<section id="backpropagation-1.-initialization" class="slide level2 smaller">
<h2>Backpropagation: 1. Initialization</h2>
<p>Initialize the <strong>weights</strong> and <strong>biases</strong> of the neural network.</p>
<ol type="1">
<li><strong><del>Zero Initialization</del></strong>
<ul>
<li>All weights are <strong>initialized to zero</strong>.</li>
<li>Symmetry problems, <strong>all neurons produce identical outputs</strong>, preventing effective learning.</li>
</ul></li>
<li><strong>Random Initialization</strong>
<ul>
<li>Weights are initialized <strong>randomly</strong>, often using a <strong>uniform</strong> or <strong>normal</strong> distribution.</li>
<li><strong>Breaks the symmetry between neurons</strong>, allowing them to learn.</li>
<li>If not scaled properly, leads to <strong>slow convergence</strong> or <strong>vanishing/exploding gradients</strong>.</li>
</ul></li>
</ol>

<aside class="notes">
<p>Initializing weights and biases to zero works for logistic regression because it is a linear model with a single layer. In logistic regression, each feature’s weight is independently adjusted during training, and the optimization process can converge correctly regardless of the initial weights, provided the data is linearly separable.</p>
<p>However, zero initialization does not work well for neural networks due to their multi-layered structure. Here’s why:</p>
<ol type="1">
<li><p><strong>Symmetry Breaking</strong>: Neural networks require breaking symmetry between neurons in each layer so that they can learn different features. If all weights are initialized to zero, each neuron in a layer will compute the same output and receive the same gradient during backpropagation. This results in the neurons updating identically, preventing them from learning distinct features and effectively rendering multiple neurons redundant.</p></li>
<li><p><strong>Non-Linearity</strong>: Neural networks rely on non-linear transformations between layers to model complex relationships in the data. Zero initialization inhibits the ability of neurons to activate differently, impeding the network’s capacity to capture non-linear patterns.</p></li>
</ol>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p><strong>See also</strong>: Xavier/Glorot and He initialization (later)</p>
</div></aside></section>
<section id="backpropagation-2.-forward-pass" class="slide level2 smaller">
<h2>Backpropagation: 2. Forward Pass</h2>
<p><strong>For each example</strong> in the training set (or in a mini-batch):</p>
<ul>
<li class="fragment"><p><strong>Input Layer</strong>: Pass input features to first layer.</p></li>
<li class="fragment"><p><strong>Hidden Layers</strong>: For each hidden layer, compute the <strong>activations</strong> (output) by applying the <strong>weighted sum of inputs plus bias</strong>, followed by an <strong>activation function</strong> (e.g., sigmoid, ReLU).</p></li>
<li class="fragment"><p><strong>Output Layer</strong>: Same process as hidden layers. Output layer activations represent the predicted values.</p></li>
</ul>

<aside class="notes">
<p>In practice, it is the mini-batch version of this algorithm that is being used.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p>The forward pass is almost identical to applying the network for prediction (<code>.predict()</code>), with the exception that intermediate (activation) results are saved, as they are needed for the backward pass.</p>
</div></aside></section>
<section id="backpropagation-3.-compute-loss" class="slide level2">
<h2>Backpropagation: 3. Compute Loss</h2>
<p>Calculate the <strong>loss</strong> (<strong>error</strong>) using a suitable loss function by comparing the <strong>predicted values</strong> to the <strong>actual target</strong> values.</p>

<aside class="notes">
<p>A smaller loss indicates that the predicted values are closer to the actual target values.</p>
<p>The value of the loss function can serve as a stopping criterion, with backpropagation halting when the loss is sufficiently small.</p>
<p>Crucially, the derivative of the loss function provides essential information for adjusting the network’s weights and bias terms.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p>More on the various loss functions coming later: <strong>mean squared error</strong> for regression tasks or <strong>cross-entropy</strong> loss for classification tasks.</p>
</div></aside></section>
<section id="backpropagation-4.-backward-pass" class="slide level2 smaller">
<h2>Backpropagation: 4. Backward Pass</h2>
<ul>
<li class="fragment"><p><strong>Output Layer</strong>: Compute the <strong>gradient of the loss</strong> with respect to the output layer’s <strong>weights</strong> and <strong>biases</strong> using the <strong>chain rule of calculus</strong>.</p></li>
<li class="fragment"><p><strong>Hidden Layers</strong>: <strong>Propagate the error backward</strong> through the network, <strong>layer by layer</strong>. For each layer, compute the gradient of the loss with respect to the weights and biases. Use the derivative of the activation function to help calculate these gradients.</p></li>
<li class="fragment"><p><strong>Update Weights and Biases</strong>: Adjust the weights and biases using the calculated gradients and a learning rate, which determines the step size for each update.</p></li>
</ul>

<aside class="notes">
<p>At the end of the presentation, links are provided to a series of videos by Herman Kamper. These videos elucidate the intricacies of the backpropagation algorithm across various architectures, both with and without forks, utilizing function composition and graph computation approaches.</p>
<p>While the algorithm is complex due to the numerous cases it entails, its regular structure makes it suitable for automation. Specifically, algorithms like automatic differentiation (autodiff) facilitate this process.</p>
<p>In 1970, <a href="https://en.wikipedia.org/wiki/Seppo_Linnainmaa">Seppo Ilmari Linnainmaa</a> introduced the algorithm known as reverse mode automatic differentiation in his MSc thesis. Although he did not apply this algorithm to neural networks, it is more general than backpropagation.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p>Common optimization techniques like <strong>gradient descent</strong> or its variants (e.g., Adam) are employed.</p>
</div></aside></section>
<section id="key-concepts" class="slide level2">
<h2>Key Concepts</h2>
<ul>
<li><p><strong>Activation Functions</strong>: Functions like sigmoid, ReLU, and tanh introduce non-linearity, which allows the network to learn complex patterns.</p></li>
<li><p><strong>Learning Rate</strong>: A hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated.</p></li>
<li><p><strong>Gradient Descent</strong>: An optimization algorithm used to minimize the loss function by iteratively moving towards the steepest descent as defined by the negative of the gradient.</p></li>
</ul>
</section>
<section id="summary-1" class="slide level2">
<h2>Summary</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="../../assets/images/angermueller-2016_box-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img data-src="../../assets/images/angermueller-2016_box-1.png" class="quarto-figure quarto-figure-center" height="475"></a></p>
</figure>
</div>

<aside><div>
<p><strong>Attribution</strong>: <span class="citation" data-cites="angermueller:2016fj">Angermueller et al. (<a href="#/references" role="doc-biblioref" onclick="">2016</a>)</span></p>
</div></aside></section></section>
<section>
<section id="training" class="title-slide slide level1 center">
<h1>Training</h1>

</section>
<section id="vanishing-gradients" class="slide level2">
<h2>Vanishing gradients</h2>
<ul>
<li class="fragment"><p><strong>Vanishing gradient problem</strong>: Gradients become too small, <strong>hindering weight updates</strong>.</p></li>
<li class="fragment"><p><strong>Stalled</strong> neural network research (again) in early 2000s.</p></li>
<li class="fragment"><p><strong>Sigmoid</strong> and its derivative (range: 0 to 0.25) were key factors.</p></li>
<li class="fragment"><p><strong>Common initialization</strong>: Weights/biases from <span class="math inline">\(\mathcal{N}(0, 1)\)</span> contributed to the issue.</p></li>
</ul>
<div class="asdie">
<p><span class="citation" data-cites="Glorot:2010aa">Glorot and Bengio (<a href="#/references" role="doc-biblioref" onclick="">2010</a>)</span> shed light on the problems.</p>
</div>
<aside class="notes">
<p>The vanishing gradient problem often occurs with activation functions like the sigmoid and hyperbolic tangent (tanh), leading to difficulties in training deep neural networks due to diminishing gradients that slow down learning.</p>
<p>In contrast, the exploding gradient problem, which involves gradients growing excessively large, is typically observed in architectures such as recurrent neural networks (RNNs).</p>
<p>Both issues can significantly affect the stability and convergence of gradient-based optimization techniques, thereby hindering the effective training of deep models.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="vanishing-gradients-solutions" class="slide level2">
<h2>Vanishing gradients: solutions</h2>
<ul>
<li class="fragment"><p><strong>Alternative activation functions</strong>: Rectified Linear Unit (<strong>ReLU</strong>) and its variants (e.g., <strong>Leaky ReLU</strong>, <strong>Parametric ReLU</strong>, and <strong>Exponential Linear Unit</strong>).</p></li>
<li class="fragment"><p><strong>Weight Initialization</strong>: Xavier (Glorot) or He initialization.</p></li>
</ul>
<aside class="notes">
<p>Other techniques exists to mitigate the problem, including those:</p>
<ul>
<li><p><strong>Batch Normalization</strong>: Implement batch normalization to standardize the inputs to each layer, which can help stabilize and accelerate training by reducing internal covariate shift and maintaining effective gradient flow.</p></li>
<li><p><strong>Residual Networks</strong>: Use residual connections, as seen in ResNet architectures, which allow gradients to flow more easily through the network by providing shortcut paths that bypass one or more layers.</p></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="glorot-and-bengio" class="slide level2">
<h2>Glorot and Bengio</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Figure 6</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="../../assets/images/glorot-2010-f6.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img data-src="../../assets/images/glorot-2010-f6.png" class="nostretch quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<p>Figure 7</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="../../assets/images/glorot-2010-f7.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img data-src="../../assets/images/glorot-2010-f7.png" class="nostretch quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div></div>

<aside class="notes">
<p>The graphs presented illustrate the normalized histograms of activation values and back-propagated gradients associated with the hyperbolic tangent activation function.</p>
<p>The produce the top diagrams, Glorot and Bengio used an initialization method that was popular at the time, whereas the bottom diagrams have been produced using a new scheme (Glorot).</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p><span class="citation" data-cites="Glorot:2010aa">Glorot and Bengio (<a href="#/references" role="doc-biblioref" onclick="">2010</a>)</span>, page 254.</p>
</div></aside></section>
<section id="glorot-and-bengio-1" class="slide level2">
<h2>Glorot and Bengio</h2>
<p><strong>Objective:</strong> Mitigate the unstable gradients problem in deep neural networks.</p>
<p><strong>Signal Flow:</strong></p>
<ul>
<li><strong>Forward Direction:</strong> Ensure stable signal propagation for accurate predictions.</li>
<li><strong>Reverse Direction:</strong> Maintain consistent gradient flow during backpropagation.</li>
</ul>

<aside><div>
<p><span class="citation" data-cites="Glorot:2010aa">Glorot and Bengio (<a href="#/references" role="doc-biblioref" onclick="">2010</a>)</span>: pay attention to signal flow in both directions!</p>
</div></aside></section>
<section id="glorot-and-bengio-2" class="slide level2">
<h2>Glorot and Bengio</h2>
<p><strong>Variance Matching:</strong></p>
<ul>
<li><p><strong>Forward Pass:</strong> Ensure the output variance of each layer matches its input variance.</p></li>
<li><p><strong>Backward Pass:</strong> Maintain equal gradient variance before and after passing through each layer.</p></li>
</ul>

<aside><div>
<p>Keras employs Glorot initialization by default, which is well-suited for activation functions such as <strong>sigmoid</strong>, <strong>tanh</strong>, and <strong>softmax</strong>.</p>
</div></aside></section>
<section id="he-initialization" class="slide level2">
<h2>He initialization</h2>
<p>A similar but slightly different initialization method design to work with <strong>ReLU</strong>, as well as <strong>Leaky ReLU</strong>, <strong>ELU</strong>, <strong>GELU</strong>, <strong>Swish</strong>, and <strong>Mish</strong>.</p>
<div class="fragment">
<p>Ensure that the initialization method <strong>matches</strong> the chosen activation function.</p>
<div id="db398777" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-2"><a></a><span class="im">from</span> tensorflow.python.keras.layers <span class="im">import</span> Dense</span>
<span id="cb1-3"><a></a></span>
<span id="cb1-4"><a></a>dense <span class="op">=</span> Dense(<span class="dv">50</span>, activation<span class="op">=</span><span class="st">"relu"</span>, kernel_initializer<span class="op">=</span><span class="st">"he_normal"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>

<aside class="notes">
<ul>
<li><p><strong>Glorot Initialization (Xavier Initialization):</strong> This method sets the initial weights based on the number of input and output units for each layer, aiming to keep the variance of activations consistent across layers. It is particularly effective for activation functions like sigmoid and tanh.</p></li>
<li><p><strong>He Initialization:</strong> This approach adjusts the weight initialization to be suitable for layers using ReLU and its variants, by scaling the variance according to the number of input units only.</p></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div>
<aside><div>
<p>AKA Kaiming initialization.</p>
</div></aside></section>
<section id="note" class="slide level2">
<h2>Note</h2>
<p>Randomly initializing the weights<sup>1</sup> is sufficient to <strong>break symmetry</strong> in a neural network, <strong>allowing the bias terms to be set to zero</strong> without impacting the network’s ability to learn effectively.</p>
<aside><ol class="aside-footnotes"><li id="fn1"><p>Proper initialization of weights, such as using Xavier/Glorot or He initialization, is crucial and should be aligned with the choice of activation function to ensure optimal network performance.</p></li></ol></aside></section>
<section id="activation-function-leaky-relu" class="slide level2">
<h2>Activation Function: Leaky ReLU</h2>
<div id="5c656e99" class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><a href="slides_files/figure-revealjs/cell-4-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img data-src="slides_files/figure-revealjs/cell-4-output-1.png" width="949" height="468"></a></p>
</figure>
</div>
</div>
</div>

<aside class="notes">
<p>When the input to the ReLU activation function, the weighted sum plus bias, is negative for all the training examples, the output value of ReLU is zero. But also, its derivative is 0, which effectively deactivates the neuron. Leaky ReLU, or other variants, effectively mitigates the issue.</p>
<div id="6a7b9c4d" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb2-2"><a></a><span class="im">from</span> tensorflow.python.keras.layers <span class="im">import</span> Dense</span>
<span id="cb2-3"><a></a></span>
<span id="cb2-4"><a></a>leaky_relu <span class="op">=</span> tf.keras.layers.LeakyReLU(negative_slope<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb2-5"><a></a>dense <span class="op">=</span> tf.keras.layers.Dense(<span class="dv">50</span>, activation<span class="op">=</span>leaky_relu, kernel_initializer<span class="op">=</span><span class="st">"he_normal"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Keras proposes 18 <a href="https://keras.io/api/layers/activations/#available-activations">layer activation functions</a> at the time of writing.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p>The Leaky ReLU, a variant of the standard ReLU activation function, effectively mitigates the issue of <em>dying ReLU nodes</em>. For negative input values, it introduces a linear component with a slope governed by the parameter <code>negative_slope</code>.</p>
</div></aside></section></section>
<section>
<section id="prologue" class="title-slide slide level1 center">
<h1>Prologue</h1>

</section>
<section id="summary-2" class="slide level2 scrollable">
<h2>Summary</h2>
<ul>
<li><strong>Artificial Neural Networks (ANNs):</strong>
<ul>
<li>Inspired by biological neural networks.</li>
<li>Consist of interconnected neurons arranged in layers.</li>
<li>Applicable to supervised, unsupervised, and reinforcement learning.</li>
</ul></li>
<li><strong>Feedforward Neural Networks (FNNs):</strong>
<ul>
<li>Information flows unidirectionally from input to output.</li>
<li>Comprised of input, hidden, and output layers.</li>
<li>Can vary in the number of layers and nodes per layer.</li>
</ul></li>
<li><strong>Activation Functions:</strong>
<ul>
<li>Introduce non-linearity to enable learning complex patterns.</li>
<li>Common functions: Sigmoid, Tanh, ReLU, Leaky ReLU.</li>
<li>Choice of activation function affects gradient flow and network performance.</li>
</ul></li>
<li><strong>Universal Approximation Theorem:</strong>
<ul>
<li>A neural network with a single hidden layer can approximate any continuous function.</li>
</ul></li>
<li><strong>Backpropagation Algorithm:</strong>
<ul>
<li>Training involves forward pass, loss computation, backward pass, and weight updates.</li>
<li>Utilizes gradient descent to minimize the loss function.</li>
<li>Enables training of multi-layer perceptrons by adjusting internal weights.</li>
</ul></li>
<li><strong>Vanishing Gradient Problem:</strong>
<ul>
<li>Gradients become too small during backpropagation, hindering training.</li>
<li>Mitigation strategies include using ReLU activation functions and proper weight initialization (Glorot or He initialization).</li>
</ul></li>
<li><strong>Weight Initialization:</strong>
<ul>
<li>Random initialization breaks symmetry and allows effective learning.</li>
<li>Glorot initialization suits sigmoid and tanh activations.</li>
<li>He initialization is optimal for ReLU and its variants.</li>
</ul></li>
<li><strong>Key Concepts:</strong>
<ul>
<li>Learning rate determines the step size during optimization.</li>
<li>Gradient descent is used to update weights in the direction of minimizing loss.</li>
<li>Proper selection of activation functions and initialization methods is crucial for effective training.</li>
</ul></li>
</ul>
</section>
<section id="blue1brown-2" class="slide level2">
<h2>3Blue1Brown</h2>
<p>A series of videos, with animations, providing the <strong>intuition</strong> behind the <strong>backpropagation algorithm</strong>.</p>
<ul>
<li><p><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">Neural networks</a> (playlist)</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;index=3&amp;t=212s">What is backpropagation really doing?</a> (12m 47s)</li>
<li><a href="https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;index=4">Backpropagation calculus</a> (10m 18s)</li>
</ul></li>
</ul>

<aside><div>
<p>Prerequisite: <a href="https://www.youtube.com/watch?v=IHZwWFHWa-w&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;index=2">Gradient descent, how neural networks learn?</a> (20m&nbsp;33s)</p>
</div></aside></section>
<section id="statquest" class="slide level2">
<h2>StatQuest</h2>
<ul>
<li><a href="https://youtu.be/IN2XmBhILt4">Neural Networks Pt. 2: Backpropagation Main Ideas</a> (17m&nbsp;34s)</li>
<li><a href="https://youtu.be/iyn2zdALii8">Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously</a> (18m&nbsp;32s)</li>
<li><a href="https://youtu.be/GKZoOHXGcLo">Backpropagation Details Pt. 2: Going bonkers with The Chain Rule</a> (13m&nbsp;9s)</li>
</ul>

<aside><div>
<p>Prerequisites: <a href="https://youtu.be/wl1myxrtQHQ">The Chain Rule</a> (18m&nbsp;24s) &amp; <a href="https://youtu.be/sDv4f4s2SB8">Gradient Descent, Step-by-Step</a> (23m&nbsp;54s)</p>
</div></aside></section>
<section id="herman-kamper" class="slide level2 smaller">
<h2>Herman Kamper</h2>
<p>One of the most thorough series of <strong>videos</strong> on the <strong>backpropagation algorithm</strong>.</p>
<ul>
<li><p><a href="https://www.youtube.com/playlist?list=PLmZlBIcArwhMHnIrNu70mlvZOwe6MqWYn">Introduction to neural networks</a> (playlist)</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=6SW1oUztmzg&amp;list=PLmZlBIcArwhMHnIrNu70mlvZOwe6MqWYn&amp;index=7&amp;t=680s">Backpropagation (without forks)</a> (31m&nbsp;1s)</li>
<li><a href="https://www.youtube.com/watch?v=dTupaVdrz1k&amp;list=PLmZlBIcArwhMHnIrNu70mlvZOwe6MqWYn&amp;index=8">Backprop for a multilayer feedforward neural network</a> (4m&nbsp;2s)</li>
<li><a href="https://www.youtube.com/watch?v=fBSm5ElvJEg&amp;list=PLmZlBIcArwhMHnIrNu70mlvZOwe6MqWYn&amp;index=9">Computational graphs and automatic differentiation for neural networks</a> (6m&nbsp;56s)</li>
<li><a href="https://www.youtube.com/watch?v=aqnjXWxiT0o&amp;list=PLmZlBIcArwhMHnIrNu70mlvZOwe6MqWYn&amp;index=10">Common derivatives for neural networks</a> (7m&nbsp;18s)</li>
<li><a href="https://www.youtube.com/watch?v=Sa5_Gl_sYoI&amp;list=PLmZlBIcArwhMHnIrNu70mlvZOwe6MqWYn&amp;index=11">A general notation for derivatives (in neural networks)</a> (7m&nbsp;56s)</li>
<li><a href="https://www.youtube.com/watch?v=6mmEw738MQo&amp;list=PLmZlBIcArwhMHnIrNu70mlvZOwe6MqWYn&amp;index=12">Forks in neural networks</a> (13m&nbsp;46s)</li>
<li><a href="https://www.youtube.com/watch?v=aRkhgm2i4p0&amp;list=PLmZlBIcArwhMHnIrNu70mlvZOwe6MqWYn&amp;index=13">Backpropagation in general (now with forks)</a> (3m&nbsp;42s)</li>
</ul></li>
</ul>
</section>
<section id="next-lecture" class="slide level2">
<h2>Next lecture</h2>
<ul>
<li>We will talk about softmas, cross-entropy, and regularization.</li>
</ul>
</section>
<section id="references" class="slide level2 smaller scrollable">
<h2>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-angermueller:2016fj" class="csl-entry" role="listitem">
Angermueller, Christof, Tanel Pärnamaa, Leopold Parts, and Oliver Stegle. 2016. <span>“Deep Learning for Computational Biology.”</span> <em>Mol Syst Biol</em> 12 (7): 878. <a href="https://doi.org/10.15252/msb.20156651">https://doi.org/10.15252/msb.20156651</a>.
</div>
<div id="ref-Cybenko:1989aa" class="csl-entry" role="listitem">
Cybenko, George V. 1989. <span>“Approximation by Superpositions of a Sigmoidal Function.”</span> <em>Mathematics of Control, Signals and Systems</em> 2: 303–14. <a href="https://api.semanticscholar.org/CorpusID:3958369">https://api.semanticscholar.org/CorpusID:3958369</a>.
</div>
<div id="ref-Geron:2022aa" class="csl-entry" role="listitem">
Géron, Aurélien. 2022. <em>Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow</em>. 3rd ed. O’Reilly Media, Inc.
</div>
<div id="ref-Glorot:2010aa" class="csl-entry" role="listitem">
Glorot, Xavier, and Yoshua Bengio. 2010. <span>“Understanding the Difficulty of Training Deep Feedforward Neural Networks.”</span> In <em>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</em>, edited by Yee Whye Teh and Mike Titterington, 9:249–56. Proceedings of Machine Learning Research. Chia Laguna Resort, Sardinia, Italy: PMLR. <a href="https://proceedings.mlr.press/v9/glorot10a.html">https://proceedings.mlr.press/v9/glorot10a.html</a>.
</div>
<div id="ref-He:2016aa" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. <span>“Deep Residual Learning for Image Recognition.”</span> In <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 770–78. <a href="https://doi.org/10.1109/CVPR.2016.90">https://doi.org/10.1109/CVPR.2016.90</a>.
</div>
<div id="ref-Hornik:1989aa" class="csl-entry" role="listitem">
Hornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989. <span>“Multilayer Feedforward Networks Are Universal Approximators.”</span> <em>Neural Networks</em> 2 (5): 359–66. https://doi.org/<a href="https://doi.org/10.1016/0893-6080(89)90020-8">https://doi.org/10.1016/0893-6080(89)90020-8</a>.
</div>
<div id="ref-10.1038/323533a0" class="csl-entry" role="listitem">
Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986. <span>“<span class="nocase">Learning representations by back-propagating errors</span>.”</span> <em>Nature</em> 323 (6088): 533–36. <a href="https://doi.org/10.1038/323533a0">https://doi.org/10.1038/323533a0</a>.
</div>
<div id="ref-Russell:2020aa" class="csl-entry" role="listitem">
Russell, Stuart, and Peter Norvig. 2020. <em>Artificial Intelligence: <span>A</span> Modern Approach</em>. 4th ed. Pearson. <a href="http://aima.cs.berkeley.edu/">http://aima.cs.berkeley.edu/</a>.
</div>
</div>
</section>
<section class="slide level2">

<p>Marcel <strong>Turcotte</strong></p>
<p><a href="mailto:Marcel.Turcotte@uOttawa.ca">Marcel.Turcotte@uOttawa.ca</a></p>
<p>School of Electrical Engineering and <strong>Computer Science</strong> (EE<strong>CS</strong>)</p>
<p>University of Ottawa</p>


</section></section>

    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../../assets/images/uottawa_hor_black.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://turcotte.xyz/teaching/csi-4106">turcotte.xyz/teaching/csi-4106</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/turcotte\.xyz\/teaching\/csi-4106");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    <script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
    (function() {
      let previousOnload = window.onload;
      window.onload = () => {
        if (previousOnload) {
          previousOnload();
        }
        lightboxQuarto.on('slide_before_load', (data) => {
          const { slideIndex, slideNode, slideConfig, player, trigger } = data;
          const href = trigger.getAttribute('href');
          if (href !== null) {
            const imgEl = window.document.querySelector(`a[href="${href}"] img`);
            if (imgEl !== null) {
              const srcAttr = imgEl.getAttribute("src");
              if (srcAttr && srcAttr.startsWith("data:")) {
                slideConfig.href = srcAttr;
              }
            }
          } 
        });
      
        lightboxQuarto.on('slide_after_load', (data) => {
          const { slideIndex, slideNode, slideConfig, player, trigger } = data;
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(slideNode);
          }
        });
      
      };
      
    })();
              </script>
    

<script src="https://platform.twitter.com/widgets.js"></script>
</body></html>